\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Declaration of Authorship}{i}{dummy.1}}
\@writefile{toc}{\vspace  {1em}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{ii}{dummy.2}}
\@writefile{toc}{\vspace  {1em}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iii}{dummy.3}}
\@writefile{toc}{\vspace  {1em}}
\@writefile{toc}{\contentsline {chapter}{Contents}{iv}{dummy.4}}
\gdef \LT@i {\LT@entry 
    {1}{48.04353pt}\LT@entry 
    {1}{169.3712pt}}
\@writefile{toc}{\contentsline {chapter}{Abbreviations}{vi}{dummy.6}}
\citation{rosenblatt1958perceptron}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}A Primer on Artifical Neural Networks}{1}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter3}{{1}{1}{A Primer on Artifical Neural Networks}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.10}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Perceptrons}{1}{section.11}}
\newlabel{sec:percep}{{1.2}{1}{Perceptrons}{section.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}What is a Perceptron}{1}{subsection.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A perceptron with three binary inputs, $x_0, x_1, x_2$.\relax }}{2}{figure.caption.13}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:perceptron}{{1.1}{2}{A perceptron with three binary inputs, $x_0, x_1, x_2$.\relax }{figure.caption.13}{}}
\newlabel{eqn:percep}{{1.1}{2}{What is a Perceptron}{equation.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces A multi-layer perceptron consisting of three perceptrons arranged in two layers, with three binary inputs, $x_0, x_1, x_2$.\relax }}{3}{figure.caption.16}}
\newlabel{fig:mlp}{{1.2}{3}{A multi-layer perceptron consisting of three perceptrons arranged in two layers, with three binary inputs, $x_0, x_1, x_2$.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Multi-Layer Perceptron}{3}{subsection.15}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Activation Functions}{3}{section.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualisation of the perceptron activation function.\relax }}{4}{figure.caption.18}}
\newlabel{fig:activation_percep}{{1.3}{4}{Visualisation of the perceptron activation function.\relax }{figure.caption.18}{}}
\newlabel{eqn:proport}{{1.2}{4}{Activation Functions}{equation.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Sigmoid Neurons}{4}{subsection.20}}
\newlabel{eqn:sig}{{1.3}{4}{Sigmoid Neurons}{equation.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Visualisation of the sigmoid activation function.\relax }}{5}{figure.caption.23}}
\newlabel{fig:activation_sigmoid}{{1.4}{5}{Visualisation of the sigmoid activation function.\relax }{figure.caption.23}{}}
\newlabel{eqn:z_act}{{1.4}{5}{Sigmoid Neurons}{equation.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1.1}Other activation functions}{5}{subsubsection.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Visualisation of the hyperbolic tangent activation function.\relax }}{6}{figure.caption.25}}
\newlabel{fig:activation_tanh}{{1.5}{6}{Visualisation of the hyperbolic tangent activation function.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Visualisation of the rectified linear unit activation function.\relax }}{6}{figure.caption.26}}
\newlabel{fig:activation_relu}{{1.6}{6}{Visualisation of the rectified linear unit activation function.\relax }{figure.caption.26}{}}
\citation{lecun1998mnist}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Learning Algorithms}{7}{section.28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Types of Training}{7}{subsection.30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Cost functions: How wrong am I?}{8}{subsection.31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2.1}Mean Squared Error}{8}{subsubsection.32}}
\newlabel{eqn:mse}{{1.5}{8}{Mean Squared Error}{equation.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2.2}Cross-Entropy}{8}{subsubsection.34}}
\newlabel{eqn:xentrpy}{{1.6}{9}{Cross-Entropy}{equation.35}{}}
\newlabel{eqn:xentrpy_eg}{{1.7}{9}{Cross-Entropy}{equation.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2.3}Kullback-Leibler Divergence}{9}{subsubsection.37}}
\newlabel{eqn:kld}{{1.8}{9}{Kullback-Leibler Divergence}{equation.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces The cost landscape of a bivariate function\relax }}{10}{figure.caption.40}}
\newlabel{fig:costscape}{{1.7}{10}{The cost landscape of a bivariate function\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Gradient Decent}{10}{subsection.39}}
\newlabel{eqn:cost_bivar}{{1.9}{11}{Gradient Decent}{equation.41}{}}
\newlabel{eqn:delta_C}{{1.10}{11}{Gradient Decent}{equation.42}{}}
\newlabel{eqn:nabla_c}{{1.11}{11}{Gradient Decent}{equation.43}{}}
\newlabel{eqn:delta_c_sub}{{1.12}{11}{Gradient Decent}{equation.44}{}}
\newlabel{eqn:delta_w_eta}{{1.13}{11}{Gradient Decent}{equation.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Backpropagation}{12}{subsection.47}}
\newlabel{eqn:BP1}{{1.14}{12}{Backpropagation}{equation.48}{}}
\newlabel{eqn:BP2}{{1.15}{12}{Backpropagation}{equation.49}{}}
\newlabel{eqn:hada}{{1.16}{12}{}{equation.51}{}}
\newlabel{eqn:BP3}{{1.17}{13}{Backpropagation}{equation.52}{}}
\newlabel{eqn:BP4}{{1.18}{13}{Backpropagation}{equation.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Extensions and Improvements}{13}{subsection.54}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.5.1}Learning Rate Schedule}{13}{subsubsection.55}}
\newlabel{sec:lr_sch}{{1.4.5.1}{13}{Learning Rate Schedule}{subsubsection.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Visualisation of cost minimisation in two dimensions for different learning rate schedules. Blue: decreaseing learning rate, Red: large fixed learning rate.\relax }}{14}{figure.caption.56}}
\newlabel{fig:lr_schedule}{{1.8}{14}{Visualisation of cost minimisation in two dimensions for different learning rate schedules. Blue: decreaseing learning rate, Red: large fixed learning rate.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.5.2}Momentum}{15}{subsubsection.57}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Convolutional Neural Networks}{15}{section.58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}What is Convolution?}{15}{subsection.59}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1.1}Kernels}{15}{subsubsection.60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1.2}Strides}{15}{subsubsection.61}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1.3}Dilations}{15}{subsubsection.62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Transposed Convolutions}{15}{subsection.63}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Recurrent Neural Networks}{15}{section.64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Vanilla RNN}{15}{subsection.65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Gated RNN}{15}{subsection.66}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Summary}{15}{section.67}}
\@writefile{toc}{\vspace  {2em}}
\@writefile{toc}{\vspace  {2em}}
\bibstyle{apalike}
\bibdata{Bibliography}
\bibcite{barsalou2008grounded}{{1}{2008}{{Barsalou}}{{}}}
\bibcite{hammami2010improved}{{2}{2010}{{Hammami and Bedda}}{{}}}
\bibcite{hammami2009tree}{{3}{2009}{{Hammami and Sellam}}{{}}}
\bibcite{lecun1998mnist}{{4}{1998}{{LeCun}}{{}}}
\bibcite{ma2009lip}{{5}{2009}{{Ma et~al.}}{{}}}
\bibcite{mcgurk1976hearing}{{6}{1976}{{McGurk and MacDonald}}{{}}}
\bibcite{ngiam2011multimodal}{{7}{2011}{{Ngiam et~al.}}{{}}}
\bibcite{samuel1997lexical}{{8}{1997}{{Samuel}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{16}{dummy.68}}
\newlabel{Bibliography}{{6}{16}{Summary}{dummy.68}{}}

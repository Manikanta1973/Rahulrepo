\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter1}{{1}{1}{Introduction}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Multimodal Representation Learning}{1}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The Stirling engine from my desk.}}{2}{figure.5}}
\newlabel{fig:stirling}{{1.1}{2}{The Stirling engine from my desk}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{2}{section.4}}
\citation{smith2008infants}
\citation{yurovsky2013statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces The bi-directional nature of language learning.}}{3}{figure.6}}
\newlabel{fig:bi_ll}{{1.2}{3}{The bi-directional nature of language learning}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Associations among words and objects across multiple ambiguous scenes allow learners to find the proper mapping of words: \textsc  {Circle}, \textsc  {Square}, \textsc  {Green} and \textsc  {Blue} to the shapes and colours: \textit  {Circle}, \textit  {Square}, \textit  {Green} and \textit  {Blue}}}{3}{figure.7}}
\newlabel{fig:cross_sitch}{{1.3}{3}{Associations among words and objects across multiple ambiguous scenes allow learners to find the proper mapping of words: \textsc {Circle}, \textsc {Square}, \textsc {Green} and \textsc {Blue} to the shapes and colours: \textit {Circle}, \textit {Square}, \textit {Green} and \textit {Blue}}{figure.7}{}}
\@writefile{tdo}{\contentsline {todo}{Figure: Placeholder}{4}{figure.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Images generated from labellings of natural images (left) and labellings generated from natural images (right)}}{4}{figure.8}}
\newlabel{fig:mrl_teaser}{{1.4}{4}{Images generated from labellings of natural images (left) and labellings generated from natural images (right)}{figure.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter2}{{2}{5}{Background}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{5}{section.10}}
\newlabel{Lit:Intro}{{2.1}{5}{Introduction}{section.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}What are Artifical Neural Networks good at?}{5}{section.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Classification}{6}{subsection.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Generation}{6}{subsection.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Reinforcement Learning}{6}{subsection.14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}What are Artificial Neural Networks bad at?}{6}{section.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Data Ineffciency}{6}{subsection.16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}How to get off the Symbol Grounding Merry-Go-Round}{6}{section.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}What is symbol grounding?}{6}{subsection.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}How do humans do it?}{6}{subsection.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}How do machines do it?}{6}{subsection.20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Why brains are better}{6}{section.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Embodiment}{6}{subsection.22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1.1}Sensory Redundancy}{6}{subsubsection.23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1.2}Biological Filters}{6}{subsubsection.24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Development}{7}{subsection.25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2.1}Biological Filters, again}{7}{subsubsection.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Pulling yourself up by the bootstraps}{7}{subsection.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Machine Equivelancy}{7}{subsection.28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4.1}How do we simulate Embodiment for ANNs?}{7}{subsubsection.29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4.2}How do we simulate Development for ANNs?}{7}{subsubsection.30}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Where do we go from here?}{7}{section.31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Robot bodies}{7}{subsection.32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}multimodality}{7}{subsection.33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}transfer learning}{7}{subsection.34}}
\citation{rosenblatt1958perceptron}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}A Primer on Artificial Neural Networks}{8}{chapter.35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter3}{{3}{8}{A Primer on Artificial Neural Networks}{chapter.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{8}{section.36}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Perceptrons}{8}{section.37}}
\newlabel{sec:percep}{{3.2}{8}{Perceptrons}{section.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}What is a Perceptron}{8}{subsection.38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A perceptron with three binary inputs, $x_0, x_1, x_2$.}}{8}{figure.39}}
\newlabel{fig:perceptron}{{3.1}{8}{A perceptron with three binary inputs, $x_0, x_1, x_2$}{figure.39}{}}
\newlabel{eqn:percep}{{3.1}{9}{What is a Perceptron}{equation.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Multi-Layer Perceptron}{9}{subsection.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A multi-layer perceptron consisting of three perceptrons arranged in two layers, with three binary inputs, $x_0, x_1, x_2$.}}{10}{figure.42}}
\newlabel{fig:mlp}{{3.2}{10}{A multi-layer perceptron consisting of three perceptrons arranged in two layers, with three binary inputs, $x_0, x_1, x_2$}{figure.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Activation Functions}{10}{section.43}}
\newlabel{eqn:proport}{{3.2}{10}{Activation Functions}{equation.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Visualisation of the perceptron activation function.}}{11}{figure.44}}
\newlabel{fig:activation_percep}{{3.3}{11}{Visualisation of the perceptron activation function}{figure.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Sigmoid Neurons}{11}{subsection.46}}
\newlabel{eqn:sig}{{3.3}{11}{Sigmoid Neurons}{equation.47}{}}
\newlabel{eqn:z_act}{{3.4}{11}{Sigmoid Neurons}{equation.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Visualisation of the sigmoid activation function.}}{12}{figure.49}}
\newlabel{fig:activation_sigmoid}{{3.4}{12}{Visualisation of the sigmoid activation function}{figure.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Visualisation of the hyperbolic tangent activation function.}}{12}{figure.51}}
\newlabel{fig:activation_tanh}{{3.5}{12}{Visualisation of the hyperbolic tangent activation function}{figure.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.1}Other activation functions}{12}{subsubsection.50}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Visualisation of the rectified linear unit activation function.}}{13}{figure.52}}
\newlabel{fig:activation_relu}{{3.6}{13}{Visualisation of the rectified linear unit activation function}{figure.52}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Learning Algorithms}{13}{section.54}}
\citation{lecun1998mnist}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Types of Training}{14}{subsection.56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Cost functions: How wrong am I?}{14}{subsection.57}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.1}Mean Squared Error}{14}{subsubsection.58}}
\newlabel{eqn:mse}{{3.5}{15}{Mean Squared Error}{equation.59}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.2}Cross-Entropy}{15}{subsubsection.60}}
\newlabel{eqn:xntrpy}{{3.6}{15}{Cross-Entropy}{equation.61}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.3}Kullback-Leibler Divergence}{16}{subsubsection.62}}
\newlabel{eqn:kld}{{3.7}{16}{Kullback-Leibler Divergence}{equation.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Gradient Descent}{16}{subsection.64}}
\newlabel{eqn:cost_bivar}{{3.8}{16}{Gradient Descent}{equation.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The cost landscape of a bivariate function}}{17}{figure.65}}
\newlabel{fig:costscape}{{3.7}{17}{The cost landscape of a bivariate function}{figure.65}{}}
\newlabel{eqn:delta_C}{{3.9}{17}{Gradient Descent}{equation.67}{}}
\newlabel{eqn:nabla_c}{{3.10}{17}{Gradient Descent}{equation.68}{}}
\newlabel{eqn:delta_c_sub}{{3.11}{17}{Gradient Descent}{equation.69}{}}
\newlabel{eqn:delta_w_eta}{{3.12}{18}{Gradient Descent}{equation.70}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Backpropagation}{18}{subsection.72}}
\newlabel{eqn:BP1}{{3.13}{18}{Backpropagation}{equation.73}{}}
\newlabel{eqn:BP2}{{3.14}{18}{Backpropagation}{equation.74}{}}
\newlabel{eqn:hada}{{3.15}{18}{}{equation.76}{}}
\newlabel{eqn:BP3}{{3.16}{19}{Backpropagation}{equation.77}{}}
\newlabel{eqn:BP4}{{3.17}{19}{Backpropagation}{equation.78}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Extensions and Improvements}{19}{subsection.79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.5.1}Learning Rate Schedule}{19}{subsubsection.80}}
\newlabel{sec:lr_sch}{{3.4.5.1}{19}{Learning Rate Schedule}{subsubsection.80}{}}
\citation{kingma2014adam}
\citation{zeiler2012adadelta}
\citation{senior2013empirical}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Visualisation of error minimisation in two dimensions for different learning rate schedules. Blue: decreaseing learning rate, Red: large fixed learning rate.}}{20}{figure.81}}
\newlabel{fig:lr_schedule}{{3.8}{20}{Visualisation of error minimisation in two dimensions for different learning rate schedules. Blue: decreaseing learning rate, Red: large fixed learning rate}{figure.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Visualisation of the cost (black), its first derivative (blue) and second derivative (red).}}{21}{figure.82}}
\newlabel{fig:2ndordr}{{3.9}{21}{Visualisation of the cost (black), its first derivative (blue) and second derivative (red)}{figure.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces A Long Short-Term Memory Unit}}{22}{figure.92}}
\newlabel{fig:lstm}{{3.10}{22}{A Long Short-Term Memory Unit}{figure.92}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Convolutional Neural Networks}{22}{section.83}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}What is Convolution?}{22}{subsection.84}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1.1}Kernels}{22}{subsubsection.85}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1.2}Strides}{22}{subsubsection.86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1.3}Dilations}{22}{subsubsection.87}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Transposed Convolutions}{22}{subsection.88}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Recurrent Neural Networks}{22}{section.89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Vanilla RNN}{22}{subsection.90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Gated RNN}{22}{subsection.91}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Autoencoders}{22}{section.94}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces A Gated Recurrent Unit}}{23}{figure.93}}
\newlabel{fig:gru}{{3.11}{23}{A Gated Recurrent Unit}{figure.93}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces A Multimodal Autoencoder}}{23}{figure.97}}
\newlabel{fig:mae}{{3.12}{23}{A Multimodal Autoencoder}{figure.97}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Variational Autoencoders}{23}{subsection.95}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Multimodal Autoencoders}{23}{subsection.96}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Summary}{23}{section.98}}
\citation{barsalou2008grounded}
\citation{mcgurk1976hearing}
\citation{ma2009lip}
\citation{ma2009lip,samuel1997lexical}
\citation{ngiam2011multimodal}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Sensory Redundancy: Are two heads better than one?}{24}{chapter.99}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter4}{{4}{24}{Sensory Redundancy: Are two heads better than one?}{chapter.99}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Why have one sensor when two are better?}{24}{section.100}}
\citation{hammami2009tree,hammami2010improved}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Generating images from natural language}{25}{section.101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Aims}{25}{subsection.102}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}UCU Arabic Spoken Digits and MNIST}{25}{subsection.103}}
\newlabel{sec:UCU}{{4.2.2}{25}{UCU Arabic Spoken Digits and MNIST}{subsection.103}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.1}UCU Arabic Spoken Digits}{25}{subsubsection.104}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.2.1.1}Padding of Utterances}{25}{paragraph.105}}
\citation{lecun1998mnist}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Mean number of samples for each digit in the UCU Arabic Spoken Digits Dataset. Red bars show the standard devaition in length for each each digit.}}{26}{figure.107}}
\newlabel{fig:ucu_dig_length}{{4.1}{26}{Mean number of samples for each digit in the UCU Arabic Spoken Digits Dataset. Red bars show the standard devaition in length for each each digit}{figure.107}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2.2}MNIST Handwritten Digits}{26}{subsubsection.109}}
\citation{barsalou2008grounded}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Mean length and standard deviation of digits in the UCU dataset.}}{27}{table.108}}
\newlabel{tab:UCU_sampLen}{{4.1}{27}{Mean length and standard deviation of digits in the UCU dataset}{table.108}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Problem Description}{27}{subsection.110}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3.1}Classification}{27}{subsubsection.111}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3.2}Bidirectional Symbol Grounding}{27}{subsubsection.112}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Experiment Details}{28}{subsection.113}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.4.0.1}Combining Datasets}{28}{paragraph.114}}
\newlabel{sec:UCU_mnist_comb}{{4.2.4.0.1}{28}{Combining Datasets}{paragraph.114}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.4.0.2}Merging Modalities}{28}{paragraph.115}}
\newlabel{eqn:concat}{{4.1}{28}{Merging Modalities}{equation.116}{}}
\newlabel{eqn:add}{{4.2}{28}{Merging Modalities}{equation.117}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.1}Training Procedures}{28}{subsubsection.118}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.4.1.1}Bimodal}{29}{paragraph.119}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.4.1.2}Randomly Degraded}{29}{paragraph.120}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.2}Testing Conditions}{29}{subsubsection.121}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.4.2.1}Bimodal}{29}{paragraph.122}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.4.2.2}Image Only}{29}{paragraph.123}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.4.2.3}MFCC Only}{30}{paragraph.124}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.3}Network Description}{30}{subsubsection.125}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.4.3.1}Baseline Models}{30}{paragraph.126}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Image autoencoder and classifier. Layer 6c performs classification, whilst the branch starting at layer 6 regenerates the image.}}{30}{table.127}}
\newlabel{tab:MNIST_AE_description}{{4.2}{30}{Image autoencoder and classifier. Layer 6c performs classification, whilst the branch starting at layer 6 regenerates the image}{table.127}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.4.3.2}Multimodal Autoencoder}{30}{paragraph.129}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces MFCC autoencoder and classifier. Layer 7c performs classification, whilst the branch starting at layer 7 regenerates the MFCCs. The addition of reshape layers is to ensure the final shape of the regenerated MFCCs matches the target shape whilst the embedding shape matches that of the embedding of the image autoencoder.}}{31}{table.128}}
\newlabel{tab:UCU_AE_description}{{4.3}{31}{MFCC autoencoder and classifier. Layer 7c performs classification, whilst the branch starting at layer 7 regenerates the MFCCs. The addition of reshape layers is to ensure the final shape of the regenerated MFCCs matches the target shape whilst the embedding shape matches that of the embedding of the image autoencoder}{table.128}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Results}{31}{subsection.131}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5.1}Classification Results}{31}{subsubsection.132}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Image and MFCC multimodal autoencoder. Layers marked i, m, im and c are image, MFCC, image and MFCC and classification respectively.}}{32}{table.130}}
\newlabel{tab:UCU_MNIST_MAE_description}{{4.4}{32}{Image and MFCC multimodal autoencoder. Layers marked i, m, im and c are image, MFCC, image and MFCC and classification respectively}{table.130}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Mean Squared Errors for the Bimodal testing condition.}}{33}{table.133}}
\newlabel{tab:mnist_ucu_bi_res}{{4.5}{33}{Mean Squared Errors for the Bimodal testing condition}{table.133}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Mean Squared Errors for the Image Only testing condition.}}{33}{table.134}}
\newlabel{tab:mnist_ucu_im_res}{{4.6}{33}{Mean Squared Errors for the Image Only testing condition}{table.134}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5.2}Reconstruction Results}{33}{subsubsection.136}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A selection of randomly sampled digits generated in different training and testing conditions for both \textit  {Add} and \textit  {Concat} merging methods.}}{33}{figure.137}}
\newlabel{fig:mnistDigits}{{4.2}{33}{A selection of randomly sampled digits generated in different training and testing conditions for both \textit {Add} and \textit {Concat} merging methods}{figure.137}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Mean Squared Errors for the MFCC Only testing condition.}}{34}{table.135}}
\newlabel{tab:mnist_ucu_mfcc_res}{{4.7}{34}{Mean Squared Errors for the MFCC Only testing condition}{table.135}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Two examples of the digit ``5'' being generated in different training and testing conditions for both \textit  {Add} and \textit  {Concat} merging methods.}}{34}{figure.138}}
\newlabel{fig:5s}{{4.3}{34}{Two examples of the digit ``5'' being generated in different training and testing conditions for both \textit {Add} and \textit {Concat} merging methods}{figure.138}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Discussion}{34}{subsection.139}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.6.1}Discussion of Classification Results}{34}{subsubsection.140}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.6.2}Discussion of Reconstruction Results}{35}{subsubsection.141}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.6.2.1}Image reconstruction from MFCCs}{36}{paragraph.142}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.6.2.2}MFCC reconstruction from Images}{36}{paragraph.143}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.6.2.3}Effects of randomly degrading inputs}{37}{paragraph.144}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.6.2.4}Multiple generations of the same digit}{37}{paragraph.145}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Conclusion}{38}{section.146}}
\citation{mikolov2013distributed}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Magical Vectors and Where to Find Them}{39}{chapter.147}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter5}{{5}{39}{Magical Vectors and Where to Find Them}{chapter.147}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}ANN Latent space: the Final Frontier}{39}{section.148}}
\newlabel{eqn:mikolov}{{5.1}{39}{ANN Latent space: the Final Frontier}{equation.149}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Vector Arithmetic}{40}{section.150}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of how novel images can be generated using Vector Arithmetic on image and word embeddings.}}{41}{figure.151}}
\newlabel{fig:vectorArthexmp}{{5.1}{41}{An example of how novel images can be generated using Vector Arithmetic on image and word embeddings}{figure.151}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Examples of the different, shapes, colours, sizes and positions of objects in the ArtS dataset. The top left object would be described as ``big indigo circle, top left''.}}{42}{figure.152}}
\newlabel{fig:shapes}{{5.2}{42}{Examples of the different, shapes, colours, sizes and positions of objects in the ArtS dataset. The top left object would be described as ``big indigo circle, top left''}{figure.152}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Artificial Shapes Dataset}{42}{section.153}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Dataset Description}{42}{subsection.154}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Artificial Shapes dataset description.}}{43}{table.155}}
\newlabel{tab:Arts_desc}{{5.1}{43}{Artificial Shapes dataset description}{table.155}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Problem Description}{43}{subsection.156}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Network Description}{44}{subsection.157}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiments with the ArtS Dataset}{44}{section.158}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Experiment 1}{45}{subsection.159}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Experiment 1 data subset.}}{45}{table.160}}
\newlabel{tab:exp1_data}{{5.2}{45}{Experiment 1 data subset}{table.160}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1.1}Results}{46}{subsubsection.161}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Experiment 1: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the word output.}}{46}{figure.162}}
\newlabel{fig:graph331}{{5.3}{46}{Experiment 1: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the word output}{figure.162}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Experiment 1: Total MSE for different embedding sizes for the three testing conditions. An Embedding Size of 200 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$)}}{47}{table.163}}
\newlabel{tab:res331}{{5.3}{47}{Experiment 1: Total MSE for different embedding sizes for the three testing conditions. An Embedding Size of 200 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$)}{table.163}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.1.1.1}Image Generation}{47}{paragraph.164}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Images generated from descriptions with an embedding size of 8 or 200.}}{48}{figure.165}}
\newlabel{fig:331multi}{{5.4}{48}{Images generated from descriptions with an embedding size of 8 or 200}{figure.165}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.1.1.2}Description Accuracy}{48}{paragraph.167}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Experiment 1, runs A-D: Images generated of each shape for different sizes of embedding.}}{49}{figure.166}}
\newlabel{fig:331shapes}{{5.5}{49}{Experiment 1, runs A-D: Images generated of each shape for different sizes of embedding}{figure.166}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1.2}Discussion}{49}{subsubsection.168}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Experiment 2}{50}{subsection.169}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Experiment 2 data subset.}}{50}{table.170}}
\newlabel{tab:exp2_data}{{5.4}{50}{Experiment 2 data subset}{table.170}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.1}Results}{50}{subsubsection.171}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Experiment 2: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the word output.}}{51}{figure.172}}
\newlabel{fig:graph333}{{5.6}{51}{Experiment 2: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the word output}{figure.172}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Experiment 2: Total MSE for different embedding sizes. An Embedding Size of 296 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$)}}{51}{table.173}}
\newlabel{tab:res333}{{5.5}{51}{Experiment 2: Total MSE for different embedding sizes. An Embedding Size of 296 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$)}{table.173}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.2.1.1}Image Generation}{52}{paragraph.174}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Experiment 2: Images generated from descriptions with an embedding size of 296 neurons.}}{52}{figure.175}}
\newlabel{fig:333multi}{{5.7}{52}{Experiment 2: Images generated from descriptions with an embedding size of 296 neurons}{figure.175}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Experiment 2 run A: Images generated of each word for different embedding sizes.}}{53}{figure.176}}
\newlabel{fig:333single}{{5.8}{53}{Experiment 2 run A: Images generated of each word for different embedding sizes}{figure.176}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Experiment 2, runs A-D: Images generated of each shape for different emedding sizes.}}{54}{figure.177}}
\newlabel{fig:shapes333}{{5.9}{54}{Experiment 2, runs A-D: Images generated of each shape for different emedding sizes}{figure.177}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.2.1.2}Description Accuracy}{54}{paragraph.179}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.2}Discussion}{54}{subsubsection.180}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Experiment 2, run A: Images generated using word pairs using an embedding size of 296 neurons.}}{55}{figure.178}}
\newlabel{fig:2word333}{{5.10}{55}{Experiment 2, run A: Images generated using word pairs using an embedding size of 296 neurons}{figure.178}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Experiment 3}{55}{subsection.182}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Images generated of each shape for different sizes of embedding using the MAEs trained in experiments 1 and 2 run A.}}{56}{figure.181}}
\newlabel{fig:shapes333v331}{{5.11}{56}{Images generated of each shape for different sizes of embedding using the MAEs trained in experiments 1 and 2 run A}{figure.181}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3.1}Results}{56}{subsubsection.184}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.3.1.1}Image Generation}{56}{paragraph.186}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Experiment 2 data subset.}}{57}{table.183}}
\newlabel{tab:exp3_data}{{5.6}{57}{Experiment 2 data subset}{table.183}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Experiment 3: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$)}}{57}{table.185}}
\newlabel{tab:res339}{{5.7}{57}{Experiment 3: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$)}{table.185}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Experiment 3, run A: Images generated from descriptions, starting training from randomly initialised weights.}}{58}{figure.187}}
\newlabel{fig:339multi}{{5.12}{58}{Experiment 3, run A: Images generated from descriptions, starting training from randomly initialised weights}{figure.187}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Experiment 3, run A: Images generated from shape, colour and size words for different weight initialisation conditions.}}{58}{figure.188}}
\newlabel{fig:339single}{{5.13}{58}{Experiment 3, run A: Images generated from shape, colour and size words for different weight initialisation conditions}{figure.188}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Experiment 3, run A: Images generated from each position word for different weight initialisation conditions.}}{59}{figure.189}}
\newlabel{fig:339single_pos}{{5.14}{59}{Experiment 3, run A: Images generated from each position word for different weight initialisation conditions}{figure.189}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Experiment 3, run A: Images generated using word pairs.}}{59}{figure.190}}
\newlabel{fig:2word339}{{5.15}{59}{Experiment 3, run A: Images generated using word pairs}{figure.190}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.3.1.2}Description Accuracy}{59}{paragraph.191}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Experiment 3: Percentage Description Accuracy for different weight initialisations. }}{59}{table.192}}
\newlabel{tab:res339_acc}{{5.8}{59}{Experiment 3: Percentage Description Accuracy for different weight initialisations}{table.192}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces Experiment 3: Total number of weight updates.}}{60}{table.194}}
\newlabel{tab:updatesTotal}{{5.9}{60}{Experiment 3: Total number of weight updates}{table.194}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3.2}Discussion}{60}{subsubsection.193}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Experiment 4}{61}{subsection.195}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.4.1}Results}{61}{subsubsection.196}}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces Experiment 4: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$)}}{61}{table.197}}
\newlabel{tab:res739}{{5.10}{61}{Experiment 4: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$)}{table.197}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.4.1.1}Image Generation}{61}{paragraph.198}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Experiment 4, run A: Images generated from shape, colour and size words for different weight initialisation conditions.}}{62}{figure.199}}
\newlabel{fig:739single_shape}{{5.16}{62}{Experiment 4, run A: Images generated from shape, colour and size words for different weight initialisation conditions}{figure.199}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Experiment 4, run A: Images generated from each colour word for different weight initialisation conditions.}}{62}{figure.200}}
\newlabel{fig:739single_col}{{5.17}{62}{Experiment 4, run A: Images generated from each colour word for different weight initialisation conditions}{figure.200}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Experiment 4, run A: Images generated from each position word for different weight initialisation conditions.}}{63}{figure.201}}
\newlabel{fig:739single_pos}{{5.18}{63}{Experiment 4, run A: Images generated from each position word for different weight initialisation conditions}{figure.201}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Experiment 4, run A: Images generated using word pairs.}}{63}{figure.202}}
\newlabel{fig:2word739}{{5.19}{63}{Experiment 4, run A: Images generated using word pairs}{figure.202}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.4.1.2}Description Accuracy}{64}{paragraph.203}}
\@writefile{lot}{\contentsline {table}{\numberline {5.11}{\ignorespaces Experiment 4: Percentage Description Accuracy for different weight initialisations.}}{64}{table.204}}
\newlabel{tab:res739acc}{{5.11}{64}{Experiment 4: Percentage Description Accuracy for different weight initialisations}{table.204}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.4.2}Discussion}{64}{subsubsection.205}}
\@writefile{lot}{\contentsline {table}{\numberline {5.12}{\ignorespaces Experiment 4: Total number of weight updates.}}{65}{table.206}}
\newlabel{tab:updatesTotal4}{{5.12}{65}{Experiment 4: Total number of weight updates}{table.206}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.13}{\ignorespaces Experiment 4: Mean Squared Error ($\times 10^{-3}$) and Percentage Description Accuracy for the randomly initialised MAE after 50 epochs of training.}}{65}{table.207}}
\newlabel{tab:res739_50}{{5.13}{65}{Experiment 4: Mean Squared Error ($\times 10^{-3}$) and Percentage Description Accuracy for the randomly initialised MAE after 50 epochs of training}{table.207}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.5}Experiment 5}{66}{subsection.208}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.5.1}Results}{66}{subsubsection.209}}
\@writefile{lot}{\contentsline {table}{\numberline {5.14}{\ignorespaces Experiment 5: Total MSE. Alternating rows show MSE for the datasubset without the omitted data and MSE for only the omitted data (marked with *). (All values are $\times 10^{-3}$)}}{66}{table.210}}
\newlabel{tab:res_exp5}{{5.14}{66}{Experiment 5: Total MSE. Alternating rows show MSE for the datasubset without the omitted data and MSE for only the omitted data (marked with *). (All values are $\times 10^{-3}$)}{table.210}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.5.1.1}Image Generation}{67}{paragraph.211}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Experiment 5, run A: Images generated from descriptions of shapes never seen by the neural network.}}{67}{figure.212}}
\newlabel{fig:739_minus1}{{5.20}{67}{Experiment 5, run A: Images generated from descriptions of shapes never seen by the neural network}{figure.212}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Experiment 5, run A: Images generated via Vector Arithmetic}}{68}{figure.213}}
\newlabel{fig:739_vectorArth}{{5.21}{68}{Experiment 5, run A: Images generated via Vector Arithmetic}{figure.213}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {5.4.5.1.2}Description Accuracy}{68}{paragraph.214}}
\@writefile{lot}{\contentsline {table}{\numberline {5.15}{\ignorespaces Experiment 5: Percentage Description Accuracy. Alternating rows show accuracy for the datasubset without the omitted data and MSE for only the omitted data (marked with *)}}{69}{table.215}}
\newlabel{tab:res_exp5_acc}{{5.15}{69}{Experiment 5: Percentage Description Accuracy. Alternating rows show accuracy for the datasubset without the omitted data and MSE for only the omitted data (marked with *)}{table.215}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.5.2}Discussion}{69}{subsubsection.216}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusion}{70}{section.217}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.0.1}How an incremental approach helps}{70}{subsubsection.218}}
\@writefile{lot}{\contentsline {table}{\numberline {5.16}{\ignorespaces Probability distributions generated from rolling a 6 sided die.}}{71}{table.219}}
\newlabel{tab:dieProb}{{5.16}{71}{Probability distributions generated from rolling a 6 sided die}{table.219}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Visualisation of an imaginary Error landscape.}}{72}{figure.220}}
\newlabel{fig:localminima}{{5.22}{72}{Visualisation of an imaginary Error landscape}{figure.220}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bidirectional Grounding of Real Data}{74}{chapter.221}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter6}{{6}{74}{Bidirectional Grounding of Real Data}{chapter.221}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}The Difficulties of Real Data}{74}{section.222}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Real Shapes Dataset}{75}{section.223}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}dataset description}{75}{subsection.224}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}problem description}{75}{subsection.225}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}network description}{75}{subsection.226}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}results}{75}{subsection.227}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4.1}Image Generation}{75}{subsubsection.228}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4.2}Multilabel classification}{75}{subsubsection.229}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4.3}Vector Arithematic}{75}{subsubsection.230}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}discussion}{75}{subsection.231}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}What have we learnt and more importantly, what have our neural nets learnt?}{76}{chapter.232}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter7}{{7}{76}{What have we learnt and more importantly, what have our neural nets learnt?}{chapter.232}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary of important points}{76}{section.233}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Conclusion}{76}{section.234}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Future Work}{77}{chapter.235}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter8}{{8}{77}{Future Work}{chapter.235}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Where do we go now?}{77}{section.236}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}More Sensors}{77}{subsection.237}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Better Sensors}{77}{subsection.238}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Swarm Robots and the Cloud}{77}{subsection.239}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix \emph  {Sensory Redundancy}}{78}{appendix.240}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix:A}{{A}{78}{Appendix \emph {Sensory Redundancy}}{appendix.240}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Results from the combined MNIST Handwritten Digits and UCU Arabic Spoken Digits}}{79}{table.241}}
\newlabel{tab:mnist_ucu_master_res}{{A.1}{79}{Results from the combined MNIST Handwritten Digits and UCU Arabic Spoken Digits}{table.241}{}}
\bibdata{bigone}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Appendix \emph  {Magical Vectors and where to find them}}{80}{appendix.242}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix:B}{{B}{80}{Appendix \emph {Magical Vectors and where to find them}}{appendix.242}{}}
\newlabel{eqn:updates}{{B.1}{80}{Appendix \emph {Magical Vectors and where to find them}}{equation.244}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Image and Word multimodal autoencoder. Layers marked i, w, and iw are image, word, and image and word respectively. The number of neurons, ``emb size'', in the Embedding block is varied in some experiments.}}{81}{table.243}}
\newlabel{tab:Arts_MAE_description}{{B.1}{81}{Image and Word multimodal autoencoder. Layers marked i, w, and iw are image, word, and image and word respectively. The number of neurons, ``emb size'', in the Embedding block is varied in some experiments}{table.243}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Number of weight updates per Epoch for each experiment.}}{81}{table.245}}
\newlabel{tab:updatesperEpoch}{{B.2}{81}{Number of weight updates per Epoch for each experiment}{table.245}{}}
\bibcite{smith2008infants}{{1}{}{{}}{{}}}
\bibcite{yurovsky2013statistical}{{2}{}{{}}{{}}}
\bibcite{rosenblatt1958perceptron}{{3}{}{{}}{{}}}
\bibcite{lecun1998mnist}{{4}{}{{}}{{}}}
\bibcite{kingma2014adam}{{5}{}{{}}{{}}}
\bibcite{zeiler2012adadelta}{{6}{}{{}}{{}}}
\bibcite{senior2013empirical}{{7}{}{{}}{{}}}
\bibcite{barsalou2008grounded}{{8}{}{{}}{{}}}
\bibcite{mcgurk1976hearing}{{9}{}{{}}{{}}}
\bibcite{ma2009lip}{{10}{}{{}}{{}}}
\bibcite{samuel1997lexical}{{11}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{82}{appendix*.246}}
\bibcite{ngiam2011multimodal}{{12}{}{{}}{{}}}
\bibcite{hammami2009tree}{{13}{}{{}}{{}}}
\bibcite{hammami2010improved}{{14}{}{{}}{{}}}
\bibcite{mikolov2013distributed}{{15}{}{{}}{{}}}
\bibcite{mikolov2013efficient}{{16}{}{{}}{{}}}
\bibstyle{latexeu}
\providecommand\NAT@force@numbers{}\NAT@force@numbers

\relax 
\providecommand\hyper@newdestlabel[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Acronyms}{i}{section.1}}
\newacro{AE}[\AC@hyperlink{AE}{AE}]{Autoencoder}
\newacro{AP}[\AC@hyperlink{AP}{AP}]{Acoustic Packaging}
\newacro{ASR}[\AC@hyperlink{ASR}{ASR}]{Automatic Speech Recognition}
\newacro{ANN}[\AC@hyperlink{ANN}{ANN}]{Artificial Neural Network}
\newacro{ConvNet}[\AC@hyperlink{ConvNet}{ConvNet}]{Convolutional Neural Network}
\newacro{CPU}[\AC@hyperlink{CPU}{CPU}]{Central Processing Unit}
\newacro{DL}[\AC@hyperlink{DL}{DL}]{Deep Learning}
\newacro{FSM}[\AC@hyperlink{FSM}{FSM}]{Finite State Machine}
\newacro{GAN}[\AC@hyperlink{GAN}{GAN}]{Generative Adversarial Network}
\newacro{GPU}[\AC@hyperlink{GPU}{GPU}]{Graphics Processing Unit}
\newacro{GRU}[\AC@hyperlink{GRU}{GRU}]{Gated Recurrent Unit}
\newacro{HRI}[\AC@hyperlink{HRI}{HRI}]{Human Robot Interaction}
\newacro{KLD}[\AC@hyperlink{KLD}{KLD}]{Kullback-Leibler Divergence}
\newacro{LSTM}[\AC@hyperlink{LSTM}{LSTM}]{Long Short-Term Memory}
\newacro{MAE}[\AC@hyperlink{MAE}{MAE}]{Multimodal Autoencoder}
\newacro{MFCC}[\AC@hyperlink{MFCC}{MFCC}]{Mel Frequency Cepstrum Coefficient}
\newacro{MLP}[\AC@hyperlink{MLP}{MLP}]{Multi-Layer Perceptron}
\newacro{MRL}[\AC@hyperlink{MRL}{MRL}]{Multimodal Representation Learning}
\newacro{MSE}[\AC@hyperlink{MSE}{MSE}]{Mean Squared Error}
\newacro{NLP}[\AC@hyperlink{NLP}{NLP}]{Natural Language Processing}
\newacro{NLU}[\AC@hyperlink{NLU}{NLU}]{Natural Language Understanding}
\newacro{PRNG}[\AC@hyperlink{PRNG}{PRNG}]{Pseudo Random Number Generator}
\newacro{Relu}[\AC@hyperlink{Relu}{Relu}]{Rectified Linear Unit}
\newacro{RGB}[\AC@hyperlink{RGB}{RGB}]{Red Green Blue}
\newacro{RNN}[\AC@hyperlink{RNN}{RNN}]{Recurrent Neural Network}
\newacro{SVM}[\AC@hyperlink{SVM}{SVM}]{Support Vector Machine}
\newacro{Tanh}[\AC@hyperlink{Tanh}{Tanh}]{Hyperbolic Tangent}
\newacro{TPT}[\AC@hyperlink{TPT}{TPT}]{Two Process Theory}
\newacro{VAE}[\AC@hyperlink{VAE}{VAE}]{Variational Autoencoder}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter1}{{1}{1}{Introduction}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Multimodal Representation Learning}{1}{section.4}}
\AC@undonewlabel{acro:MRL}
\newlabel{acro:MRL}{{1.1}{1}{Multimodal Representation Learning}{section*.5}{}}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The Stirling engine from my desk.}}{2}{figure.7}}
\newlabel{fig:stirling}{{1.1}{2}{The Stirling engine from my desk}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{2}{section.6}}
\acronymused{MRL}
\citation{smith2008infants}
\citation{yurovsky2013statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces The bi-directional nature of language learning.}}{3}{figure.8}}
\newlabel{fig:bi_ll}{{1.2}{3}{The bi-directional nature of language learning}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Associations among words and objects across multiple ambiguous scenes allow learners to find the proper mapping of words: \textsc  {Circle}, \textsc  {Square}, \textsc  {Green} and \textsc  {Blue} to the shapes and colours: \textit  {Circle}, \textit  {Square}, \textit  {Green} and \textit  {Blue}}}{3}{figure.9}}
\newlabel{fig:cross_sitch}{{1.3}{3}{Associations among words and objects across multiple ambiguous scenes allow learners to find the proper mapping of words: \textsc {Circle}, \textsc {Square}, \textsc {Green} and \textsc {Blue} to the shapes and colours: \textit {Circle}, \textit {Square}, \textit {Green} and \textit {Blue}}{figure.9}{}}
\citation{repRev}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Images generated from textual descriptions (B, M and S are Big, Medium and Small, respectively).}}{4}{figure.10}}
\newlabel{fig:mrl_teaser}{{1.4}{4}{Images generated from textual descriptions (B, M and S are Big, Medium and Small, respectively)}{figure.10}{}}
\acronymused{MRL}
\acronymused{MRL}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Hypotheses}{4}{section.11}}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\AC@undonewlabel{acro:MAE}
\newlabel{acro:MAE}{{5}{4}{Hypotheses}{section*.17}{}}
\acronymused{MAE}
\acronymused{MRL}
\newlabel{fig:final_arch}{{1.4}{5}{Methodology}{section.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces \leavevmode {\color  {red}The architecture of the MAE used in \autoref  {Chapter6} to generate the images seen in \autoref  {fig:mrl_teaser}. Layers marked with C are convolution TC, Transposed Convolution, RS, Reshape and N, Dense.}}}{5}{figure.22}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Methodology}{5}{section.19}}
\acronymused{MAE}
\acronymused{MAE}
\AC@undonewlabel{acro:VAE}
\newlabel{acro:VAE}{{1.4}{5}{Methodology}{section*.20}{}}
\acronymused{VAE}
\AC@undonewlabel{acro:ANN}
\newlabel{acro:ANN}{{1.4}{5}{Methodology}{section*.21}{}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{MAE}
\acronymused{MAE}
\citation{rosenblatt1958perceptron}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}A Primer on Artificial Neural Networks}{7}{chapter.23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter2}{{2}{7}{A Primer on Artificial Neural Networks}{chapter.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{7}{section.24}}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Perceptrons}{7}{section.25}}
\newlabel{sec:percep}{{2.2}{7}{Perceptrons}{section.25}{}}
\acronymused{ANN}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A perceptron with three binary inputs, $x_0, x_1, x_2$.}}{7}{figure.27}}
\newlabel{fig:perceptron}{{2.1}{7}{A perceptron with three binary inputs, $x_0, x_1, x_2$}{figure.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}What is a Perceptron}{8}{subsection.26}}
\newlabel{eqn:percep}{{2.1}{8}{What is a Perceptron}{equation.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Multi-Layer Perceptron}{8}{subsection.29}}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.2.2}{8}{Multi-Layer Perceptron}{section*.30}{}}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \leavevmode {\color  {red}A multi-layer perceptron consisting of three perceptrons arranged in two layers, with two binary inputs, $x_0, x_1$.}}}{8}{figure.31}}
\newlabel{fig:mlp}{{2.2}{8}{\textcolor {red}{A multi-layer perceptron consisting of three perceptrons arranged in two layers, with two binary inputs, $x_0, x_1$.}}{figure.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{8}{section.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Visualisation of the perceptron activation function. Where z is the activation $\DOTSB \sum@ \slimits@ _{j} w_j x_j + b$ and the output is the activation function defined in \autoref  {eqn:percep} }}{9}{figure.33}}
\newlabel{fig:activation_percep}{{2.3}{9}{Visualisation of the perceptron activation function. Where z is the activation $\sum _{j} w_j x_j + b$ and the output is the activation function defined in \autoref {eqn:percep}}{figure.33}{}}
\newlabel{eqn:proport}{{2.2}{9}{Activation Functions}{equation.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Sigmoid Neurons}{9}{subsection.35}}
\newlabel{eqn:sig}{{2.3}{9}{Sigmoid Neurons}{equation.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Visualisation of the Sigmoid activation function. Where z is the activation $\DOTSB \sum@ \slimits@ _{j} w_j x_j + b$ and $\sigma $(z) is the activation function defined in \autoref  {eqn:sig} }}{10}{figure.38}}
\newlabel{fig:activation_sigmoid}{{2.4}{10}{Visualisation of the Sigmoid activation function. Where z is the activation $\sum _{j} w_j x_j + b$ and $\sigma $(z) is the activation function defined in \autoref {eqn:sig}}{figure.38}{}}
\newlabel{eqn:z_act}{{2.4}{10}{Sigmoid Neurons}{equation.37}{}}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Other activation functions}{10}{subsection.39}}
\AC@undonewlabel{acro:Tanh}
\newlabel{acro:Tanh}{{2.3.2}{10}{Other activation functions}{section*.40}{}}
\acronymused{Tanh}
\AC@undonewlabel{acro:Relu}
\newlabel{acro:Relu}{{2.3.2}{10}{Other activation functions}{section*.41}{}}
\acronymused{Relu}
\acronymused{Tanh}
\citation{hochreiter1998vanishing}
\citation{bengio2007greedy}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualisation of the hyperbolic tangent activation function. Where z is the activation $\DOTSB \sum@ \slimits@ _{j} w_j x_j + b$ and $\sigma $(z) is the Hyperbolic Tangent Function applied to z.}}{11}{figure.42}}
\newlabel{fig:activation_tanh}{{2.5}{11}{Visualisation of the hyperbolic tangent activation function. Where z is the activation $\sum _{j} w_j x_j + b$ and $\sigma $(z) is the Hyperbolic Tangent Function applied to z}{figure.42}{}}
\acronymused{Relu}
\acronymused{Tanh}
\acronymused{Tanh}
\acronymused{MAE}
\acronymused{Relu}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Learning Algorithms}{11}{section.44}}
\citation{lecun1998mnist}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Visualisation of the rectified linear unit activation function.Where z is the activation $\DOTSB \sum@ \slimits@ _{j} w_j x_j + b$ and $\sigma $(z) is the \ac {Relu} activation function applied to z.}}{12}{figure.43}}
\acronymused{Relu}
\newlabel{fig:activation_relu}{{2.6}{12}{Visualisation of the rectified linear unit activation function.Where z is the activation $\sum _{j} w_j x_j + b$ and $\sigma $(z) is the \ac {Relu} activation function applied to z}{figure.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Types of Training}{12}{subsection.45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Cost functions}{13}{subsection.46}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Squared Error}{13}{section*.47}}
\AC@undonewlabel{acro:MSE}
\newlabel{acro:MSE}{{2.4.2}{13}{Mean Squared Error}{section*.48}{}}
\acronymused{MSE}
\newlabel{eqn:mse}{{2.5}{13}{Mean Squared Error}{equation.49}{}}
\acronymused{MSE}
\acronymused{MSE}
\@writefile{toc}{\contentsline {subsubsection}{Cross-Entropy}{13}{section*.50}}
\newlabel{eqn:xntrpy}{{2.6}{14}{Cross-Entropy}{equation.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{Kullback-Leibler Divergence}{14}{section*.52}}
\AC@undonewlabel{acro:KLD}
\newlabel{acro:KLD}{{2.4.2}{14}{Kullback-Leibler Divergence}{section*.53}{}}
\acronymused{KLD}
\acronymused{KLD}
\newlabel{eqn:kld}{{2.7}{14}{Kullback-Leibler Divergence}{equation.54}{}}
\acronymused{KLD}
\acronymused{KLD}
\acronymused{KLD}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Gradient Descent}{14}{subsection.55}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The cost landscape of a bivariate function. Where $w_0$ and $w_1$ are the weights of a neural network and $C$ is its error.}}{15}{figure.56}}
\newlabel{fig:costscape}{{2.7}{15}{The cost landscape of a bivariate function. Where $w_0$ and $w_1$ are the weights of a neural network and $C$ is its error}{figure.56}{}}
\newlabel{eqn:cost_bivar}{{2.8}{15}{Gradient Descent}{equation.57}{}}
\newlabel{eqn:delta_C}{{2.9}{15}{Gradient Descent}{equation.58}{}}
\newlabel{eqn:nabla_c}{{2.10}{15}{Gradient Descent}{equation.59}{}}
\newlabel{eqn:delta_c_sub}{{2.11}{16}{Gradient Descent}{equation.60}{}}
\newlabel{eqn:delta_w_eta}{{2.12}{16}{Gradient Descent}{equation.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Backpropagation}{16}{subsection.62}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Convolutional Neural Networks}{16}{section.63}}
\acronymused{MLP}
\citation{dumoulin2016guide}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A british short-hair cat (left) and the same image, with its pixels scrambled (right).}}{17}{figure.64}}
\newlabel{fig:cat}{{2.8}{17}{A british short-hair cat (left) and the same image, with its pixels scrambled (right)}{figure.64}{}}
\AC@undonewlabel{acro:ConvNet}
\newlabel{acro:ConvNet}{{2.5}{17}{Convolutional Neural Networks}{section*.65}{}}
\acronymused{ConvNet}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}What is Convolution?}{17}{subsection.66}}
\acronymused{ConvNet}
\newlabel{eqn:convolution}{{2.13}{17}{What is Convolution?}{equation.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces A single step of computing a convolution }}{18}{figure.68}}
\newlabel{fig:oneStepConv}{{2.9}{18}{A single step of computing a convolution}{figure.68}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Using Convolution Kernels in a Neural Network}{18}{subsection.71}}
\acronymused{ConvNet}
\acronymused{ConvNet}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces An example of convolution of a (3x3) array with a (3x3) kernel.}}{19}{figure.69}}
\newlabel{fig:convolve}{{2.10}{19}{An example of convolution of a (3x3) array with a (3x3) kernel}{figure.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces The implementation of the convolution of a (3x3) array with a (3x3) kernel.}}{20}{figure.70}}
\newlabel{fig:convImpl}{{2.11}{20}{The implementation of the convolution of a (3x3) array with a (3x3) kernel}{figure.70}{}}
\citation{radford2015unsupervised}
\citation{zeiler2010deconvolutional}
\@writefile{toc}{\contentsline {subsubsection}{Strided Convolution}{21}{section*.72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Transposed Convolutions}{21}{subsection.73}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces The implementation of the transposed convolution of a (3x3) array with a (3x3) kernel.}}{22}{figure.74}}
\newlabel{fig:transConvImpl}{{2.12}{22}{The implementation of the transposed convolution of a (3x3) array with a (3x3) kernel}{figure.74}{}}
\citation{kingma2013auto}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces An \ac {AE} trained using the \ac {MSE} of its input and reconstruction of the input (Input*). The red arrow shows how the error is backpropgated through the network.}}{23}{figure.78}}
\acronymused{AE}
\acronymused{MSE}
\newlabel{fig:ae}{{2.13}{23}{An \ac {AE} trained using the \ac {MSE} of its input and reconstruction of the input (Input*). The red arrow shows how the error is backpropgated through the network}{figure.78}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Autoencoders}{23}{section.75}}
\AC@undonewlabel{acro:AE}
\newlabel{acro:AE}{{2.6}{23}{Autoencoders}{section*.76}{}}
\acronymused{AE}
\acronymused{ConvNet}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{2.6}{23}{Autoencoders}{section*.77}{}}
\acronymused{LSTM}
\acronymused{AE}
\acronymused{AE}
\acronymused{AE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Variational Autoencoders}{23}{subsection.79}}
\acronymused{AE}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{MSE}
\acronymused{KLD}
\acronymused{AE}
\acronymused{VAE}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces A Multimodal Autoencoder}}{24}{figure.81}}
\newlabel{fig:mae}{{2.14}{24}{A Multimodal Autoencoder}{figure.81}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Multimodal Autoencoders}{24}{subsection.80}}
\acronymused{MAE}
\acronymused{AE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\citation{ngiam2011multimodal}
\citation{mcgurk1976hearing}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MRL}
\acronymused{MAE}
\acronymused{MRL}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\citation{rosenblatt1958perceptron}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background}{26}{chapter.82}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter3}{{3}{26}{Background}{chapter.82}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{26}{section.83}}
\newlabel{Lit:Intro}{{3.1}{26}{Introduction}{section.83}{}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{MRL}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}What are Artifical Neural Networks Good at?}{26}{section.84}}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{ANN}
\acronymused{MLP}
\acronymused{ANN}
\acronymused{ANN}
\AC@undonewlabel{acro:GPU}
\newlabel{acro:GPU}{{3.2}{26}{What are Artifical Neural Networks Good at?}{section*.85}{}}
\acronymused{GPU}
\citation{vinyals2019alphastar}
\citation{ma2004facial}
\citation{levi2015emotion}
\citation{kussul2017deep,qi2017pointnet}
\citation{lohan2016distinguishing,sheppard2017understanding,lohan2018toward}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\citation{springenberg2014striving}
\acronymused{ANN}
\acronymused{GPU}
\acronymused{ANN}
\acronymused{ANN}
\AC@undonewlabel{acro:DL}
\newlabel{acro:DL}{{3.2}{27}{What are Artifical Neural Networks Good at?}{section*.86}{}}
\acronymused{DL}
\acronymused{ANN}
\acronymused{DL}
\AC@undonewlabel{acro:NLP}
\newlabel{acro:NLP}{{3.2}{27}{What are Artifical Neural Networks Good at?}{section*.87}{}}
\acronymused{NLP}
\acronymused{MRL}
\acronymused{NLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Classification}{27}{subsection.88}}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsubsection}{AlexNet}{27}{section*.89}}
\acronymused{ConvNet}
\citation{srivastava2014dropout}
\citation{simonyan2014very}
\citation{he2016deep}
\citation{hochreiter1998vanishing}
\citation{huang2017densely}
\citation{szegedy2015going}
\citation{szegedy2015going}
\citation{szegedy2016rethinking,szegedy2017inception}
\citation{gregor2015draw}
\citation{lecun1998mnist}
\citation{netzer2011reading}
\@writefile{toc}{\contentsline {subsubsection}{Residual Connections}{28}{section*.90}}
\@writefile{toc}{\contentsline {subsubsection}{Inception: Multiscale Convolutions}{28}{section*.91}}
\citation{mirza2014conditional,odena2017conditional}
\citation{vinyals2015show,lebret2015phrase,donahue2015long,jia2015guiding,rohrbach2014coherent,rohrbach2013translating,yao2015describing,yao2015video,venugopalan2014translating,johnson2016densecap,ordonez2011im2text,sheppard2016video}
\citation{zhu2017unpaired}
\citation{lu2013speech}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Recurrent Neural Networks}{29}{subsection.92}}
\acronymused{ANN}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Generation}{29}{subsection.93}}
\AC@undonewlabel{acro:GAN}
\newlabel{acro:GAN}{{3.2.3}{29}{Generation}{section*.94}{}}
\acronymused{GAN}
\citation{mirza2014conditional}
\citation{odena2017conditional}
\citation{mirza2014conditional}
\citation{zhu2017unpaired}
\citation{zhu2017unpaired}
\citation{reed2016generative}
\acronymused{AE}
\acronymused{AE}
\@writefile{toc}{\contentsline {subsubsection}{Generative Adversarial Networks}{30}{section*.95}}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{MAE}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\citation{hochreiter1997long}
\citation{vinyals2015show,venugopalan2014translating,johnson2016densecap}
\citation{johnson2016densecap}
\citation{simonyan2014very}
\citation{keller}
\citation{donahue2015long}
\citation{yao2015describing,yao2015video}
\citation{rohrbach2014coherent,rohrbach2013translating}
\acronymused{GAN}
\acronymused{MAE}
\acronymused{GAN}
\acronymused{MAE}
\acronymused{GAN}
\@writefile{toc}{\contentsline {subsubsection}{Modality Translation: Image Caption Generation}{31}{section*.96}}
\acronymused{GAN}
\acronymused{ConvNet}
\acronymused{LSTM}
\acronymused{ConvNet}
\acronymused{ConvNet}
\acronymused{LSTM}
\acronymused{ConvNet}
\acronymused{ConvNet}
\acronymused{ANN}
\citation{mansimov2015generating}
\citation{mansimov2015generating}
\citation{zhang2017stackgan}
\citation{krizhevsky2012imagenet}
\citation{radford2015unsupervised,silberer2014learning,wavenet,vincent2010stacked,mikolov2013distributed,mikolov2013efficient,mikolov2013linguistic,feng2010visual,eslami2018neural,donahue2019large}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Generating Images from Captions}{32}{subsection.97}}
\citation{repRev}
\citation{vinyals2015show,venugopalan2014translating,johnson2016densecap}
\citation{johnson2016densecap}
\citation{simonyan2014very}
\citation{mikolov2013distributed,mikolov2013efficient,mikolov2013linguistic}
\citation{repRev}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Representation Learning}{33}{subsection.98}}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsubsection}{Natural Language Processing}{33}{section*.99}}
\acronymused{NLP}
\citation{repRev}
\citation{pu2016variational}
\citation{lin2014microsoft}
\citation{simonyan2014very}
\citation{szegedy2015going}
\citation{simonyan2014very,szegedy2015going}
\citation{repRev}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Autoencoders}{34}{section*.100}}
\acronymused{AE}
\acronymused{AE}
\acronymused{VAE}
\AC@undonewlabel{acro:SVM}
\newlabel{acro:SVM}{{3.2.5}{34}{Autoencoders}{section*.101}{}}
\acronymused{SVM}
\acronymused{VAE}
\acronymused{GPU}
\acronymused{VAE}
\acronymused{SVM}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{3.2.5}{34}{Autoencoders}{section*.102}{}}
\acronymused{RNN}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{VAE}
\citation{bleu}
\citation{wavenet}
\citation{radford2015unsupervised}
\citation{van2017neural}
\citation{vincent2010stacked,lu2013speech}
\acronymused{VAE}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{AE}
\citation{wavenet}
\citation{dunbar2017zero}
\citation{wavenet}
\citation{van2017neural}
\citation{wavenet}
\acronymused{AE}
\citation{vinyals2019alphastar}
\citation{krizhevsky2012imagenet,simonyan2014very,szegedy2015going,szegedy2016rethinking,szegedy2017inception,he2016deep,huang2017densely,russakovsky2015imagenet,chiu2018state,eslami2018neural}
\citation{nguyen2015deep}
\citation{szegedy2013intriguing}
\citation{moosavi2016deepfool}
\citation{nguyen2015deep}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{ANN}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}What are Artificial Neural Networks Bad at?}{37}{section.103}}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Artificial Neural Networks are Easily Fooled}{37}{subsection.104}}
\acronymused{ANN}
\citation{barsalou2008grounded}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A cat that looks like a crow.}}{38}{figure.105}}
\newlabel{fig:catcrow}{{3.1}{38}{A cat that looks like a crow}{figure.105}{}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ConvNet}
\acronymused{ConvNet}
\citation{wavenet}
\citation{krizhevsky2012imagenet}
\citation{vinyals2015show}
\citation{ImNet}
\citation{ha2018world,ha2018recurrent}
\citation{ha2018world,ha2018recurrent}
\citation{vinyals2016matching}
\citation{repRev}
\citation{searle1980minds}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Data Ineffciency}{39}{subsection.106}}
\acronymused{ANN}
\acronymused{GPU}
\AC@undonewlabel{acro:CPU}
\newlabel{acro:CPU}{{3.3.2}{39}{Data Ineffciency}{section*.107}{}}
\acronymused{CPU}
\acronymused{ANN}
\acronymused{ANN}
\citation{turing2009computing}
\citation{cangelosi2000robotic}
\citation{harnad1990symbol}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}How to get off the Symbol Grounding Merry-Go-Round}{40}{section.108}}
\acronymused{MAE}
\citation{steels2008symbol}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A Semiotic Network.}}{41}{figure.110}}
\newlabel{fig:semNet}{{3.2}{41}{A Semiotic Network}{figure.110}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}What is Symbol Grounding?}{41}{subsection.109}}
\citation{barsalou2008grounded}
\citation{slater1999intermodal}
\citation{masland2012neuronal,fantz1963pattern,johnson2015two}
\citation{webb2015mother,reid2017human}
\citation{johnson2015two}
\citation{pfeifer2006body,smith2005development}
\citation{barsalou2008grounded}
\citation{nicolelis1995sensorimotor}
\citation{mordvintsev2015inceptionism}
\acronymused{MRL}
\acronymused{MRL}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}How do Humans do it?}{42}{subsection.111}}
\acronymused{ANN}
\citation{glenberg2015few}
\citation{barsalou2008grounded}
\citation{glenberg2015few}
\citation{barsalou2008grounded}
\citation{pfeifer2006body}
\citation{Goldin-MeadowSusan2015Fata,de2014making,rucinski2012robotic}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Embodiment}{43}{section*.112}}
\newlabel{sec:embodi}{{3.4.2}{43}{Embodiment}{section*.112}{}}
\citation{bahrick2000intersensory}
\citation{smith2008infants}
\citation{walker2010preverbal,fischer2011multi,scott20122,slater1999intermodal}
\@writefile{toc}{\contentsline {paragraph}{Multimodality}{44}{section*.113}}
\acronymused{MRL}
\@writefile{toc}{\contentsline {paragraph}{Development}{44}{section*.114}}
\acronymused{ANN}
\acronymused{ANN}
\citation{webb2015mother}
\citation{fantz1963pattern}
\citation{reid2017human}
\citation{fernald1987acoustic}
\citation{lohan2012contingency,lohan2012tutor}
\citation{pezzulo2013computational}
\citation{johnson2015two}
\citation{bambach2017egocentric}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{MRL}
\@writefile{toc}{\contentsline {paragraph}{Biological Filters}{45}{section*.115}}
\AC@undonewlabel{acro:TPT}
\newlabel{acro:TPT}{{3.4.2}{45}{Biological Filters}{section*.116}{}}
\acronymused{TPT}
\acronymused{TPT}
\acronymused{TPT}
\citation{bambach2017egocentric}
\citation{masland2012neuronal}
\citation{olveczky2003segregation}
\citation{oxenham2018we}
\citation{webb2015mother}
\citation{barsalou2008grounded}
\citation{cangelosi2000robotic}
\citation{lemonlearning,yu2017learning}
\acronymused{TPT}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\citation{lemonlearning}
\citation{yu2017learning}
\citation{coradeschi2000anchoring,coradeschi2003introduction}
\citation{coradeschi2013short}
\citation{vogt2002physical}
\citation{cangelosi2006grounding}
\citation{coradeschi2013short}
\citation{lemonlearning,yu2017learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A depiction of Anchoring relative to a Semiotic Network.}}{47}{figure.118}}
\newlabel{fig:ank}{{3.3}{47}{A depiction of Anchoring relative to a Semiotic Network}{figure.118}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}How do Machines do it?}{47}{subsection.117}}
\acronymused{NLP}
\acronymused{NLP}
\acronymused{NLP}
\citation{coradeschi2013short}
\citation{nakamura2009grounding,nakamura2011grounding}
\citation{cangelosi2000robotic}
\citation{cangelosi2000robotic}
\citation{cangelosi2001adaptive,cangelosi2002symbol,cangelosi1998emergence,horst2019object,cangelosi2018speech,cangelosi2008italk,broz2014italk}
\citation{cangelosi1998emergence}
\acronymused{MRL}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\citation{vinyals2015show}
\citation{ngiam2011multimodal}
\citation{silberer2014learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A Semiotic Network modified to demonstrate Cangelosi et al.'s approach to symbol grounding.}}{49}{figure.119}}
\newlabel{fig:percNet}{{3.4}{49}{A Semiotic Network modified to demonstrate Cangelosi et al.'s approach to symbol grounding}{figure.119}{}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsubsection}{My Approach to Symbol Grounding}{49}{section*.120}}
\acronymused{MAE}
\citation{ngiam2011multimodal}
\citation{mcgurk1976hearing}
\citation{silberer2014learning}
\citation{samuel1997lexical}
\citation{barsalou2008grounded}
\citation{lee2008sparse}
\citation{vinyals2015show}
\citation{barsalou2008grounded}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Why use MAEs?}{50}{section*.121}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Learning and Pretraining}{50}{section*.122}}
\acronymused{ANN}
\acronymused{ANN}
\citation{repRev}
\citation{repRev}
\acronymused{ANN}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Research Plan}{51}{section.123}}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MAE}
\acronymused{MRL}
\acronymused{MRL}
\citation{barsalou2008grounded}
\citation{mcgurk1976hearing}
\citation{ma2009lip}
\citation{ma2009lip,samuel1997lexical}
\citation{ngiam2011multimodal,silberer2014learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Sensory Redundancy: Are Two Heads Better Than One?}{52}{chapter.130}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter4}{{4}{52}{Sensory Redundancy: Are Two Heads Better Than One?}{chapter.130}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Why Have One Sensor When Two Are Better?}{52}{section.131}}
\citation{reed2016generative,zhang2017stackgan,xu2018attngan,li2018video,mansimov2015generating}
\citation{zhang2017stackgan,xu2018attngan,li2018video}
\citation{zhang2017stackgan,xu2018attngan,li2018video}
\citation{szegedy2017inception}
\citation{zhang2017stackgan}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Generating Images From Natural Language}{53}{section.132}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Aims}{53}{subsection.133}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}A note on Qualatitive Anaylsis of Image Generation}{53}{subsection.134}}
\acronymused{KLD}
\citation{hammami2009tree,hammami2010improved}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Adjectives used for the qualititive analysis of generated images.}}{54}{table.135}}
\newlabel{tab:adjs}{{4.1}{54}{Adjectives used for the qualititive analysis of generated images}{table.135}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}UCU Arabic Spoken Digits and MNIST}{54}{subsection.136}}
\newlabel{sec:UCU}{{4.2.3}{54}{UCU Arabic Spoken Digits and MNIST}{subsection.136}{}}
\@writefile{toc}{\contentsline {subsubsection}{UCU Arabic Spoken Digits}{54}{section*.137}}
\AC@undonewlabel{acro:MFCC}
\newlabel{acro:MFCC}{{4.2.3}{54}{UCU Arabic Spoken Digits}{section*.138}{}}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{Padding of Utterances}{54}{section*.139}}
\acronymused{MFCC}
\citation{lecun1998mnist}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Mean number of samples for each digit in the UCU Arabic Spoken Digits Dataset. Red bars show the standard devaition in length for each each digit.}}{55}{figure.140}}
\newlabel{fig:ucu_dig_length}{{4.1}{55}{Mean number of samples for each digit in the UCU Arabic Spoken Digits Dataset. Red bars show the standard devaition in length for each each digit}{figure.140}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Mean length and standard deviation of digits in the UCU dataset.}}{55}{table.141}}
\newlabel{tab:UCU_sampLen}{{4.2}{55}{Mean length and standard deviation of digits in the UCU dataset}{table.141}{}}
\@writefile{toc}{\contentsline {subsubsection}{MNIST Handwritten Digits}{55}{section*.142}}
\citation{barsalou2008grounded}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Problem Description}{56}{subsection.143}}
\@writefile{toc}{\contentsline {subsubsection}{Classification}{56}{section*.144}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Bidirectional Symbol Grounding}{56}{section*.145}}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Experiment Details}{56}{subsection.146}}
\acronymused{MFCC}
\acronymused{MFCC}
\newlabel{sec:UCU_mnist_comb}{{4.2.5}{57}{Combining Datasets}{section*.147}{}}
\@writefile{toc}{\contentsline {paragraph}{Combining Datasets}{57}{section*.147}}
\@writefile{toc}{\contentsline {paragraph}{Merging Modalities}{57}{section*.148}}
\newlabel{eqn:concat}{{4.1}{57}{Merging Modalities}{equation.149}{}}
\newlabel{eqn:add}{{4.2}{57}{Merging Modalities}{equation.150}{}}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Training Procedures}{57}{section*.151}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Bimodal}{57}{section*.152}}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Randomly Removed}{57}{section*.153}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Testing Conditions}{58}{section*.154}}
\acronymused{MAE}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{Bimodal}{58}{section*.155}}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{Image Only}{58}{section*.156}}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{\ac {MFCC} Only}{58}{section*.157}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Network Description}{58}{section*.158}}
\@writefile{toc}{\contentsline {paragraph}{Baseline Models}{58}{section*.159}}
\@writefile{toc}{\contentsline {paragraph}{Multimodal Autoencoder}{58}{section*.162}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Image autoencoder and classifier. Layer 6c performs classification, whilst the branch starting at layer 6 regenerates the image.}}{59}{table.160}}
\newlabel{tab:MNIST_AE_description}{{4.3}{59}{Image autoencoder and classifier. Layer 6c performs classification, whilst the branch starting at layer 6 regenerates the image}{table.160}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces \ac {MFCC} autoencoder and classifier. Layer 7c performs classification, whilst the branch starting at layer 7 regenerates the \acp {MFCC}. The addition of reshape layers is to ensure the final shape of the regenerated \acp {MFCC} matches the target shape whilst the embedding shape matches that of the embedding of the image autoencoder.}}{59}{table.161}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\newlabel{tab:UCU_AE_description}{{4.4}{59}{\ac {MFCC} autoencoder and classifier. Layer 7c performs classification, whilst the branch starting at layer 7 regenerates the \acp {MFCC}. The addition of reshape layers is to ensure the final shape of the regenerated \acp {MFCC} matches the target shape whilst the embedding shape matches that of the embedding of the image autoencoder}{table.161}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Image and MFCC multimodal autoencoder. Layers marked i, m, im and c are image, MFCC, image and MFCC and classification respectively.}}{60}{table.163}}
\newlabel{tab:UCU_MNIST_MAE_description}{{4.5}{60}{Image and MFCC multimodal autoencoder. Layers marked i, m, im and c are image, MFCC, image and MFCC and classification respectively}{table.163}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Results}{60}{subsection.165}}
\@writefile{toc}{\contentsline {subsubsection}{Classification Results}{60}{section*.166}}
\acronymused{MFCC}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \leavevmode {\color  {red}The \ac {MAE} architecture used to bidirectionally ground the MNIST and UCU datasets. Layers marked with C are convolution TC, Transposed Convolution, RS, Reshape and N, Dense.}}}{61}{figure.164}}
\acronymused{MAE}
\newlabel{fig:netMnist}{{4.2}{61}{\textcolor {red}{The \ac {MAE} architecture used to bidirectionally ground the MNIST and UCU datasets. Layers marked with C are convolution TC, Transposed Convolution, RS, Reshape and N, Dense.}}{figure.164}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Mean Squared Errors for the Bimodal testing condition, comparing the Bimodal (Bi) and Randomly Removed (RR) training procedures.}}{61}{table.167}}
\newlabel{tab:mnist_ucu_bi_res}{{4.6}{61}{Mean Squared Errors for the Bimodal testing condition, comparing the Bimodal (Bi) and Randomly Removed (RR) training procedures}{table.167}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Reconstruction Results}{61}{section*.170}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Mean Squared Errors for the Image Only testing condition, comparing the Bimodal (Bi) and Randomly Removed (RR) training procedures.}}{62}{table.168}}
\newlabel{tab:mnist_ucu_im_res}{{4.7}{62}{Mean Squared Errors for the Image Only testing condition, comparing the Bimodal (Bi) and Randomly Removed (RR) training procedures}{table.168}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Mean Squared Errors for the MFCC Only testing condition, comparing the Bimodal (Bi) and Randomly Removed (RR) training procedures. }}{62}{table.169}}
\newlabel{tab:mnist_ucu_mfcc_res}{{4.8}{62}{Mean Squared Errors for the MFCC Only testing condition, comparing the Bimodal (Bi) and Randomly Removed (RR) training procedures}{table.169}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.7}Discussion}{62}{subsection.173}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion of Classification Results}{62}{section*.174}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MFCC}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces A selection of randomly sampled digits generated in different training and testing conditions for both \textit  {Add} and \textit  {Concat} merging methods.}}{63}{figure.171}}
\newlabel{fig:mnistDigits}{{4.3}{63}{A selection of randomly sampled digits generated in different training and testing conditions for both \textit {Add} and \textit {Concat} merging methods}{figure.171}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Two examples of the digit ``5'' being generated in different training and testing conditions for both \textit  {Add} and \textit  {Concat} merging methods.}}{63}{figure.172}}
\newlabel{fig:5s}{{4.4}{63}{Two examples of the digit ``5'' being generated in different training and testing conditions for both \textit {Add} and \textit {Concat} merging methods}{figure.172}{}}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Discussion of Reconstruction Results}{64}{section*.175}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Image reconstruction from MFCCs}{64}{section*.176}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{MFCC reconstruction from Images}{65}{section*.177}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{Effects of randomly removing inputs}{65}{section*.178}}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\citation{sheppardtowards}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Multiple generations of the same digit}{66}{section*.179}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Summary}{66}{section.180}}
\citation{mikolov2013distributed}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Magical Vectors and Where to Find Them}{68}{chapter.182}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter5}{{5}{68}{Magical Vectors and Where to Find Them}{chapter.182}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}ANN Latent Space}{68}{section.183}}
\acronymused{ANN}
\acronymused{MAE}
\acronymused{ANN}
\acronymused{ANN}
\newlabel{eqn:mikolov}{{5.1}{68}{ANN Latent Space}{equation.184}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Vector Arithmetic}{69}{section.185}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of how novel images can be generated using Vector Arithmetic on image and word embeddings.}}{70}{figure.186}}
\newlabel{fig:vectorArthexmp}{{5.1}{70}{An example of how novel images can be generated using Vector Arithmetic on image and word embeddings}{figure.186}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Examples of the different, shapes, colours, sizes and positions of objects in the ArtS dataset. The top left object would be described as ``\textsc  {big indigo circle, top left}''.}}{71}{figure.187}}
\newlabel{fig:shapes}{{5.2}{71}{Examples of the different, shapes, colours, sizes and positions of objects in the ArtS dataset. The top left object would be described as ``\textsc {big indigo circle, top left}''}{figure.187}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Artificial Shapes Dataset}{71}{section.188}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Dataset Description}{71}{subsection.189}}
\AC@undonewlabel{acro:RGB}
\newlabel{acro:RGB}{{5.3.1}{71}{Dataset Description}{section*.190}{}}
\acronymused{RGB}
\acronymused{RGB}
\acronymused{RGB}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Artificial Shapes dataset description.}}{72}{table.191}}
\newlabel{tab:Arts_desc}{{5.1}{72}{Artificial Shapes dataset description}{table.191}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Problem Description}{72}{subsection.192}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Network Description}{73}{subsection.193}}
\acronymused{ANN}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiments with the ArtS Dataset}{73}{section.196}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Image and Word multimodal autoencoder. Layers marked i, w, and iw are image, word, and image and word respectively. The number of neurons, ``emb size'', in the Embedding block is varied in some experiments.}}{74}{table.194}}
\newlabel{tab:Arts_MAE_description}{{5.2}{74}{Image and Word multimodal autoencoder. Layers marked i, w, and iw are image, word, and image and word respectively. The number of neurons, ``emb size'', in the Embedding block is varied in some experiments}{table.194}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces \leavevmode {\color  {red}The \ac {MAE} architecture used in this chapters. Layers marked with C are convolution TC, Transposed Convolution, RS, Reshape and N, Dense.}}}{75}{figure.195}}
\acronymused{MAE}
\newlabel{fig:netArts}{{5.3}{75}{\textcolor {red}{The \ac {MAE} architecture used in this chapters. Layers marked with C are convolution TC, Transposed Convolution, RS, Reshape and N, Dense.}}{figure.195}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Experiment 1}{76}{subsection.197}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Experiment 1 data-subset.}}{76}{table.198}}
\newlabel{tab:exp1_data}{{5.3}{76}{Experiment 1 data-subset}{table.198}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{77}{section*.199}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Experiment 1: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the description output.}}{77}{figure.200}}
\newlabel{fig:graph331}{{5.4}{77}{Experiment 1: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the description output}{figure.200}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Experiment 1: Total MSE for different embedding sizes for the three testing conditions. An Embedding Size of 200 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$.)}}{78}{table.201}}
\newlabel{tab:res331}{{5.4}{78}{Experiment 1: Total MSE for different embedding sizes for the three testing conditions. An Embedding Size of 200 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$.)}{table.201}{}}
\acronymused{RGB}
\acronymused{RGB}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{78}{section*.202}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Images generated from descriptions with an embedding size of 8 or 200 neurons.}}{79}{figure.203}}
\newlabel{fig:331multi}{{5.5}{79}{Images generated from descriptions with an embedding size of 8 or 200 neurons}{figure.203}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{79}{section*.205}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Experiment 1, runs A-D: Images generated from shape names (\textsc  {Circle, Rectangle, Triangle}) for different sizes of embedding.}}{80}{figure.204}}
\newlabel{fig:331shapes}{{5.6}{80}{Experiment 1, runs A-D: Images generated from shape names (\textsc {Circle, Rectangle, Triangle}) for different sizes of embedding}{figure.204}{}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{80}{section*.206}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Experiment 2}{81}{subsection.207}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Experiment 2 data-subset.}}{81}{table.208}}
\newlabel{tab:exp2_data}{{5.5}{81}{Experiment 2 data-subset}{table.208}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{81}{section*.209}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Experiment 2: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the description output.}}{82}{figure.210}}
\newlabel{fig:graph333}{{5.7}{82}{Experiment 2: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the description output}{figure.210}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Experiment 2: Total MSE for different embedding sizes. An Embedding Size of 296 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$.)}}{82}{table.211}}
\newlabel{tab:res333}{{5.6}{82}{Experiment 2: Total MSE for different embedding sizes. An Embedding Size of 296 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$.)}{table.211}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Experiment 2: Images generated from descriptions with an embedding size of 296 neurons.}}{83}{figure.213}}
\newlabel{fig:333multi}{{5.8}{83}{Experiment 2: Images generated from descriptions with an embedding size of 296 neurons}{figure.213}{}}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{83}{figure.213}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Experiment 2 run A: Images generated of each word (\textsc  {Circle, Rectangle, Triangle, Big, Medium, Small, Red, Green, Blue, Centre-Left, Centre-Centre, Centre-Right}) for different embedding sizes.}}{84}{figure.214}}
\newlabel{fig:333single}{{5.9}{84}{Experiment 2 run A: Images generated of each word (\textsc {Circle, Rectangle, Triangle, Big, Medium, Small, Red, Green, Blue, Centre-Left, Centre-Centre, Centre-Right}) for different embedding sizes}{figure.214}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{84}{section*.217}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Experiment 2, runs A-D: Images generated of each shape for different emedding sizes.}}{85}{figure.215}}
\newlabel{fig:shapes333}{{5.10}{85}{Experiment 2, runs A-D: Images generated of each shape for different emedding sizes}{figure.215}{}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{85}{section*.218}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Experiment 2, run A: Images generated using word pairs using an embedding size of 296 neurons.}}{86}{figure.216}}
\newlabel{fig:2word333}{{5.11}{86}{Experiment 2, run A: Images generated using word pairs using an embedding size of 296 neurons}{figure.216}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Experiment 3}{86}{subsection.220}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Images generated of each shape for different sizes of embedding using the \acp {MAE} trained in experiments 1 and 2 run A.}}{87}{figure.219}}
\acronymused{MAE}
\newlabel{fig:shapes333v331}{{5.12}{87}{Images generated of each shape for different sizes of embedding using the \acp {MAE} trained in experiments 1 and 2 run A}{figure.219}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{87}{section*.222}}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{87}{section*.224}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Experiment 3 data-subset.}}{88}{table.221}}
\newlabel{tab:exp3_data}{{5.7}{88}{Experiment 3 data-subset}{table.221}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Experiment 3: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$.)}}{88}{table.223}}
\newlabel{tab:res339}{{5.8}{88}{Experiment 3: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$.)}{table.223}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Experiment 3, run A: Images generated from descriptions, starting training from randomly initialised weights.}}{89}{figure.225}}
\newlabel{fig:339multi}{{5.13}{89}{Experiment 3, run A: Images generated from descriptions, starting training from randomly initialised weights}{figure.225}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Experiment 3, run A: Images generated from shape, colour and size words for different weight initialisation conditions.}}{89}{figure.226}}
\newlabel{fig:339single}{{5.14}{89}{Experiment 3, run A: Images generated from shape, colour and size words for different weight initialisation conditions}{figure.226}{}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Experiment 3, run A: Images generated from each position word for different weight initialisation conditions.}}{90}{figure.227}}
\newlabel{fig:339single_pos}{{5.15}{90}{Experiment 3, run A: Images generated from each position word for different weight initialisation conditions}{figure.227}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Experiment 3, run A: Images generated using word pairs.}}{90}{figure.228}}
\newlabel{fig:2word339}{{5.16}{90}{Experiment 3, run A: Images generated using word pairs}{figure.228}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces Experiment 3: Percentage Description Accuracy for different weight initialisations. }}{90}{table.230}}
\newlabel{tab:res339_acc}{{5.9}{90}{Experiment 3: Percentage Description Accuracy for different weight initialisations}{table.230}{}}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{90}{table.230}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces Experiment 3: Total number of weight updates.}}{91}{table.232}}
\newlabel{tab:updatesTotal}{{5.10}{91}{Experiment 3: Total number of weight updates}{table.232}{}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{91}{section*.231}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MSE}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Experiment 4}{92}{subsection.233}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{92}{section*.234}}
\@writefile{lot}{\contentsline {table}{\numberline {5.11}{\ignorespaces Experiment 4: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$.)}}{92}{table.235}}
\newlabel{tab:res739}{{5.11}{92}{Experiment 4: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$.)}{table.235}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{92}{section*.236}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Experiment 4, run A: Images generated from shape, and size word for different weight initialisation conditions.}}{93}{figure.237}}
\newlabel{fig:739single_shape}{{5.17}{93}{Experiment 4, run A: Images generated from shape, and size word for different weight initialisation conditions}{figure.237}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Experiment 4, run A: Images generated from each colour word for different weight initialisation conditions.}}{93}{figure.238}}
\newlabel{fig:739single_col}{{5.18}{93}{Experiment 4, run A: Images generated from each colour word for different weight initialisation conditions}{figure.238}{}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Experiment 4, run A: Images generated from each position word for different weight initialisation conditions.}}{94}{figure.239}}
\newlabel{fig:739single_pos}{{5.19}{94}{Experiment 4, run A: Images generated from each position word for different weight initialisation conditions}{figure.239}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Experiment 4, run A: Images generated using word pairs.}}{94}{figure.240}}
\newlabel{fig:2word739}{{5.20}{94}{Experiment 4, run A: Images generated using word pairs}{figure.240}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.12}{\ignorespaces Experiment 4: Percentage Description Accuracy for different weight initialisations.}}{95}{table.242}}
\newlabel{tab:res739acc}{{5.12}{95}{Experiment 4: Percentage Description Accuracy for different weight initialisations}{table.242}{}}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{95}{table.242}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{95}{section*.243}}
\acronymused{MAE}
\acronymused{MSE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.13}{\ignorespaces Experiment 4: Total number of weight updates.}}{96}{table.244}}
\newlabel{tab:updatesTotal4}{{5.13}{96}{Experiment 4: Total number of weight updates}{table.244}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.14}{\ignorespaces Experiment 4: Mean Squared Error ($\times 10^{-3}$) and Percentage Description Accuracy for the randomly initialised MAE after 50 epochs of training.}}{96}{table.245}}
\newlabel{tab:res739_50}{{5.14}{96}{Experiment 4: Mean Squared Error ($\times 10^{-3}$) and Percentage Description Accuracy for the randomly initialised MAE after 50 epochs of training}{table.245}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.5}Experiment 5}{97}{subsection.246}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{97}{section*.247}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.15}{\ignorespaces Experiment 5: Total MSE. Alternating rows show MSE for the datasubset without the omitted data and MSE for only the omitted data (marked with *). (All values are $\times 10^{-3}$.)}}{97}{table.248}}
\newlabel{tab:res_exp5}{{5.15}{97}{Experiment 5: Total MSE. Alternating rows show MSE for the datasubset without the omitted data and MSE for only the omitted data (marked with *). (All values are $\times 10^{-3}$.)}{table.248}{}}
\acronymused{MSE}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{98}{section*.249}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Experiment 5, run A: Images generated from descriptions of shapes never seen by the \ac {MAE}.}}{98}{figure.250}}
\acronymused{MAE}
\newlabel{fig:739_minus1}{{5.21}{98}{Experiment 5, run A: Images generated from descriptions of shapes never seen by the \ac {MAE}}{figure.250}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Experiment 5, run A: Images generated via Vector Arithmetic by MAE 1.}}{99}{figure.251}}
\newlabel{fig:739_vectorArth}{{5.22}{99}{Experiment 5, run A: Images generated via Vector Arithmetic by MAE 1}{figure.251}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.16}{\ignorespaces Experiment 5: Percentage Description Accuracy. Alternating rows show accuracy for the datasubset without the omitted data and MSE for only the omitted data (marked with *).}}{99}{table.253}}
\newlabel{tab:res_exp5_acc}{{5.16}{99}{Experiment 5: Percentage Description Accuracy. Alternating rows show accuracy for the datasubset without the omitted data and MSE for only the omitted data (marked with *)}{table.253}{}}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{99}{table.253}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.6}Discussion}{100}{subsection.254}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{How an incremental approach helps}{101}{section*.255}}
\acronymused{ANN}
\@writefile{lot}{\contentsline {table}{\numberline {5.17}{\ignorespaces Probability distributions generated from rolling a 6 sided die.}}{101}{table.256}}
\newlabel{tab:dieProb}{{5.17}{101}{Probability distributions generated from rolling a 6 sided die}{table.256}{}}
\AC@undonewlabel{acro:PRNG}
\newlabel{acro:PRNG}{{5.4.6}{101}{How an incremental approach helps}{section*.257}{}}
\acronymused{PRNG}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{lof}{\contentsline {figure}{\numberline {5.23}{\ignorespaces Visualisation of an imaginary Cost landscape.}}{102}{figure.258}}
\newlabel{fig:localminima}{{5.23}{102}{Visualisation of an imaginary Cost landscape}{figure.258}{}}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Summary}{103}{section.259}}
\acronymused{MAE}
\acronymused{MAE}
\citation{keller2016analysis}
\citation{schillingmann2009towards,schillingmann2009computational}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bidirectional Grounding of Real Data}{105}{chapter.260}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter6}{{6}{105}{Bidirectional Grounding of Real Data}{chapter.260}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}The Difficulties of Real Data}{105}{section.261}}
\acronymused{MAE}
\acronymused{MRL}
\acronymused{MRL}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces System schematic. Data is captured from sensors by an acoustic packager and fed to the multimodal autoencoder.}}{105}{figure.262}}
\newlabel{fig:schematic}{{6.1}{105}{System schematic. Data is captured from sensors by an acoustic packager and fed to the multimodal autoencoder}{figure.262}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Acoustic Packaging}{105}{section.263}}
\acronymused{MRL}
\AC@undonewlabel{acro:AP}
\newlabel{acro:AP}{{6.2}{105}{Acoustic Packaging}{section*.264}{}}
\acronymused{AP}
\AC@undonewlabel{acro:ASR}
\newlabel{acro:ASR}{{6.2}{106}{Acoustic Packaging}{section*.265}{}}
\acronymused{ASR}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Real Shapes Dataset}{106}{section.266}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Example images from ReShape.}}{106}{figure.267}}
\newlabel{fig:ReShape}{{6.2}{106}{Example images from ReShape}{figure.267}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Dataset Description}{106}{subsection.268}}
\citation{keller2016analysis,keller}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Exemplar images for all objects in the ReShape dataset.}}{107}{figure.269}}
\newlabel{fig:ReShapeAll}{{6.3}{107}{Exemplar images for all objects in the ReShape dataset}{figure.269}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Problem Description}{108}{subsection.270}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Network Description}{108}{subsection.271}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experiments with the ReShape dataset}{108}{section.273}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces \leavevmode {\color  {red}The \ac {MAE} architecture used in this chapters. Layers marked with C are convolution TC, Transposed Convolution, RS, Reshape and N, Dense.}}}{109}{figure.272}}
\acronymused{MAE}
\newlabel{fig:netReShape}{{6.4}{109}{\textcolor {red}{The \ac {MAE} architecture used in this chapters. Layers marked with C are convolution TC, Transposed Convolution, RS, Reshape and N, Dense.}}{figure.272}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Data Preprocessing}{110}{subsection.274}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Example cropped images from ReShape.}}{110}{figure.275}}
\newlabel{fig:ReShapeCrop}{{6.5}{110}{Example cropped images from ReShape}{figure.275}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Regions to crop to given different postion words.}}{111}{figure.276}}
\newlabel{fig:CropHeur}{{6.6}{111}{Regions to crop to given different postion words}{figure.276}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Training Procedures}{111}{subsection.277}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Exp 1: Average of the training images of bricks and ducks in different sizes and colours.}}{111}{figure.278}}
\newlabel{fig:AvgBrickDuck}{{6.7}{111}{Exp 1: Average of the training images of bricks and ducks in different sizes and colours}{figure.278}{}}
\acronymused{MSE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Exp 2: Average of the training images for bricks, cups, donuts, ducks, and rectangles, in different sizes and colours.}}{112}{figure.279}}
\newlabel{fig:avgMost}{{6.8}{112}{Exp 2: Average of the training images for bricks, cups, donuts, ducks, and rectangles, in different sizes and colours}{figure.279}{}}
\acronymused{MAE}
\acronymused{MSE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Exp 1: Exemplar images of bricks and ducks in different sizes and colours.}}{112}{figure.280}}
\newlabel{fig:ExmBrickDuck}{{6.9}{112}{Exp 1: Exemplar images of bricks and ducks in different sizes and colours}{figure.280}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Experiment 1}{113}{subsection.281}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Experiment 1 data subset.}}{113}{table.282}}
\newlabel{tab:6_exp1_data}{{6.1}{113}{Experiment 1 data subset}{table.282}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{113}{section*.283}}
\acronymused{MAE}
\acronymused{MSE}
\acronymused{MSE}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Exp 1: Total Mean Squared Error for different test conditions and training procedures. (All values are $\times 10^{-3}$.)}}{113}{table.284}}
\newlabel{tab:6_res_exp1}{{6.2}{113}{Exp 1: Total Mean Squared Error for different test conditions and training procedures. (All values are $\times 10^{-3}$.)}{table.284}{}}
\acronymused{MSE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MSE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{114}{section*.285}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Exp 1: Images generated from full descriptions using the \ac {MAE} trained with the simple procedure.}}{114}{figure.286}}
\acronymused{MAE}
\newlabel{fig:simpleGen_1}{{6.10}{114}{Exp 1: Images generated from full descriptions using the \ac {MAE} trained with the simple procedure}{figure.286}{}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Exp 1: Images generated from (only) full descriptions using the MAE trained with the exemplar procedure.}}{114}{figure.287}}
\newlabel{fig:exemplarGen_1}{{6.11}{114}{Exp 1: Images generated from (only) full descriptions using the MAE trained with the exemplar procedure}{figure.287}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Exp 1: Images generated using word pairs using the MAE trained with the simple procedure.}}{115}{figure.288}}
\newlabel{fig:2wordsSimpleBrickDuck}{{6.12}{115}{Exp 1: Images generated using word pairs using the MAE trained with the simple procedure}{figure.288}{}}
\acronymused{MAE}
\acronymused{MAE}
\citation{keller2016analysis,keller}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Images generated using word pairs using the MAE trained using image exemplars. (Only words are provided as input.)}}{116}{figure.289}}
\newlabel{fig:2wordsExemplarBrickDuck}{{6.13}{116}{Images generated using word pairs using the MAE trained using image exemplars. (Only words are provided as input.)}{figure.289}{}}
\@writefile{toc}{\contentsline {paragraph}{Multilabel Classification}{116}{section*.290}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Exp 1: Percentage Description Accuracy for different test conditions and training procedures.}}{117}{table.291}}
\newlabel{tab:6_res_exp1_acc}{{6.3}{117}{Exp 1: Percentage Description Accuracy for different test conditions and training procedures}{table.291}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Experiment 2}{117}{subsection.292}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Experiment 2 data subset.}}{117}{table.293}}
\newlabel{tab:6_exp2_data}{{6.4}{117}{Experiment 2 data subset}{table.293}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Exp 2: Exemplar images of bricks, cups, donuts, ducks, and rectangles, in different sizes and colours.}}{117}{figure.294}}
\newlabel{fig:ExmMost}{{6.14}{117}{Exp 2: Exemplar images of bricks, cups, donuts, ducks, and rectangles, in different sizes and colours}{figure.294}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{117}{section*.295}}
\@writefile{toc}{\contentsline {subsubsection}{Image Generation}{117}{section*.297}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Exp 2: Total Mean Squared Error for different test conditions and training procedures. (All values are $\times 10^{-3}$.)}}{118}{table.296}}
\newlabel{tab:6_res_exp2}{{6.5}{118}{Exp 2: Total Mean Squared Error for different test conditions and training procedures. (All values are $\times 10^{-3}$.)}{table.296}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Exp 2: Images generated from full descriptions using the \ac {MAE} trained with the exemplar procedure. Red borders show objects with exemplar images.}}{118}{figure.298}}
\acronymused{MAE}
\newlabel{fig:mostExemplarGen}{{6.15}{118}{Exp 2: Images generated from full descriptions using the \ac {MAE} trained with the exemplar procedure. Red borders show objects with exemplar images}{figure.298}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Exp 2: Images generated using word pairs using the \ac {MAE} trained using image exemplars when only words are provided as input.}}{119}{figure.299}}
\acronymused{MAE}
\newlabel{fig:2wordsMostExemplar}{{6.16}{119}{Exp 2: Images generated using word pairs using the \ac {MAE} trained using image exemplars when only words are provided as input}{figure.299}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Multilabel Classification}{120}{section*.300}}
\acronymused{MSE}
\acronymused{MSE}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Accuracy}}{120}{table.302}}
\newlabel{tab:6_res_exp2_acc}{{6.6}{120}{Accuracy}{table.302}{}}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{120}{table.302}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}Discussion}{120}{subsection.303}}
\acronymused{MRL}
\citation{sheppard2020multimodal}
\acronymused{MRL}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Summary}{121}{section.304}}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MAE}
\acronymused{ANN}
\acronymused{MAE}
\citation{repRev}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{122}{chapter.306}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter7}{{7}{122}{Conclusion}{chapter.306}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary of Important Points}{122}{section.307}}
\acronymused{MRL}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{ANN}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Conclusion}{123}{section.308}}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Future Work}{123}{section.309}}
\acronymused{MRL}
\acronymused{MAE}
\citation{redmon2018yolov3}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces An example of how the \ac {MAE} can be used as part of a robotic system.}}{124}{figure.311}}
\acronymused{MAE}
\newlabel{fig:fsm}{{7.1}{124}{An example of how the \ac {MAE} can be used as part of a robotic system}{figure.311}{}}
\acronymused{MRL}
\AC@undonewlabel{acro:HRI}
\newlabel{acro:HRI}{{7.3}{124}{Future Work}{section*.310}{}}
\acronymused{HRI}
\acronymused{MAE}
\AC@undonewlabel{acro:FSM}
\newlabel{acro:FSM}{{7.3}{124}{Future Work}{section*.312}{}}
\acronymused{FSM}
\AC@undonewlabel{acro:NLU}
\newlabel{acro:NLU}{{7.3}{124}{Future Work}{section*.313}{}}
\acronymused{NLU}
\acronymused{MAE}
\acronymused{MRL}
\citation{rasa}
\citation{gregor2015draw}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces An example of using an MAE trained with MRL to disambiguate which object is being referred to in a scene.}}{125}{figure.314}}
\newlabel{fig:disamb}{{7.2}{125}{An example of using an MAE trained with MRL to disambiguate which object is being referred to in a scene}{figure.314}{}}
\acronymused{MSE}
\acronymused{NLU}
\acronymused{NLU}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MRL}
\acronymused{HRI}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Accounting for time}{126}{subsection.315}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MRL}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix \emph  {A Primer on Artifical Neural Networks}}{127}{appendix.316}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix:app3}{{A}{127}{Appendix \emph {A Primer on Artifical Neural Networks}}{appendix.316}{}}
\newlabel{eqn:BP1}{{A.1}{127}{Appendix \emph {A Primer on Artifical Neural Networks}}{equation.317}{}}
\newlabel{eqn:BP2}{{A.2}{127}{Appendix \emph {A Primer on Artifical Neural Networks}}{equation.318}{}}
\newlabel{eqn:BP3}{{A.3}{127}{Appendix \emph {A Primer on Artifical Neural Networks}}{equation.319}{}}
\newlabel{eqn:BP4}{{A.4}{128}{Appendix \emph {A Primer on Artifical Neural Networks}}{equation.320}{}}
\newlabel{eqn:hada}{{A.5}{128}{\notesname \@mkboth {\MakeUppercase {\notesname }}{\MakeUppercase {\notesname }}}{equation.322}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Appendix \emph  {Sensory Redundancy}}{129}{appendix.323}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix:app4}{{B}{129}{Appendix \emph {Sensory Redundancy}}{appendix.323}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Results from the combined MNIST Handwritten Digits and UCU Arabic Spoken Digits}}{130}{table.324}}
\newlabel{tab:mnist_ucu_master_res}{{B.1}{130}{Results from the combined MNIST Handwritten Digits and UCU Arabic Spoken Digits}{table.324}{}}
\bibdata{bigone}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Appendix \emph  {Magical Vectors and Where to Find Them}}{131}{appendix.325}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix:app5}{{C}{131}{Appendix \emph {Magical Vectors and Where to Find Them}}{appendix.325}{}}
\newlabel{eqn:updates}{{C.1}{131}{Appendix \emph {Magical Vectors and Where to Find Them}}{equation.326}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.1}{\ignorespaces Number of weight updates per Epoch for each experiment.}}{131}{table.327}}
\newlabel{tab:updatesperEpoch}{{C.1}{131}{Number of weight updates per Epoch for each experiment}{table.327}{}}
\bibcite{smith2008infants}{{1}{}{{}}{{}}}
\bibcite{yurovsky2013statistical}{{2}{}{{}}{{}}}
\bibcite{repRev}{{3}{}{{}}{{}}}
\bibcite{rosenblatt1958perceptron}{{4}{}{{}}{{}}}
\bibcite{hochreiter1998vanishing}{{5}{}{{}}{{}}}
\bibcite{bengio2007greedy}{{6}{}{{}}{{}}}
\bibcite{lecun1998mnist}{{7}{}{{}}{{}}}
\bibcite{dumoulin2016guide}{{8}{}{{}}{{}}}
\bibcite{radford2015unsupervised}{{9}{}{{}}{{}}}
\bibcite{zeiler2010deconvolutional}{{10}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{132}{appendix*.328}}
\bibcite{kingma2013auto}{{11}{}{{}}{{}}}
\bibcite{ngiam2011multimodal}{{12}{}{{}}{{}}}
\bibcite{mcgurk1976hearing}{{13}{}{{}}{{}}}
\bibcite{krizhevsky2012imagenet}{{14}{}{{}}{{}}}
\bibcite{vinyals2019alphastar}{{15}{}{{}}{{}}}
\bibcite{ma2004facial}{{16}{}{{}}{{}}}
\bibcite{levi2015emotion}{{17}{}{{}}{{}}}
\bibcite{kussul2017deep}{{18}{}{{}}{{}}}
\bibcite{qi2017pointnet}{{19}{}{{}}{{}}}
\bibcite{lohan2016distinguishing}{{20}{}{{}}{{}}}
\bibcite{sheppard2017understanding}{{21}{}{{}}{{}}}
\bibcite{lohan2018toward}{{22}{}{{}}{{}}}
\bibcite{simonyan2014very}{{23}{}{{}}{{}}}
\bibcite{springenberg2014striving}{{24}{}{{}}{{}}}
\bibcite{srivastava2014dropout}{{25}{}{{}}{{}}}
\bibcite{he2016deep}{{26}{}{{}}{{}}}
\bibcite{huang2017densely}{{27}{}{{}}{{}}}
\bibcite{szegedy2015going}{{28}{}{{}}{{}}}
\bibcite{szegedy2016rethinking}{{29}{}{{}}{{}}}
\bibcite{szegedy2017inception}{{30}{}{{}}{{}}}
\bibcite{gregor2015draw}{{31}{}{{}}{{}}}
\bibcite{netzer2011reading}{{32}{}{{}}{{}}}
\bibcite{mirza2014conditional}{{33}{}{{}}{{}}}
\bibcite{odena2017conditional}{{34}{}{{}}{{}}}
\bibcite{vinyals2015show}{{35}{}{{}}{{}}}
\bibcite{lebret2015phrase}{{36}{}{{}}{{}}}
\bibcite{donahue2015long}{{37}{}{{}}{{}}}
\bibcite{jia2015guiding}{{38}{}{{}}{{}}}
\bibcite{rohrbach2014coherent}{{39}{}{{}}{{}}}
\bibcite{rohrbach2013translating}{{40}{}{{}}{{}}}
\bibcite{yao2015describing}{{41}{}{{}}{{}}}
\bibcite{yao2015video}{{42}{}{{}}{{}}}
\bibcite{venugopalan2014translating}{{43}{}{{}}{{}}}
\bibcite{johnson2016densecap}{{44}{}{{}}{{}}}
\bibcite{ordonez2011im2text}{{45}{}{{}}{{}}}
\bibcite{sheppard2016video}{{46}{}{{}}{{}}}
\bibcite{zhu2017unpaired}{{47}{}{{}}{{}}}
\bibcite{lu2013speech}{{48}{}{{}}{{}}}
\bibcite{reed2016generative}{{49}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{50}{}{{}}{{}}}
\bibcite{keller}{{51}{}{{}}{{}}}
\bibcite{mansimov2015generating}{{52}{}{{}}{{}}}
\bibcite{zhang2017stackgan}{{53}{}{{}}{{}}}
\bibcite{silberer2014learning}{{54}{}{{}}{{}}}
\bibcite{wavenet}{{55}{}{{}}{{}}}
\bibcite{vincent2010stacked}{{56}{}{{}}{{}}}
\bibcite{mikolov2013distributed}{{57}{}{{}}{{}}}
\bibcite{mikolov2013efficient}{{58}{}{{}}{{}}}
\bibcite{mikolov2013linguistic}{{59}{}{{}}{{}}}
\bibcite{feng2010visual}{{60}{}{{}}{{}}}
\bibcite{eslami2018neural}{{61}{}{{}}{{}}}
\bibcite{donahue2019large}{{62}{}{{}}{{}}}
\bibcite{pu2016variational}{{63}{}{{}}{{}}}
\bibcite{lin2014microsoft}{{64}{}{{}}{{}}}
\bibcite{bleu}{{65}{}{{}}{{}}}
\bibcite{van2017neural}{{66}{}{{}}{{}}}
\bibcite{dunbar2017zero}{{67}{}{{}}{{}}}
\bibcite{russakovsky2015imagenet}{{68}{}{{}}{{}}}
\bibcite{chiu2018state}{{69}{}{{}}{{}}}
\bibcite{nguyen2015deep}{{70}{}{{}}{{}}}
\bibcite{szegedy2013intriguing}{{71}{}{{}}{{}}}
\bibcite{moosavi2016deepfool}{{72}{}{{}}{{}}}
\bibcite{barsalou2008grounded}{{73}{}{{}}{{}}}
\bibcite{ImNet}{{74}{}{{}}{{}}}
\bibcite{ha2018world}{{75}{}{{}}{{}}}
\bibcite{ha2018recurrent}{{76}{}{{}}{{}}}
\bibcite{vinyals2016matching}{{77}{}{{}}{{}}}
\bibcite{searle1980minds}{{78}{}{{}}{{}}}
\bibcite{turing2009computing}{{79}{}{{}}{{}}}
\bibcite{cangelosi2000robotic}{{80}{}{{}}{{}}}
\bibcite{harnad1990symbol}{{81}{}{{}}{{}}}
\bibcite{steels2008symbol}{{82}{}{{}}{{}}}
\bibcite{slater1999intermodal}{{83}{}{{}}{{}}}
\bibcite{masland2012neuronal}{{84}{}{{}}{{}}}
\bibcite{fantz1963pattern}{{85}{}{{}}{{}}}
\bibcite{johnson2015two}{{86}{}{{}}{{}}}
\bibcite{webb2015mother}{{87}{}{{}}{{}}}
\bibcite{reid2017human}{{88}{}{{}}{{}}}
\bibcite{pfeifer2006body}{{89}{}{{}}{{}}}
\bibcite{smith2005development}{{90}{}{{}}{{}}}
\bibcite{nicolelis1995sensorimotor}{{91}{}{{}}{{}}}
\bibcite{mordvintsev2015inceptionism}{{92}{}{{}}{{}}}
\bibcite{glenberg2015few}{{93}{}{{}}{{}}}
\bibcite{Goldin-MeadowSusan2015Fata}{{94}{}{{}}{{}}}
\bibcite{de2014making}{{95}{}{{}}{{}}}
\bibcite{rucinski2012robotic}{{96}{}{{}}{{}}}
\bibcite{bahrick2000intersensory}{{97}{}{{}}{{}}}
\bibcite{walker2010preverbal}{{98}{}{{}}{{}}}
\bibcite{fischer2011multi}{{99}{}{{}}{{}}}
\bibcite{scott20122}{{100}{}{{}}{{}}}
\bibcite{fernald1987acoustic}{{101}{}{{}}{{}}}
\bibcite{lohan2012contingency}{{102}{}{{}}{{}}}
\bibcite{lohan2012tutor}{{103}{}{{}}{{}}}
\bibcite{pezzulo2013computational}{{104}{}{{}}{{}}}
\bibcite{bambach2017egocentric}{{105}{}{{}}{{}}}
\bibcite{olveczky2003segregation}{{106}{}{{}}{{}}}
\bibcite{oxenham2018we}{{107}{}{{}}{{}}}
\bibcite{lemonlearning}{{108}{}{{}}{{}}}
\bibcite{yu2017learning}{{109}{}{{}}{{}}}
\bibcite{coradeschi2000anchoring}{{110}{}{{}}{{}}}
\bibcite{coradeschi2003introduction}{{111}{}{{}}{{}}}
\bibcite{coradeschi2013short}{{112}{}{{}}{{}}}
\bibcite{vogt2002physical}{{113}{}{{}}{{}}}
\bibcite{cangelosi2006grounding}{{114}{}{{}}{{}}}
\bibcite{nakamura2009grounding}{{115}{}{{}}{{}}}
\bibcite{nakamura2011grounding}{{116}{}{{}}{{}}}
\bibcite{cangelosi2001adaptive}{{117}{}{{}}{{}}}
\bibcite{cangelosi2002symbol}{{118}{}{{}}{{}}}
\bibcite{cangelosi1998emergence}{{119}{}{{}}{{}}}
\bibcite{horst2019object}{{120}{}{{}}{{}}}
\bibcite{cangelosi2018speech}{{121}{}{{}}{{}}}
\bibcite{cangelosi2008italk}{{122}{}{{}}{{}}}
\bibcite{broz2014italk}{{123}{}{{}}{{}}}
\bibcite{samuel1997lexical}{{124}{}{{}}{{}}}
\bibcite{lee2008sparse}{{125}{}{{}}{{}}}
\bibcite{ma2009lip}{{126}{}{{}}{{}}}
\bibcite{xu2018attngan}{{127}{}{{}}{{}}}
\bibcite{li2018video}{{128}{}{{}}{{}}}
\bibcite{hammami2009tree}{{129}{}{{}}{{}}}
\bibcite{hammami2010improved}{{130}{}{{}}{{}}}
\bibcite{sheppardtowards}{{131}{}{{}}{{}}}
\bibcite{keller2016analysis}{{132}{}{{}}{{}}}
\bibcite{schillingmann2009towards}{{133}{}{{}}{{}}}
\bibcite{schillingmann2009computational}{{134}{}{{}}{{}}}
\bibcite{sheppard2020multimodal}{{135}{}{{}}{{}}}
\bibcite{redmon2018yolov3}{{136}{}{{}}{{}}}
\bibcite{rasa}{{137}{}{{}}{{}}}
\bibstyle{IEEEtran}
\providecommand\NAT@force@numbers{}\NAT@force@numbers

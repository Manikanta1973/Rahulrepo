\relax 
\providecommand\hyper@newdestlabel[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Acronyms}{i}{section.1}}
\newacro{AE}[\AC@hyperlink{AE}{AE}]{Autoencoder}
\newacro{AP}[\AC@hyperlink{AP}{AP}]{Acoustic Packaging}
\newacro{ASR}[\AC@hyperlink{ASR}{ASR}]{Automatic Speech Recognition}
\newacro{ANN}[\AC@hyperlink{ANN}{ANN}]{Artificial Neural Network}
\newacro{ConvNet}[\AC@hyperlink{ConvNet}{ConvNet}]{Convolutional Neural Network}
\newacro{CPU}[\AC@hyperlink{CPU}{CPU}]{Central Processing Unit}
\newacro{DL}[\AC@hyperlink{DL}{DL}]{Deep Learning}
\newacro{FSM}[\AC@hyperlink{FSM}{FSM}]{Finite State Machine}
\newacro{GAN}[\AC@hyperlink{GAN}{GAN}]{Generative Adversarial Network}
\newacro{GPU}[\AC@hyperlink{GPU}{GPU}]{Graphics Processing Unit}
\newacro{GRU}[\AC@hyperlink{GRU}{GRU}]{Gated Recurrent Unit}
\newacro{HRI}[\AC@hyperlink{HRI}{HRI}]{Human Robot Interaction}
\newacro{KLD}[\AC@hyperlink{KLD}{KLD}]{Kullback-Leibler Divergence}
\newacro{LSTM}[\AC@hyperlink{LSTM}{LSTM}]{Long Short-Term Memory}
\newacro{MAE}[\AC@hyperlink{MAE}{MAE}]{Multimodal Autoencoder}
\newacro{MFCC}[\AC@hyperlink{MFCC}{MFCC}]{Mel Frequency Cepstrum Coefficient}
\newacro{MLP}[\AC@hyperlink{MLP}{MLP}]{Multi-Layer Perceptron}
\newacro{MRL}[\AC@hyperlink{MRL}{MRL}]{Multimodal Representation Learning}
\newacro{MSE}[\AC@hyperlink{MSE}{MSE}]{Mean Squared Error}
\newacro{NLP}[\AC@hyperlink{NLP}{NLP}]{Natural Language Processing}
\newacro{NLU}[\AC@hyperlink{NLU}{NLU}]{Natural Language Understanding}
\newacro{PRNG}[\AC@hyperlink{PRNG}{PRNG}]{Pseudo Random Number Generator}
\newacro{Relu}[\AC@hyperlink{Relu}{Relu}]{Rectified Linear Unit}
\newacro{RGB}[\AC@hyperlink{RGB}{RGB}]{Red Green Blue}
\newacro{RNN}[\AC@hyperlink{RNN}{RNN}]{Recurrent Neural Network}
\newacro{SVM}[\AC@hyperlink{SVM}{SVM}]{Support Vector Machine}
\newacro{Tanh}[\AC@hyperlink{Tanh}{Tanh}]{Hyperbolic Tangent}
\newacro{TPT}[\AC@hyperlink{TPT}{TPT}]{Two Process Theory}
\newacro{VAE}[\AC@hyperlink{VAE}{VAE}]{Variational Autoencoder}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter1}{{1}{1}{Introduction}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Multimodal Representation Learning}{1}{section.4}}
\AC@undonewlabel{acro:MRL}
\newlabel{acro:MRL}{{1.1}{1}{Multimodal Representation Learning}{section*.5}{}}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MRL}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The Stirling engine from my desk.}}{2}{figure.7}}
\newlabel{fig:stirling}{{1.1}{2}{The Stirling engine from my desk}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{2}{section.6}}
\citation{smith2008infants}
\citation{yurovsky2013statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces The bi-directional nature of language learning.}}{3}{figure.8}}
\newlabel{fig:bi_ll}{{1.2}{3}{The bi-directional nature of language learning}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Associations among words and objects across multiple ambiguous scenes allow learners to find the proper mapping of words: \textsc  {Circle}, \textsc  {Square}, \textsc  {Green} and \textsc  {Blue} to the shapes and colours: \textit  {Circle}, \textit  {Square}, \textit  {Green} and \textit  {Blue}}}{3}{figure.9}}
\newlabel{fig:cross_sitch}{{1.3}{3}{Associations among words and objects across multiple ambiguous scenes allow learners to find the proper mapping of words: \textsc {Circle}, \textsc {Square}, \textsc {Green} and \textsc {Blue} to the shapes and colours: \textit {Circle}, \textit {Square}, \textit {Green} and \textit {Blue}}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Images generated from textual descriptions.}}{4}{figure.10}}
\newlabel{fig:mrl_teaser}{{1.4}{4}{Images generated from textual descriptions}{figure.10}{}}
\acronymused{MRL}
\acronymused{MRL}
\citation{rosenblatt1958perceptron}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}A Primer on Artificial Neural Networks}{5}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter2}{{2}{5}{A Primer on Artificial Neural Networks}{chapter.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{5}{section.12}}
\AC@undonewlabel{acro:ANN}
\newlabel{acro:ANN}{{2.1}{5}{Introduction}{section*.13}{}}
\acronymused{ANN}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Perceptrons}{5}{section.14}}
\newlabel{sec:percep}{{2.2}{5}{Perceptrons}{section.14}{}}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}What is a Perceptron}{5}{subsection.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A perceptron with three binary inputs, $x_0, x_1, x_2$.}}{5}{figure.16}}
\newlabel{fig:perceptron}{{2.1}{5}{A perceptron with three binary inputs, $x_0, x_1, x_2$}{figure.16}{}}
\newlabel{eqn:percep}{{2.1}{6}{What is a Perceptron}{equation.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Multi-Layer Perceptron}{6}{subsection.18}}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.2.2}{6}{Multi-Layer Perceptron}{section*.19}{}}
\acronymused{MLP}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A multi-layer perceptron consisting of three perceptrons arranged in two layers, with three binary inputs, $x_0, x_1, x_2$.}}{7}{figure.20}}
\newlabel{fig:mlp}{{2.2}{7}{A multi-layer perceptron consisting of three perceptrons arranged in two layers, with three binary inputs, $x_0, x_1, x_2$}{figure.20}{}}
\acronymused{MLP}
\acronymused{MLP}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{7}{section.21}}
\newlabel{eqn:proport}{{2.2}{7}{Activation Functions}{equation.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Visualisation of the perceptron activation function. Where z is the activation $\DOTSB \sum@ \slimits@ _{j} w_j x_j + b$ and the output is the activation function defined in \autoref  {eqn:percep} }}{8}{figure.22}}
\newlabel{fig:activation_percep}{{2.3}{8}{Visualisation of the perceptron activation function. Where z is the activation $\sum _{j} w_j x_j + b$ and the output is the activation function defined in \autoref {eqn:percep}}{figure.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Sigmoid Neurons}{8}{subsection.24}}
\newlabel{eqn:sig}{{2.3}{8}{Sigmoid Neurons}{equation.25}{}}
\newlabel{eqn:z_act}{{2.4}{8}{Sigmoid Neurons}{equation.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Visualisation of the Sigmoid activation function. Where z is the activation $\DOTSB \sum@ \slimits@ _{j} w_j x_j + b$ and $\sigma $(z) is the activation function defined in \autoref  {eqn:sig} }}{9}{figure.27}}
\newlabel{fig:activation_sigmoid}{{2.4}{9}{Visualisation of the Sigmoid activation function. Where z is the activation $\sum _{j} w_j x_j + b$ and $\sigma $(z) is the activation function defined in \autoref {eqn:sig}}{figure.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{Other activation functions}{9}{subsubsection*.28}}
\AC@undonewlabel{acro:Tanh}
\newlabel{acro:Tanh}{{2.3.1}{9}{Other activation functions}{section*.29}{}}
\acronymused{Tanh}
\AC@undonewlabel{acro:Relu}
\newlabel{acro:Relu}{{2.3.1}{9}{Other activation functions}{section*.30}{}}
\acronymused{Relu}
\acronymused{Tanh}
\acronymused{Relu}
\acronymused{Tanh}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Learning Algorithms}{9}{section.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualisation of the hyperbolic tangent activation function. Where z is the activation $\DOTSB \sum@ \slimits@ _{j} w_j x_j + b$ and $\sigma $(z) is the Hyperbolic Tangent Function applied to z.}}{10}{figure.31}}
\newlabel{fig:activation_tanh}{{2.5}{10}{Visualisation of the hyperbolic tangent activation function. Where z is the activation $\sum _{j} w_j x_j + b$ and $\sigma $(z) is the Hyperbolic Tangent Function applied to z}{figure.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Visualisation of the rectified linear unit activation function.Where z is the activation $\DOTSB \sum@ \slimits@ _{j} w_j x_j + b$ and $\sigma $(z) is the Relu activation function applied to z.}}{10}{figure.32}}
\newlabel{fig:activation_relu}{{2.6}{10}{Visualisation of the rectified linear unit activation function.Where z is the activation $\sum _{j} w_j x_j + b$ and $\sigma $(z) is the Relu activation function applied to z}{figure.32}{}}
\citation{lecun1998mnist}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Types of Training}{11}{subsection.34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Cost functions}{11}{subsection.35}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Squared Error}{12}{subsubsection*.36}}
\AC@undonewlabel{acro:MSE}
\newlabel{acro:MSE}{{2.4.2}{12}{Mean Squared Error}{section*.37}{}}
\acronymused{MSE}
\newlabel{eqn:mse}{{2.5}{12}{Mean Squared Error}{equation.38}{}}
\acronymused{MSE}
\acronymused{MSE}
\@writefile{toc}{\contentsline {subsubsection}{Cross-Entropy}{12}{subsubsection*.39}}
\newlabel{eqn:xntrpy}{{2.6}{12}{Cross-Entropy}{equation.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Kullback-Leibler Divergence}{13}{subsubsection*.41}}
\AC@undonewlabel{acro:KLD}
\newlabel{acro:KLD}{{2.4.2}{13}{Kullback-Leibler Divergence}{section*.42}{}}
\acronymused{KLD}
\acronymused{KLD}
\newlabel{eqn:kld}{{2.7}{13}{Kullback-Leibler Divergence}{equation.43}{}}
\acronymused{KLD}
\acronymused{KLD}
\acronymused{KLD}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Gradient Descent}{13}{subsection.44}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The cost landscape of a bivariate function. Where $w_0$ and $w_1$ are the weights of a neural network and $C$ is its error.}}{14}{figure.45}}
\newlabel{fig:costscape}{{2.7}{14}{The cost landscape of a bivariate function. Where $w_0$ and $w_1$ are the weights of a neural network and $C$ is its error}{figure.45}{}}
\newlabel{eqn:cost_bivar}{{2.8}{14}{Gradient Descent}{equation.46}{}}
\newlabel{eqn:delta_C}{{2.9}{14}{Gradient Descent}{equation.47}{}}
\newlabel{eqn:nabla_c}{{2.10}{14}{Gradient Descent}{equation.48}{}}
\newlabel{eqn:delta_c_sub}{{2.11}{15}{Gradient Descent}{equation.49}{}}
\newlabel{eqn:delta_w_eta}{{2.12}{15}{Gradient Descent}{equation.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Backpropagation}{15}{subsection.51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Extensions and Improvements}{15}{subsection.52}}
\@writefile{toc}{\contentsline {subsubsection}{Learning Rate Schedule}{15}{subsubsection*.53}}
\newlabel{sec:lr_sch}{{2.4.5}{15}{Learning Rate Schedule}{subsubsection*.53}{}}
\citation{kingma2014adam}
\citation{zeiler2012adadelta}
\citation{senior2013empirical}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Visualisation of error minimisation in two dimensions for different learning rate schedules. Blue: decreaseing learning rate, Red: large fixed learning rate. Where $\Delta w$ is the change of weights of a neural network and and $C$ is its error.}}{16}{figure.54}}
\newlabel{fig:lr_schedule}{{2.8}{16}{Visualisation of error minimisation in two dimensions for different learning rate schedules. Blue: decreaseing learning rate, Red: large fixed learning rate. Where $\Delta w$ is the change of weights of a neural network and and $C$ is its error}{figure.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Convolutional Neural Networks}{17}{section.56}}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Visualisation of the cost (black), its first derivative (blue) and second derivative (red). Where $\Delta w$ is the change of weights of a neural network and and $C$ is its error.}}{18}{figure.55}}
\newlabel{fig:2ndordr}{{2.9}{18}{Visualisation of the cost (black), its first derivative (blue) and second derivative (red). Where $\Delta w$ is the change of weights of a neural network and and $C$ is its error}{figure.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces A british short-hair cat (left) and the same image, with its pixels scrambled (right).}}{18}{figure.57}}
\newlabel{fig:cat}{{2.10}{18}{A british short-hair cat (left) and the same image, with its pixels scrambled (right)}{figure.57}{}}
\citation{dumoulin2016guide}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces A single step of computing a convolution }}{19}{figure.61}}
\newlabel{fig:oneStepConv}{{2.11}{19}{A single step of computing a convolution}{figure.61}{}}
\AC@undonewlabel{acro:ConvNet}
\newlabel{acro:ConvNet}{{2.5}{19}{Convolutional Neural Networks}{section*.58}{}}
\acronymused{ConvNet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}What is Convolution?}{19}{subsection.59}}
\acronymused{ConvNet}
\newlabel{eqn:convolution}{{2.13}{19}{What is Convolution?}{equation.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces An example of convolution of a (3x3) array with a (3x3) kernel.}}{20}{figure.62}}
\newlabel{fig:convolve}{{2.12}{20}{An example of convolution of a (3x3) array with a (3x3) kernel}{figure.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces The implementation of the convolution of a (3x3) array with a (3x3) kernel.}}{21}{figure.63}}
\newlabel{fig:convImpl}{{2.13}{21}{The implementation of the convolution of a (3x3) array with a (3x3) kernel}{figure.63}{}}
\citation{radford2015unsupervised}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Using Convolution Kernels in a Neural Network}{22}{subsection.64}}
\acronymused{ConvNet}
\acronymused{ConvNet}
\@writefile{toc}{\contentsline {subsubsection}{Strided Convolution}{22}{subsubsection*.65}}
\citation{zeiler2010deconvolutional}
\citation{kingma2013auto}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Transposed Convolutions}{23}{subsection.66}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Autoencoders}{23}{section.68}}
\AC@undonewlabel{acro:AE}
\newlabel{acro:AE}{{2.6}{23}{Autoencoders}{section*.69}{}}
\acronymused{AE}
\acronymused{ConvNet}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{2.6}{23}{Autoencoders}{section*.70}{}}
\acronymused{LSTM}
\acronymused{AE}
\acronymused{AE}
\acronymused{AE}
\AC@undonewlabel{acro:MAE}
\newlabel{acro:MAE}{{2.6}{23}{Autoencoders}{section*.71}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Variational Autoencoders}{24}{subsection.73}}
\acronymused{AE}
\AC@undonewlabel{acro:VAE}
\newlabel{acro:VAE}{{2.6.1}{24}{Variational Autoencoders}{section*.74}{}}
\acronymused{VAE}
\acronymused{MSE}
\acronymused{KLD}
\acronymused{AE}
\acronymused{VAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Multimodal Autoencoders}{24}{subsection.75}}
\acronymused{MAE}
\acronymused{AE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces The implementation of the transposed convolution of a (3x3) array with a (3x3) kernel.}}{25}{figure.67}}
\newlabel{fig:transConvImpl}{{2.14}{25}{The implementation of the transposed convolution of a (3x3) array with a (3x3) kernel}{figure.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces An \ac {AE} trained using the \ac {MSE} of its input and reconstruction of the input (Input*). The red arrow shows how the error is backpropgated through the network.}}{26}{figure.72}}
\acronymused{AE}
\acronymused{MSE}
\newlabel{fig:ae}{{2.15}{26}{An \ac {AE} trained using the \ac {MSE} of its input and reconstruction of the input (Input*). The red arrow shows how the error is backpropgated through the network}{figure.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces A Multimodal Autoencoder}}{26}{figure.76}}
\newlabel{fig:mae}{{2.16}{26}{A Multimodal Autoencoder}{figure.76}{}}
\citation{rosenblatt1958perceptron}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background}{27}{chapter.77}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter3}{{3}{27}{Background}{chapter.77}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{27}{section.78}}
\newlabel{Lit:Intro}{{3.1}{27}{Introduction}{section.78}{}}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}What are Artifical Neural Networks Good at?}{27}{section.79}}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{ANN}
\acronymused{MLP}
\acronymused{ANN}
\acronymused{ANN}
\AC@undonewlabel{acro:GPU}
\newlabel{acro:GPU}{{3.2}{27}{What are Artifical Neural Networks Good at?}{section*.80}{}}
\acronymused{GPU}
\citation{vinyals2019alphastar}
\citation{ma2004facial}
\citation{levi2015emotion}
\citation{kussul2017deep,qi2017pointnet}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\citation{springenberg2014striving}
\citation{srivastava2014dropout}
\citation{simonyan2014very}
\citation{he2016deep}
\citation{hochreiter1998vanishing}
\citation{huang2017densely}
\acronymused{ANN}
\acronymused{GPU}
\acronymused{ANN}
\acronymused{ANN}
\AC@undonewlabel{acro:DL}
\newlabel{acro:DL}{{3.2}{28}{What are Artifical Neural Networks Good at?}{section*.81}{}}
\acronymused{DL}
\acronymused{ANN}
\acronymused{DL}
\AC@undonewlabel{acro:NLP}
\newlabel{acro:NLP}{{3.2}{28}{What are Artifical Neural Networks Good at?}{section*.82}{}}
\acronymused{NLP}
\acronymused{MRL}
\acronymused{NLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Classification}{28}{subsection.83}}
\acronymused{MRL}
\acronymused{MRL}
\@writefile{toc}{\contentsline {subsubsection}{AlexNet}{28}{subsubsection*.84}}
\acronymused{ConvNet}
\citation{szegedy2015going}
\citation{szegedy2015going}
\citation{szegedy2016rethinking,szegedy2017inception}
\citation{mirza2014conditional,odena2017conditional}
\citation{vinyals2015show,lebret2015phrase,donahue2015long,jia2015guiding,rohrbach2014coherent,rohrbach2013translating,yao2015describing,yao2015video,venugopalan2014translating,johnson2016densecap,ordonez2011im2text}
\citation{zhu2017unpaired}
\citation{lu2013speech}
\@writefile{toc}{\contentsline {subsubsection}{Residual Connections}{29}{subsubsection*.85}}
\@writefile{toc}{\contentsline {subsubsection}{Inception: Multiscale Convolutions}{29}{subsubsection*.86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Generation}{29}{subsection.87}}
\AC@undonewlabel{acro:GAN}
\newlabel{acro:GAN}{{3.2.2}{29}{Generation}{section*.88}{}}
\acronymused{GAN}
\acronymused{AE}
\citation{mirza2014conditional}
\citation{odena2017conditional}
\citation{mirza2014conditional}
\citation{zhu2017unpaired}
\citation{zhu2017unpaired}
\citation{hochreiter1997long}
\citation{vinyals2015show,venugopalan2014translating,johnson2016densecap}
\citation{johnson2016densecap}
\citation{simonyan2014very}
\citation{keller}
\acronymused{AE}
\@writefile{toc}{\contentsline {subsubsection}{Generative Adversarial Networks}{30}{subsubsection*.89}}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\@writefile{toc}{\contentsline {subsubsection}{Modality Translation: Image Caption Generation}{30}{subsubsection*.90}}
\acronymused{GAN}
\acronymused{ConvNet}
\acronymused{LSTM}
\citation{donahue2015long}
\citation{yao2015describing,yao2015video}
\citation{rohrbach2014coherent,rohrbach2013translating}
\citation{krizhevsky2012imagenet}
\citation{radford2015unsupervised,silberer2014learning,wavenet,vincent2010stacked,mikolov2013distributed,mikolov2013efficient,mikolov2013linguistic,feng2010visual,eslami2018neural,donahue2019large}
\citation{repRev}
\citation{vinyals2015show,venugopalan2014translating,johnson2016densecap}
\citation{johnson2016densecap}
\citation{simonyan2014very}
\citation{mikolov2013distributed,mikolov2013efficient,mikolov2013linguistic}
\citation{repRev}
\acronymused{ConvNet}
\acronymused{ConvNet}
\acronymused{LSTM}
\acronymused{ConvNet}
\acronymused{ConvNet}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Representation Learning}{31}{subsection.91}}
\acronymused{ANN}
\citation{pu2016variational}
\citation{lin2014microsoft}
\citation{simonyan2014very}
\citation{szegedy2015going}
\citation{simonyan2014very,szegedy2015going}
\citation{repRev}
\@writefile{toc}{\contentsline {subsubsection}{Natural Language Processing}{32}{subsubsection*.92}}
\acronymused{NLP}
\@writefile{toc}{\contentsline {subsubsection}{Autoencoders}{32}{subsubsection*.93}}
\acronymused{AE}
\acronymused{AE}
\acronymused{VAE}
\AC@undonewlabel{acro:SVM}
\newlabel{acro:SVM}{{3.2.3}{32}{Autoencoders}{section*.94}{}}
\acronymused{SVM}
\acronymused{VAE}
\acronymused{GPU}
\acronymused{VAE}
\acronymused{SVM}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{3.2.3}{33}{Autoencoders}{section*.95}{}}
\acronymused{RNN}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{VAE}
\citation{bleu}
\citation{wavenet}
\citation{radford2015unsupervised}
\citation{van2017neural}
\citation{vincent2010stacked,lu2013speech}
\citation{dunbar2017zero}
\citation{wavenet}
\acronymused{VAE}
\acronymused{VAE}
\acronymused{AE}
\acronymused{AE}
\citation{van2017neural}
\citation{wavenet}
\citation{vinyals2019alphastar}
\citation{krizhevsky2012imagenet,simonyan2014very,szegedy2015going,szegedy2016rethinking,szegedy2017inception,he2016deep,huang2017densely,russakovsky2015imagenet,chiu2018state,eslami2018neural}
\citation{nguyen2015deep}
\citation{szegedy2013intriguing}
\citation{moosavi2016deepfool}
\citation{nguyen2015deep}
\acronymused{VAE}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}What are Artificial Neural Networks Bad at?}{35}{section.96}}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Artificial Neural Networks are Easily Fooled}{35}{subsection.97}}
\acronymused{ANN}
\citation{barsalou2008grounded}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A cat that looks like a crow.}}{36}{figure.98}}
\newlabel{fig:catcrow}{{3.1}{36}{A cat that looks like a crow}{figure.98}{}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ConvNet}
\acronymused{ConvNet}
\citation{wavenet}
\citation{krizhevsky2012imagenet}
\citation{vinyals2015show}
\citation{ImNet}
\citation{ha2018world,ha2018recurrent}
\citation{ha2018world,ha2018recurrent}
\citation{vinyals2016matching}
\citation{repRev}
\citation{searle1980minds}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Data Ineffciency}{37}{subsection.99}}
\acronymused{ANN}
\acronymused{GPU}
\AC@undonewlabel{acro:CPU}
\newlabel{acro:CPU}{{3.3.2}{37}{Data Ineffciency}{section*.100}{}}
\acronymused{CPU}
\acronymused{ANN}
\acronymused{ANN}
\citation{turing2009computing}
\citation{cangelosi2000robotic}
\citation{harnad1990symbol}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}How to get off the Symbol Grounding Merry-Go-Round}{38}{section.101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}What is Symbol Grounding?}{38}{subsection.102}}
\citation{steels2008symbol}
\citation{barsalou2008grounded}
\citation{slater1999intermodal}
\citation{masland2012neuronal,fantz1963pattern,johnson2015two}
\citation{webb2015mother,reid2017human}
\citation{johnson2015two}
\citation{pfeifer2006body,smith2005development}
\citation{barsalou2008grounded}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A Semiotic Network.}}{39}{figure.103}}
\newlabel{fig:semNet}{{3.2}{39}{A Semiotic Network}{figure.103}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}How do Humans do it?}{39}{subsection.104}}
\acronymused{ANN}
\citation{nicolelis1995sensorimotor}
\citation{mordvintsev2015inceptionism}
\citation{glenberg2015few}
\citation{barsalou2008grounded}
\citation{glenberg2015few}
\citation{barsalou2008grounded}
\citation{pfeifer2006body}
\@writefile{toc}{\contentsline {subsubsection}{Embodiment}{40}{subsubsection*.105}}
\citation{Goldin-MeadowSusan2015Fata,de2014making,rucinski2012robotic}
\citation{bahrick2000intersensory}
\citation{smith2008infants}
\citation{walker2010preverbal,fischer2011multi,scott20122,slater1999intermodal}
\citation{webb2015mother}
\@writefile{toc}{\contentsline {paragraph}{Multimodality}{41}{paragraph*.106}}
\@writefile{toc}{\contentsline {paragraph}{Development}{41}{paragraph*.107}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\citation{fantz1963pattern}
\citation{reid2017human}
\citation{fernald1987acoustic}
\citation{lohan2012contingency,lohan2012tutor}
\citation{pezzulo2013computational}
\citation{johnson2015two}
\citation{bambach2017egocentric}
\acronymused{ANN}
\@writefile{toc}{\contentsline {paragraph}{Biological Filters}{42}{paragraph*.108}}
\AC@undonewlabel{acro:TPT}
\newlabel{acro:TPT}{{3.4.2}{42}{Biological Filters}{section*.109}{}}
\acronymused{TPT}
\acronymused{TPT}
\acronymused{TPT}
\citation{masland2012neuronal}
\citation{olveczky2003segregation}
\citation{oxenham2018we}
\citation{barsalou2008grounded}
\citation{cangelosi2000robotic}
\citation{lemonlearning,yu2017learning}
\citation{lemonlearning}
\citation{yu2017learning}
\citation{coradeschi2000anchoring,coradeschi2003introduction}
\citation{coradeschi2013short}
\citation{vogt2002physical}
\citation{cangelosi2006grounding}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}How do Machines do it?}{43}{subsection.110}}
\acronymused{NLP}
\acronymused{NLP}
\acronymused{NLP}
\citation{lemonlearning,yu2017learning}
\citation{coradeschi2013short}
\citation{nakamura2009grounding,nakamura2011grounding}
\citation{cangelosi2000robotic}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A depiction of Anchoring relative to a Semiotic Network.}}{44}{figure.111}}
\newlabel{fig:ank}{{3.3}{44}{A depiction of Anchoring relative to a Semiotic Network}{figure.111}{}}
\acronymused{MRL}
\acronymused{ANN}
\citation{cangelosi2000robotic}
\citation{cangelosi2001adaptive,cangelosi2002symbol,cangelosi1998emergence,horst2019object,cangelosi2018speech,cangelosi2008italk,broz2014italk}
\citation{cangelosi1998emergence}
\citation{cangelosi1998emergence}
\citation{vinyals2015show}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A Semiotic Network modified to demonstrate Cangelosi's approach to symbol grounding.}}{45}{figure.112}}
\newlabel{fig:percNet}{{3.4}{45}{A Semiotic Network modified to demonstrate Cangelosi's approach to symbol grounding}{figure.112}{}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\citation{ngiam2011multimodal}
\citation{silberer2014learning}
\citation{ngiam2011multimodal}
\citation{mcgurk1976hearing}
\citation{silberer2014learning}
\citation{samuel1997lexical}
\citation{barsalou2008grounded}
\@writefile{toc}{\contentsline {subsubsection}{My Approach to Symbol Grounding}{46}{subsubsection*.113}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Why use MAEs?}{46}{subsubsection*.114}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Learning and Pretraining}{46}{subsubsection*.115}}
\citation{lee2008sparse}
\citation{vinyals2015show}
\citation{barsalou2008grounded}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\citation{barsalou2008grounded}
\citation{mcgurk1976hearing}
\citation{ma2009lip}
\citation{ma2009lip,samuel1997lexical}
\citation{ngiam2011multimodal}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Sensory Redundancy: Are Two Heads Better Than One?}{48}{chapter.116}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter4}{{4}{48}{Sensory Redundancy: Are Two Heads Better Than One?}{chapter.116}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Why Have One Sensor When Two Are Better?}{48}{section.117}}
\citation{hammami2009tree,hammami2010improved}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Generating Images From Natural Language}{49}{section.118}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Aims}{49}{subsection.119}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}UCU Arabic Spoken Digits and MNIST}{49}{subsection.120}}
\newlabel{sec:UCU}{{4.2.2}{49}{UCU Arabic Spoken Digits and MNIST}{subsection.120}{}}
\@writefile{toc}{\contentsline {subsubsection}{UCU Arabic Spoken Digits}{49}{subsubsection*.121}}
\AC@undonewlabel{acro:MFCC}
\newlabel{acro:MFCC}{{4.2.2}{49}{UCU Arabic Spoken Digits}{section*.122}{}}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{Padding of Utterances}{49}{paragraph*.123}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Mean number of samples for each digit in the UCU Arabic Spoken Digits Dataset. Red bars show the standard devaition in length for each each digit.}}{50}{figure.124}}
\newlabel{fig:ucu_dig_length}{{4.1}{50}{Mean number of samples for each digit in the UCU Arabic Spoken Digits Dataset. Red bars show the standard devaition in length for each each digit}{figure.124}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Mean length and standard deviation of digits in the UCU dataset.}}{50}{table.125}}
\newlabel{tab:UCU_sampLen}{{4.1}{50}{Mean length and standard deviation of digits in the UCU dataset}{table.125}{}}
\acronymused{MFCC}
\citation{lecun1998mnist}
\citation{barsalou2008grounded}
\@writefile{toc}{\contentsline {subsubsection}{MNIST Handwritten Digits}{51}{subsubsection*.126}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Problem Description}{51}{subsection.127}}
\@writefile{toc}{\contentsline {subsubsection}{Classification}{51}{subsubsection*.128}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Bidirectional Symbol Grounding}{51}{subsubsection*.129}}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Experiment Details}{52}{subsection.130}}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{Combining Datasets}{52}{paragraph*.131}}
\newlabel{sec:UCU_mnist_comb}{{4.2.4}{52}{Combining Datasets}{paragraph*.131}{}}
\@writefile{toc}{\contentsline {paragraph}{Merging Modalities}{52}{paragraph*.132}}
\newlabel{eqn:concat}{{4.1}{52}{Merging Modalities}{equation.133}{}}
\newlabel{eqn:add}{{4.2}{52}{Merging Modalities}{equation.134}{}}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Training Procedures}{52}{subsubsection*.135}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Bimodal}{53}{paragraph*.136}}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Randomly Degraded}{53}{paragraph*.137}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Testing Conditions}{53}{subsubsection*.138}}
\acronymused{MAE}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{Bimodal}{53}{paragraph*.139}}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{Image Only}{53}{paragraph*.140}}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{\ac {MFCC} Only}{54}{paragraph*.141}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Network Description}{54}{subsubsection*.142}}
\@writefile{toc}{\contentsline {paragraph}{Baseline Models}{54}{paragraph*.143}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Image autoencoder and classifier. Layer 6c performs classification, whilst the branch starting at layer 6 regenerates the image.}}{54}{table.144}}
\newlabel{tab:MNIST_AE_description}{{4.2}{54}{Image autoencoder and classifier. Layer 6c performs classification, whilst the branch starting at layer 6 regenerates the image}{table.144}{}}
\@writefile{toc}{\contentsline {paragraph}{Multimodal Autoencoder}{54}{paragraph*.146}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces \ac {MFCC} autoencoder and classifier. Layer 7c performs classification, whilst the branch starting at layer 7 regenerates the \acp {MFCC}. The addition of reshape layers is to ensure the final shape of the regenerated \acp {MFCC} matches the target shape whilst the embedding shape matches that of the embedding of the image autoencoder.}}{55}{table.145}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\newlabel{tab:UCU_AE_description}{{4.3}{55}{\ac {MFCC} autoencoder and classifier. Layer 7c performs classification, whilst the branch starting at layer 7 regenerates the \acp {MFCC}. The addition of reshape layers is to ensure the final shape of the regenerated \acp {MFCC} matches the target shape whilst the embedding shape matches that of the embedding of the image autoencoder}{table.145}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Results}{55}{subsection.148}}
\@writefile{toc}{\contentsline {subsubsection}{Classification Results}{55}{subsubsection*.149}}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MFCC}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Image and MFCC multimodal autoencoder. Layers marked i, m, im and c are image, MFCC, image and MFCC and classification respectively.}}{56}{table.147}}
\newlabel{tab:UCU_MNIST_MAE_description}{{4.4}{56}{Image and MFCC multimodal autoencoder. Layers marked i, m, im and c are image, MFCC, image and MFCC and classification respectively}{table.147}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Mean Squared Errors for the Bimodal testing condition, comparing the Bimodal (Bi) and Randomly Degraded (RD) training procedures.}}{57}{table.150}}
\newlabel{tab:mnist_ucu_bi_res}{{4.5}{57}{Mean Squared Errors for the Bimodal testing condition, comparing the Bimodal (Bi) and Randomly Degraded (RD) training procedures}{table.150}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Mean Squared Errors for the Image Only testing condition, comparing the Bimodal (Bi) and Randomly Degraded (RD) training procedures.}}{57}{table.151}}
\newlabel{tab:mnist_ucu_im_res}{{4.6}{57}{Mean Squared Errors for the Image Only testing condition, comparing the Bimodal (Bi) and Randomly Degraded (RD) training procedures}{table.151}{}}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Reconstruction Results}{57}{subsubsection*.153}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A selection of randomly sampled digits generated in different training and testing conditions for both \textit  {Add} and \textit  {Concat} merging methods.}}{57}{figure.154}}
\newlabel{fig:mnistDigits}{{4.2}{57}{A selection of randomly sampled digits generated in different training and testing conditions for both \textit {Add} and \textit {Concat} merging methods}{figure.154}{}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Mean Squared Errors for the MFCC Only testing condition, comparing the Bimodal (Bi) and Randomly Degraded (RD) training procedures. }}{58}{table.152}}
\newlabel{tab:mnist_ucu_mfcc_res}{{4.7}{58}{Mean Squared Errors for the MFCC Only testing condition, comparing the Bimodal (Bi) and Randomly Degraded (RD) training procedures}{table.152}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Two examples of the digit ``5'' being generated in different training and testing conditions for both \textit  {Add} and \textit  {Concat} merging methods.}}{58}{figure.155}}
\newlabel{fig:5s}{{4.3}{58}{Two examples of the digit ``5'' being generated in different training and testing conditions for both \textit {Add} and \textit {Concat} merging methods}{figure.155}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Discussion}{58}{subsection.156}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion of Classification Results}{58}{subsubsection*.157}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {subsubsection}{Discussion of Reconstruction Results}{59}{subsubsection*.158}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Image reconstruction from MFCCs}{60}{paragraph*.159}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{MFCC reconstruction from Images}{60}{paragraph*.160}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {paragraph}{Effects of randomly degrading inputs}{61}{paragraph*.161}}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MFCC}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Multiple generations of the same digit}{61}{paragraph*.162}}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Conclusion}{62}{section.163}}
\citation{mikolov2013distributed}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Magical Vectors and Where to Find Them}{63}{chapter.164}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter5}{{5}{63}{Magical Vectors and Where to Find Them}{chapter.164}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}ANN Latent Space: the Final Frontier}{63}{section.165}}
\acronymused{ANN}
\acronymused{MAE}
\acronymused{ANN}
\acronymused{ANN}
\newlabel{eqn:mikolov}{{5.1}{63}{ANN Latent Space: the Final Frontier}{equation.166}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Vector Arithmetic}{64}{section.167}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of how novel images can be generated using Vector Arithmetic on image and word embeddings.}}{65}{figure.168}}
\newlabel{fig:vectorArthexmp}{{5.1}{65}{An example of how novel images can be generated using Vector Arithmetic on image and word embeddings}{figure.168}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Examples of the different, shapes, colours, sizes and positions of objects in the ArtS dataset. The top left object would be described as ``\textsc  {big indigo circle, top left}''.}}{66}{figure.169}}
\newlabel{fig:shapes}{{5.2}{66}{Examples of the different, shapes, colours, sizes and positions of objects in the ArtS dataset. The top left object would be described as ``\textsc {big indigo circle, top left}''}{figure.169}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Artificial Shapes Dataset}{66}{section.170}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Dataset Description}{66}{subsection.171}}
\AC@undonewlabel{acro:RGB}
\newlabel{acro:RGB}{{5.3.1}{66}{Dataset Description}{section*.172}{}}
\acronymused{RGB}
\acronymused{RGB}
\acronymused{RGB}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Artificial Shapes dataset description.}}{67}{table.173}}
\newlabel{tab:Arts_desc}{{5.1}{67}{Artificial Shapes dataset description}{table.173}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Problem Description}{67}{subsection.174}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Network Description}{68}{subsection.175}}
\acronymused{ANN}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiments with the ArtS Dataset}{68}{section.176}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Experiment 1}{69}{subsection.177}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Experiment 1 data subset.}}{69}{table.178}}
\newlabel{tab:exp1_data}{{5.2}{69}{Experiment 1 data subset}{table.178}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{70}{subsubsection*.179}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Experiment 1: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the description output.}}{70}{figure.180}}
\newlabel{fig:graph331}{{5.3}{70}{Experiment 1: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the description output}{figure.180}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Experiment 1: Total MSE for different embedding sizes for the three testing conditions. An Embedding Size of 200 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$.)}}{71}{table.181}}
\newlabel{tab:res331}{{5.3}{71}{Experiment 1: Total MSE for different embedding sizes for the three testing conditions. An Embedding Size of 200 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$.)}{table.181}{}}
\acronymused{RGB}
\acronymused{RGB}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{71}{paragraph*.182}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Images generated from descriptions with an embedding size of 8 or 200 neurons.}}{72}{figure.183}}
\newlabel{fig:331multi}{{5.4}{72}{Images generated from descriptions with an embedding size of 8 or 200 neurons}{figure.183}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{72}{paragraph*.185}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Experiment 1, runs A-D: Images generated from shape names (\textsc  {Circle, Rectangle, Triangle}) for different sizes of embedding.}}{73}{figure.184}}
\newlabel{fig:331shapes}{{5.5}{73}{Experiment 1, runs A-D: Images generated from shape names (\textsc {Circle, Rectangle, Triangle}) for different sizes of embedding}{figure.184}{}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{73}{subsubsection*.186}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Experiment 2}{74}{subsection.187}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Experiment 2 data subset.}}{74}{table.188}}
\newlabel{tab:exp2_data}{{5.4}{74}{Experiment 2 data subset}{table.188}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{74}{subsubsection*.189}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Experiment 2: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the description output.}}{75}{figure.190}}
\newlabel{fig:graph333}{{5.6}{75}{Experiment 2: MSE for reconstruction of images and descriptions under different testing conditions. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the description output}{figure.190}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Experiment 2: Total MSE for different embedding sizes. An Embedding Size of 296 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$.)}}{75}{table.191}}
\newlabel{tab:res333}{{5.5}{75}{Experiment 2: Total MSE for different embedding sizes. An Embedding Size of 296 neurons provides the minimum reconstruction error. (All values are $\times 10^{-3}$.)}{table.191}{}}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{76}{paragraph*.192}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Experiment 2: Images generated from descriptions with an embedding size of 296 neurons.}}{76}{figure.193}}
\newlabel{fig:333multi}{{5.7}{76}{Experiment 2: Images generated from descriptions with an embedding size of 296 neurons}{figure.193}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Experiment 2 run A: Images generated of each word (\textsc  {Circle, Rectangle, Triangle, Big, Medium, Small, Red, Green, Blue, Centre-Left, Centre-Centre, Centre-Right}) for different embedding sizes.}}{77}{figure.194}}
\newlabel{fig:333single}{{5.8}{77}{Experiment 2 run A: Images generated of each word (\textsc {Circle, Rectangle, Triangle, Big, Medium, Small, Red, Green, Blue, Centre-Left, Centre-Centre, Centre-Right}) for different embedding sizes}{figure.194}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Experiment 2, runs A-D: Images generated of each shape for different emedding sizes.}}{78}{figure.195}}
\newlabel{fig:shapes333}{{5.9}{78}{Experiment 2, runs A-D: Images generated of each shape for different emedding sizes}{figure.195}{}}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{78}{paragraph*.197}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{78}{subsubsection*.198}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Experiment 2, run A: Images generated using word pairs using an embedding size of 296 neurons.}}{79}{figure.196}}
\newlabel{fig:2word333}{{5.10}{79}{Experiment 2, run A: Images generated using word pairs using an embedding size of 296 neurons}{figure.196}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Experiment 3}{79}{subsection.200}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Images generated of each shape for different sizes of embedding using the \acp {MAE} trained in experiments 1 and 2 run A.}}{80}{figure.199}}
\acronymused{MAE}
\newlabel{fig:shapes333v331}{{5.11}{80}{Images generated of each shape for different sizes of embedding using the \acp {MAE} trained in experiments 1 and 2 run A}{figure.199}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{80}{subsubsection*.202}}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{80}{paragraph*.204}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Experiment 3 data subset.}}{81}{table.201}}
\newlabel{tab:exp3_data}{{5.6}{81}{Experiment 3 data subset}{table.201}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Experiment 3: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$.)}}{81}{table.203}}
\newlabel{tab:res339}{{5.7}{81}{Experiment 3: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$.)}{table.203}{}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Experiment 3, run A: Images generated from descriptions, starting training from randomly initialised weights.}}{82}{figure.205}}
\newlabel{fig:339multi}{{5.12}{82}{Experiment 3, run A: Images generated from descriptions, starting training from randomly initialised weights}{figure.205}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Experiment 3, run A: Images generated from shape, colour and size words for different weight initialisation conditions.}}{82}{figure.206}}
\newlabel{fig:339single}{{5.13}{82}{Experiment 3, run A: Images generated from shape, colour and size words for different weight initialisation conditions}{figure.206}{}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Experiment 3, run A: Images generated from each position word for different weight initialisation conditions.}}{83}{figure.207}}
\newlabel{fig:339single_pos}{{5.14}{83}{Experiment 3, run A: Images generated from each position word for different weight initialisation conditions}{figure.207}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Experiment 3, run A: Images generated using word pairs.}}{83}{figure.208}}
\newlabel{fig:2word339}{{5.15}{83}{Experiment 3, run A: Images generated using word pairs}{figure.208}{}}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{83}{paragraph*.209}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Experiment 3: Percentage Description Accuracy for different weight initialisations. }}{83}{table.210}}
\newlabel{tab:res339_acc}{{5.8}{83}{Experiment 3: Percentage Description Accuracy for different weight initialisations}{table.210}{}}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces Experiment 3: Total number of weight updates.}}{84}{table.212}}
\newlabel{tab:updatesTotal}{{5.9}{84}{Experiment 3: Total number of weight updates}{table.212}{}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{84}{subsubsection*.211}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MSE}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Experiment 4}{85}{subsection.213}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{85}{subsubsection*.214}}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces Experiment 4: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$.)}}{85}{table.215}}
\newlabel{tab:res739}{{5.10}{85}{Experiment 4: Total MSE for different weight initialisations. (All values are $\times 10^{-3}$.)}{table.215}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{85}{paragraph*.216}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Experiment 4, run A: Images generated from shape, and size word for different weight initialisation conditions.}}{86}{figure.217}}
\newlabel{fig:739single_shape}{{5.16}{86}{Experiment 4, run A: Images generated from shape, and size word for different weight initialisation conditions}{figure.217}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Experiment 4, run A: Images generated from each colour word for different weight initialisation conditions.}}{86}{figure.218}}
\newlabel{fig:739single_col}{{5.17}{86}{Experiment 4, run A: Images generated from each colour word for different weight initialisation conditions}{figure.218}{}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Experiment 4, run A: Images generated from each position word for different weight initialisation conditions.}}{87}{figure.219}}
\newlabel{fig:739single_pos}{{5.18}{87}{Experiment 4, run A: Images generated from each position word for different weight initialisation conditions}{figure.219}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Experiment 4, run A: Images generated using word pairs.}}{87}{figure.220}}
\newlabel{fig:2word739}{{5.19}{87}{Experiment 4, run A: Images generated using word pairs}{figure.220}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{88}{paragraph*.221}}
\@writefile{lot}{\contentsline {table}{\numberline {5.11}{\ignorespaces Experiment 4: Percentage Description Accuracy for different weight initialisations.}}{88}{table.222}}
\newlabel{tab:res739acc}{{5.11}{88}{Experiment 4: Percentage Description Accuracy for different weight initialisations}{table.222}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{88}{subsubsection*.223}}
\acronymused{MAE}
\acronymused{MSE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.12}{\ignorespaces Experiment 4: Total number of weight updates.}}{89}{table.224}}
\newlabel{tab:updatesTotal4}{{5.12}{89}{Experiment 4: Total number of weight updates}{table.224}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.13}{\ignorespaces Experiment 4: Mean Squared Error ($\times 10^{-3}$) and Percentage Description Accuracy for the randomly initialised MAE after 50 epochs of training.}}{89}{table.225}}
\newlabel{tab:res739_50}{{5.13}{89}{Experiment 4: Mean Squared Error ($\times 10^{-3}$) and Percentage Description Accuracy for the randomly initialised MAE after 50 epochs of training}{table.225}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.5}Experiment 5}{90}{subsection.226}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{90}{subsubsection*.227}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lot}{\contentsline {table}{\numberline {5.14}{\ignorespaces Experiment 5: Total MSE. Alternating rows show MSE for the datasubset without the omitted data and MSE for only the omitted data (marked with *). (All values are $\times 10^{-3}$.)}}{90}{table.228}}
\newlabel{tab:res_exp5}{{5.14}{90}{Experiment 5: Total MSE. Alternating rows show MSE for the datasubset without the omitted data and MSE for only the omitted data (marked with *). (All values are $\times 10^{-3}$.)}{table.228}{}}
\acronymused{MSE}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{91}{paragraph*.229}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Experiment 5, run A: Images generated from descriptions of shapes never seen by the \ac {MAE}.}}{91}{figure.230}}
\acronymused{MAE}
\newlabel{fig:739_minus1}{{5.20}{91}{Experiment 5, run A: Images generated from descriptions of shapes never seen by the \ac {MAE}}{figure.230}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Experiment 5, run A: Images generated via Vector Arithmetic by MAE 1.}}{92}{figure.231}}
\newlabel{fig:739_vectorArth}{{5.21}{92}{Experiment 5, run A: Images generated via Vector Arithmetic by MAE 1}{figure.231}{}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{92}{paragraph*.232}}
\@writefile{lot}{\contentsline {table}{\numberline {5.15}{\ignorespaces Experiment 5: Percentage Description Accuracy. Alternating rows show accuracy for the datasubset without the omitted data and MSE for only the omitted data (marked with *).}}{92}{table.233}}
\newlabel{tab:res_exp5_acc}{{5.15}{92}{Experiment 5: Percentage Description Accuracy. Alternating rows show accuracy for the datasubset without the omitted data and MSE for only the omitted data (marked with *)}{table.233}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.6}Discussion}{93}{subsection.234}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusion}{94}{section.235}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsubsection}{How an incremental approach helps}{94}{subsubsection*.236}}
\acronymused{ANN}
\@writefile{lot}{\contentsline {table}{\numberline {5.16}{\ignorespaces Probability distributions generated from rolling a 6 sided die.}}{95}{table.237}}
\newlabel{tab:dieProb}{{5.16}{95}{Probability distributions generated from rolling a 6 sided die}{table.237}{}}
\AC@undonewlabel{acro:PRNG}
\newlabel{acro:PRNG}{{5.5}{95}{How an incremental approach helps}{section*.238}{}}
\acronymused{PRNG}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Visualisation of an imaginary Cost landscape.}}{96}{figure.239}}
\newlabel{fig:localminima}{{5.22}{96}{Visualisation of an imaginary Cost landscape}{figure.239}{}}
\acronymused{ANN}
\acronymused{ANN}
\citation{keller2016analysis}
\citation{schillingmann2009towards,schillingmann2009computational}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bidirectional Grounding of Real Data}{98}{chapter.240}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter6}{{6}{98}{Bidirectional Grounding of Real Data}{chapter.240}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}The Difficulties of Real Data}{98}{section.241}}
\acronymused{MAE}
\acronymused{MRL}
\acronymused{MRL}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Acoustic Packaging}{98}{section.242}}
\acronymused{MRL}
\AC@undonewlabel{acro:AP}
\newlabel{acro:AP}{{6.2}{98}{Acoustic Packaging}{section*.243}{}}
\acronymused{AP}
\AC@undonewlabel{acro:ASR}
\newlabel{acro:ASR}{{6.2}{98}{Acoustic Packaging}{section*.244}{}}
\acronymused{ASR}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces System schematic. Data is captured from sensors by an acoustic packager and fed to the multimodal autoencoder.}}{99}{figure.245}}
\newlabel{fig:schematic}{{6.1}{99}{System schematic. Data is captured from sensors by an acoustic packager and fed to the multimodal autoencoder}{figure.245}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Real Shapes Dataset}{99}{section.246}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Example images from ReShape.}}{99}{figure.247}}
\newlabel{fig:ReShape}{{6.2}{99}{Example images from ReShape}{figure.247}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Dataset Description}{99}{subsection.248}}
\citation{keller2016analysis,keller}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Exemplar images for all objects in the ReShape dataset.}}{100}{figure.249}}
\newlabel{fig:ReShapeAll}{{6.3}{100}{Exemplar images for all objects in the ReShape dataset}{figure.249}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Problem Description}{101}{subsection.250}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Network Description}{101}{subsection.251}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experiments with the ReShape dataset}{101}{section.252}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Data Preprocessing}{101}{subsection.253}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Example cropped images from ReShape.}}{102}{figure.254}}
\newlabel{fig:ReShapeCrop}{{6.4}{102}{Example cropped images from ReShape}{figure.254}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Regions to crop to given different postion words.}}{103}{figure.255}}
\newlabel{fig:CropHeur}{{6.5}{103}{Regions to crop to given different postion words}{figure.255}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Training Procedures}{103}{subsection.256}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Exp 1: Average of the training images of bricks and ducks in different sizes and colours.}}{103}{figure.257}}
\newlabel{fig:AvgBrickDuck}{{6.6}{103}{Exp 1: Average of the training images of bricks and ducks in different sizes and colours}{figure.257}{}}
\acronymused{MSE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Exp 2: Average of the training images for bricks, cups, donuts, ducks, and rectangles, in different sizes and colours.}}{104}{figure.258}}
\newlabel{fig:avgMost}{{6.7}{104}{Exp 2: Average of the training images for bricks, cups, donuts, ducks, and rectangles, in different sizes and colours}{figure.258}{}}
\acronymused{MAE}
\acronymused{MSE}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Experiment 1}{104}{subsection.259}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Experiment 1 data subset.}}{104}{table.260}}
\newlabel{tab:6_exp1_data}{{6.1}{104}{Experiment 1 data subset}{table.260}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Exp 1: Exemplar images of bricks and ducks in different sizes and colours.}}{105}{figure.261}}
\newlabel{fig:ExmBrickDuck}{{6.8}{105}{Exp 1: Exemplar images of bricks and ducks in different sizes and colours}{figure.261}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Exp 1: Total Mean Squared Error for different test conditions and training procedures. (All values are $\times 10^{-3}$.)}}{105}{table.263}}
\newlabel{tab:6_res_exp1}{{6.2}{105}{Exp 1: Total Mean Squared Error for different test conditions and training procedures. (All values are $\times 10^{-3}$.)}{table.263}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{105}{subsubsection*.262}}
\acronymused{MAE}
\acronymused{MSE}
\acronymused{MSE}
\acronymused{MSE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MSE}
\acronymused{MAE}
\@writefile{toc}{\contentsline {paragraph}{Image Generation}{106}{paragraph*.264}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Exp 1: Images generated from full descriptions using the \ac {MAE} trained with the simple procedure.}}{106}{figure.265}}
\acronymused{MAE}
\newlabel{fig:simpleGen_1}{{6.9}{106}{Exp 1: Images generated from full descriptions using the \ac {MAE} trained with the simple procedure}{figure.265}{}}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Exp 1: Images generated from (only) full descriptions using the MAE trained with the exemplar procedure.}}{106}{figure.266}}
\newlabel{fig:exemplarGen_1}{{6.10}{106}{Exp 1: Images generated from (only) full descriptions using the MAE trained with the exemplar procedure}{figure.266}{}}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Exp 1: Images generated using word pairs using the MAE trained with the simple procedure.}}{107}{figure.267}}
\newlabel{fig:2wordsSimpleBrickDuck}{{6.11}{107}{Exp 1: Images generated using word pairs using the MAE trained with the simple procedure}{figure.267}{}}
\acronymused{MAE}
\acronymused{MAE}
\citation{keller2016analysis,keller}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Images generated using word pairs using the MAE trained using image exemplars. (Only words are provided as input.)}}{108}{figure.268}}
\newlabel{fig:2wordsExemplarBrickDuck}{{6.12}{108}{Images generated using word pairs using the MAE trained using image exemplars. (Only words are provided as input.)}{figure.268}{}}
\@writefile{toc}{\contentsline {paragraph}{Multilabel Classification}{108}{paragraph*.269}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Exp 1: Percentage Description Accuracy for different test conditions and training procedures.}}{108}{table.270}}
\newlabel{tab:6_res_exp1_acc}{{6.3}{108}{Exp 1: Percentage Description Accuracy for different test conditions and training procedures}{table.270}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Experiment 2}{109}{subsection.271}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Experiment 2 data subset.}}{109}{table.272}}
\newlabel{tab:6_exp2_data}{{6.4}{109}{Experiment 2 data subset}{table.272}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Exp 2: Exemplar images of bricks, cups, donuts, ducks, and rectangles, in different sizes and colours.}}{109}{figure.273}}
\newlabel{fig:ExmMost}{{6.13}{109}{Exp 2: Exemplar images of bricks, cups, donuts, ducks, and rectangles, in different sizes and colours}{figure.273}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Exp 2: Total Mean Squared Error for different test conditions and training procedures. (All values are $\times 10^{-3}$.)}}{110}{table.275}}
\newlabel{tab:6_res_exp2}{{6.5}{110}{Exp 2: Total Mean Squared Error for different test conditions and training procedures. (All values are $\times 10^{-3}$.)}{table.275}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{110}{subsubsection*.274}}
\@writefile{toc}{\contentsline {subsubsection}{Image Generation}{110}{subsubsection*.276}}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Exp 2: Images generated using word pairs using the \ac {MAE} trained using image exemplars when only words are provided as input.}}{111}{figure.277}}
\acronymused{MAE}
\newlabel{fig:2wordsMostExemplar}{{6.14}{111}{Exp 2: Images generated using word pairs using the \ac {MAE} trained using image exemplars when only words are provided as input}{figure.277}{}}
\@writefile{toc}{\contentsline {paragraph}{Multilabel Classification}{112}{paragraph*.278}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Exp 2: Images generated from full descriptions using the \ac {MAE} trained with the exemplar procedure. Red borders show objects with exemplar images.}}{112}{figure.279}}
\acronymused{MAE}
\newlabel{fig:mostExemplarGen}{{6.15}{112}{Exp 2: Images generated from full descriptions using the \ac {MAE} trained with the exemplar procedure. Red borders show objects with exemplar images}{figure.279}{}}
\acronymused{MSE}
\acronymused{MSE}
\@writefile{toc}{\contentsline {paragraph}{Description Accuracy}{112}{paragraph*.280}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Accuracy}}{112}{table.281}}
\newlabel{tab:6_res_exp2_acc}{{6.6}{112}{Accuracy}{table.281}{}}
\acronymused{MAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}Discussion}{112}{subsection.282}}
\acronymused{MRL}
\acronymused{MRL}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Conclusion}{113}{section.283}}
\acronymused{MRL}
\acronymused{MRL}
\acronymused{MAE}
\acronymused{ANN}
\acronymused{MAE}
\citation{repRev}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{115}{chapter.284}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter7}{{7}{115}{Conclusion}{chapter.284}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary of Important Points}{115}{section.285}}
\acronymused{MRL}
\acronymused{MAE}
\acronymused{MAE}
\acronymused{MRL}
\acronymused{MRL}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Conclusion}{116}{section.286}}
\acronymused{MRL}
\acronymused{MRL}
\citation{rasa}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Future Work}{117}{chapter.287}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Chapter8}{{8}{117}{Future Work}{chapter.287}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}What Comes Next?}{117}{section.288}}
\acronymused{MRL}
\AC@undonewlabel{acro:HRI}
\newlabel{acro:HRI}{{8.1}{117}{What Comes Next?}{section*.289}{}}
\acronymused{HRI}
\acronymused{MAE}
\AC@undonewlabel{acro:FSM}
\newlabel{acro:FSM}{{8.1}{117}{What Comes Next?}{section*.291}{}}
\acronymused{FSM}
\AC@undonewlabel{acro:NLU}
\newlabel{acro:NLU}{{8.1}{117}{What Comes Next?}{section*.292}{}}
\acronymused{NLU}
\acronymused{MAE}
\acronymused{MRL}
\acronymused{NLU}
\acronymused{NLU}
\acronymused{MAE}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces An example of how the \ac {MAE} can be used as part of a robotic system.}}{118}{figure.290}}
\acronymused{MAE}
\newlabel{fig:fsm}{{8.1}{118}{An example of how the \ac {MAE} can be used as part of a robotic system}{figure.290}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces An example of using an MAE trained with MRL to disambiguate which object is being referred to in a scene.}}{118}{figure.293}}
\newlabel{fig:disamb}{{8.2}{118}{An example of using an MAE trained with MRL to disambiguate which object is being referred to in a scene}{figure.293}{}}
\acronymused{MAE}
\@writefile{tdo}{\contentsline {todo}{this has just been shoved here, extracted from chapter 4's conclusion}{119}{section*.294}}
\pgfsyspdfmark {pgfid48}{21427434}{50092587}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix \emph  {A Primer on Artifical Neural Networks}}{120}{appendix.295}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix:app3}{{A}{120}{Appendix \emph {A Primer on Artifical Neural Networks}}{appendix.295}{}}
\newlabel{eqn:BP1}{{A.1}{120}{Appendix \emph {A Primer on Artifical Neural Networks}}{equation.296}{}}
\newlabel{eqn:BP2}{{A.2}{120}{Appendix \emph {A Primer on Artifical Neural Networks}}{equation.297}{}}
\newlabel{eqn:BP3}{{A.3}{120}{Appendix \emph {A Primer on Artifical Neural Networks}}{equation.298}{}}
\newlabel{eqn:BP4}{{A.4}{121}{Appendix \emph {A Primer on Artifical Neural Networks}}{equation.299}{}}
\newlabel{eqn:hada}{{A.5}{121}{}{equation.300}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Appendix \emph  {Sensory Redundancy}}{122}{appendix.301}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix:app4}{{B}{122}{Appendix \emph {Sensory Redundancy}}{appendix.301}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Results from the combined MNIST Handwritten Digits and UCU Arabic Spoken Digits}}{123}{table.302}}
\newlabel{tab:mnist_ucu_master_res}{{B.1}{123}{Results from the combined MNIST Handwritten Digits and UCU Arabic Spoken Digits}{table.302}{}}
\bibdata{bigone}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Appendix \emph  {Magical Vectors and Where to Find Them}}{124}{appendix.303}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix:app5}{{C}{124}{Appendix \emph {Magical Vectors and Where to Find Them}}{appendix.303}{}}
\newlabel{eqn:updates}{{C.1}{124}{Appendix \emph {Magical Vectors and Where to Find Them}}{equation.305}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.1}{\ignorespaces Image and Word multimodal autoencoder. Layers marked i, w, and iw are image, word, and image and word respectively. The number of neurons, ``emb size'', in the Embedding block is varied in some experiments.}}{125}{table.304}}
\newlabel{tab:Arts_MAE_description}{{C.1}{125}{Image and Word multimodal autoencoder. Layers marked i, w, and iw are image, word, and image and word respectively. The number of neurons, ``emb size'', in the Embedding block is varied in some experiments}{table.304}{}}
\@writefile{lot}{\contentsline {table}{\numberline {C.2}{\ignorespaces Number of weight updates per Epoch for each experiment.}}{125}{table.306}}
\newlabel{tab:updatesperEpoch}{{C.2}{125}{Number of weight updates per Epoch for each experiment}{table.306}{}}
\bibcite{smith2008infants}{{1}{}{{}}{{}}}
\bibcite{yurovsky2013statistical}{{2}{}{{}}{{}}}
\bibcite{rosenblatt1958perceptron}{{3}{}{{}}{{}}}
\bibcite{lecun1998mnist}{{4}{}{{}}{{}}}
\bibcite{kingma2014adam}{{5}{}{{}}{{}}}
\bibcite{zeiler2012adadelta}{{6}{}{{}}{{}}}
\bibcite{senior2013empirical}{{7}{}{{}}{{}}}
\bibcite{dumoulin2016guide}{{8}{}{{}}{{}}}
\bibcite{radford2015unsupervised}{{9}{}{{}}{{}}}
\bibcite{zeiler2010deconvolutional}{{10}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{126}{appendix*.307}}
\bibcite{kingma2013auto}{{11}{}{{}}{{}}}
\bibcite{krizhevsky2012imagenet}{{12}{}{{}}{{}}}
\bibcite{vinyals2019alphastar}{{13}{}{{}}{{}}}
\bibcite{ma2004facial}{{14}{}{{}}{{}}}
\bibcite{levi2015emotion}{{15}{}{{}}{{}}}
\bibcite{kussul2017deep}{{16}{}{{}}{{}}}
\bibcite{qi2017pointnet}{{17}{}{{}}{{}}}
\bibcite{simonyan2014very}{{18}{}{{}}{{}}}
\bibcite{springenberg2014striving}{{19}{}{{}}{{}}}
\bibcite{srivastava2014dropout}{{20}{}{{}}{{}}}
\bibcite{he2016deep}{{21}{}{{}}{{}}}
\bibcite{hochreiter1998vanishing}{{22}{}{{}}{{}}}
\bibcite{huang2017densely}{{23}{}{{}}{{}}}
\bibcite{szegedy2015going}{{24}{}{{}}{{}}}
\bibcite{szegedy2016rethinking}{{25}{}{{}}{{}}}
\bibcite{szegedy2017inception}{{26}{}{{}}{{}}}
\bibcite{mirza2014conditional}{{27}{}{{}}{{}}}
\bibcite{odena2017conditional}{{28}{}{{}}{{}}}
\bibcite{vinyals2015show}{{29}{}{{}}{{}}}
\bibcite{lebret2015phrase}{{30}{}{{}}{{}}}
\bibcite{donahue2015long}{{31}{}{{}}{{}}}
\bibcite{jia2015guiding}{{32}{}{{}}{{}}}
\bibcite{rohrbach2014coherent}{{33}{}{{}}{{}}}
\bibcite{rohrbach2013translating}{{34}{}{{}}{{}}}
\bibcite{yao2015describing}{{35}{}{{}}{{}}}
\bibcite{yao2015video}{{36}{}{{}}{{}}}
\bibcite{venugopalan2014translating}{{37}{}{{}}{{}}}
\bibcite{johnson2016densecap}{{38}{}{{}}{{}}}
\bibcite{ordonez2011im2text}{{39}{}{{}}{{}}}
\bibcite{zhu2017unpaired}{{40}{}{{}}{{}}}
\bibcite{lu2013speech}{{41}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{42}{}{{}}{{}}}
\bibcite{keller}{{43}{}{{}}{{}}}
\bibcite{silberer2014learning}{{44}{}{{}}{{}}}
\bibcite{wavenet}{{45}{}{{}}{{}}}
\bibcite{vincent2010stacked}{{46}{}{{}}{{}}}
\bibcite{mikolov2013distributed}{{47}{}{{}}{{}}}
\bibcite{mikolov2013efficient}{{48}{}{{}}{{}}}
\bibcite{mikolov2013linguistic}{{49}{}{{}}{{}}}
\bibcite{feng2010visual}{{50}{}{{}}{{}}}
\bibcite{eslami2018neural}{{51}{}{{}}{{}}}
\bibcite{donahue2019large}{{52}{}{{}}{{}}}
\bibcite{repRev}{{53}{}{{}}{{}}}
\bibcite{pu2016variational}{{54}{}{{}}{{}}}
\bibcite{lin2014microsoft}{{55}{}{{}}{{}}}
\bibcite{bleu}{{56}{}{{}}{{}}}
\bibcite{van2017neural}{{57}{}{{}}{{}}}
\bibcite{dunbar2017zero}{{58}{}{{}}{{}}}
\bibcite{russakovsky2015imagenet}{{59}{}{{}}{{}}}
\bibcite{chiu2018state}{{60}{}{{}}{{}}}
\bibcite{nguyen2015deep}{{61}{}{{}}{{}}}
\bibcite{szegedy2013intriguing}{{62}{}{{}}{{}}}
\bibcite{moosavi2016deepfool}{{63}{}{{}}{{}}}
\bibcite{barsalou2008grounded}{{64}{}{{}}{{}}}
\bibcite{ImNet}{{65}{}{{}}{{}}}
\bibcite{ha2018world}{{66}{}{{}}{{}}}
\bibcite{ha2018recurrent}{{67}{}{{}}{{}}}
\bibcite{vinyals2016matching}{{68}{}{{}}{{}}}
\bibcite{searle1980minds}{{69}{}{{}}{{}}}
\bibcite{turing2009computing}{{70}{}{{}}{{}}}
\bibcite{cangelosi2000robotic}{{71}{}{{}}{{}}}
\bibcite{harnad1990symbol}{{72}{}{{}}{{}}}
\bibcite{steels2008symbol}{{73}{}{{}}{{}}}
\bibcite{slater1999intermodal}{{74}{}{{}}{{}}}
\bibcite{masland2012neuronal}{{75}{}{{}}{{}}}
\bibcite{fantz1963pattern}{{76}{}{{}}{{}}}
\bibcite{johnson2015two}{{77}{}{{}}{{}}}
\bibcite{webb2015mother}{{78}{}{{}}{{}}}
\bibcite{reid2017human}{{79}{}{{}}{{}}}
\bibcite{pfeifer2006body}{{80}{}{{}}{{}}}
\bibcite{smith2005development}{{81}{}{{}}{{}}}
\bibcite{nicolelis1995sensorimotor}{{82}{}{{}}{{}}}
\bibcite{mordvintsev2015inceptionism}{{83}{}{{}}{{}}}
\bibcite{glenberg2015few}{{84}{}{{}}{{}}}
\bibcite{Goldin-MeadowSusan2015Fata}{{85}{}{{}}{{}}}
\bibcite{de2014making}{{86}{}{{}}{{}}}
\bibcite{rucinski2012robotic}{{87}{}{{}}{{}}}
\bibcite{bahrick2000intersensory}{{88}{}{{}}{{}}}
\bibcite{walker2010preverbal}{{89}{}{{}}{{}}}
\bibcite{fischer2011multi}{{90}{}{{}}{{}}}
\bibcite{scott20122}{{91}{}{{}}{{}}}
\bibcite{fernald1987acoustic}{{92}{}{{}}{{}}}
\bibcite{lohan2012contingency}{{93}{}{{}}{{}}}
\bibcite{lohan2012tutor}{{94}{}{{}}{{}}}
\bibcite{pezzulo2013computational}{{95}{}{{}}{{}}}
\bibcite{bambach2017egocentric}{{96}{}{{}}{{}}}
\bibcite{olveczky2003segregation}{{97}{}{{}}{{}}}
\bibcite{oxenham2018we}{{98}{}{{}}{{}}}
\bibcite{lemonlearning}{{99}{}{{}}{{}}}
\bibcite{yu2017learning}{{100}{}{{}}{{}}}
\bibcite{coradeschi2000anchoring}{{101}{}{{}}{{}}}
\bibcite{coradeschi2003introduction}{{102}{}{{}}{{}}}
\bibcite{coradeschi2013short}{{103}{}{{}}{{}}}
\bibcite{vogt2002physical}{{104}{}{{}}{{}}}
\bibcite{cangelosi2006grounding}{{105}{}{{}}{{}}}
\bibcite{nakamura2009grounding}{{106}{}{{}}{{}}}
\bibcite{nakamura2011grounding}{{107}{}{{}}{{}}}
\bibcite{cangelosi2001adaptive}{{108}{}{{}}{{}}}
\bibcite{cangelosi2002symbol}{{109}{}{{}}{{}}}
\bibcite{cangelosi1998emergence}{{110}{}{{}}{{}}}
\bibcite{horst2019object}{{111}{}{{}}{{}}}
\bibcite{cangelosi2018speech}{{112}{}{{}}{{}}}
\bibcite{cangelosi2008italk}{{113}{}{{}}{{}}}
\bibcite{broz2014italk}{{114}{}{{}}{{}}}
\bibcite{ngiam2011multimodal}{{115}{}{{}}{{}}}
\bibcite{mcgurk1976hearing}{{116}{}{{}}{{}}}
\bibcite{samuel1997lexical}{{117}{}{{}}{{}}}
\bibcite{lee2008sparse}{{118}{}{{}}{{}}}
\bibcite{ma2009lip}{{119}{}{{}}{{}}}
\bibcite{hammami2009tree}{{120}{}{{}}{{}}}
\bibcite{hammami2010improved}{{121}{}{{}}{{}}}
\bibcite{keller2016analysis}{{122}{}{{}}{{}}}
\bibcite{schillingmann2009towards}{{123}{}{{}}{{}}}
\bibcite{schillingmann2009computational}{{124}{}{{}}{{}}}
\bibcite{rasa}{{125}{}{{}}{{}}}
\bibstyle{latexeu}
\providecommand\NAT@force@numbers{}\NAT@force@numbers

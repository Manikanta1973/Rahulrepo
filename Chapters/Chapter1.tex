% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 1. \emph{Introduction}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

\section{Multimodal Representation Learning}
What is Multimodal Representation Learning (MRL) and why is it useful? These are the questions that I will address in this thesis.
Briefly, MRL is the act of learning an abstract representation of sensory data from multiple sensors. As sensors respond directly to the world in which they are situated, MRL is closely related to learning a world representation. Therefore, an accurate representation of sensory data contains in it, a representation of the world.

MRL is therefore useful any time we have more than one sensor and if we can abstract sensory data to learn more about the world, or how the different modalities relate to one and other. Perhaps the best example of this is Language Learning - when a baby learns its first words, it is doing MRL, learning the association of sounds to objects and actions and vice-versa.

Beyond the highly complex and abstract task of learning a language, MRL can also be used for sensor fusion. Providing auxilory information using extra sensors can help to improve classification accuracy (which I will demonstrate later) and can also be used to regenerate missing data when only one modality is available.  A good example of this is when humans read lips in noisy environments, using the shape of the mouth to help recognise which words are being uttered even if they are not heard.


\section{Motivation}
One of the major challenges in robotics and machine learning is that of knowing how to develop systems capable of life long learning. If robots are to become common place in society, they must be capable of continuing to learn throughout their lives. They will not be very useful if they cannot learn new things that they weren't programmed with at the factory.

Multimodal representation learning addresses this by providing an unsupervised method for extracting useful and actionable information about the world. Let us return to the language learning scenario, except this time, instead of a baby, lets imagine a robot, fresh out of the box. Whilst we might expect that a robot would have some proficiency with language built into it, we cannot expect it to know the meaning of every single word, especially given the flexible and evolving way that humans use language. Our robot therefore needs a way to learn the meaning of new words. 

Similarly, we cannot expect the robot to be able to recognise every single object in its environment. On my desk I have a Stirling engine, I would wager that most people who read this don't know what a Stirling engine is. However, if I showed you a picture of a Stirling engine (like the one in \autoref{fig:stirling}) you would be able to recognise what a Stirling engine is in future. Ideally, robots need to be able to do this.

\begin{figure}
\centering
	\includegraphics[width=0.3\textwidth]{Figs/introduction/stirling.jpg}
	\caption{The Stirling engine from my desk.}
	\label{fig:stirling}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
[node distance=1.5cm,
    every node/.style={fill=white, font=\sffamily}, align=center]
  % Specification of nodes (position, etc.)
  \node (object)   [activityStarts]              					{Object};
  \node (name)     [activityStarts, right of=object, xshift=4cm] {Name};
  
  \node (A)        [activityStarts, below of=object, yshift=-0.7cm]          {\includegraphics[width=0.96cm,height=1.5cm]{Figs/introduction/stirling.png}};
 \node (stirling) [activityStarts, right of=A, xshift=4cm]  {``Stirling Engine"};       
 
  % Specification of lines between nodes specified above
  % with aditional nodes for description 
  \draw[<->]     (object) -- (name);
  \draw[<->]     (A) -- (stirling);
 
\end{tikzpicture}
\caption{The bi-directional nature of language learning.}
\label{fig:bi_ll}
\end{figure}

The Stirling engine example shows how language learning is a bi-directional process. Not only do you need to learn the name of the object \textit{Stirling Engine} but also that the words ``Stirling Engine'' mean the object \textit{Stirling Engine} as shown in \autoref{fig:bi_ll}. 

Multimodal representation learning allows us to jointly model both the object \textit{Stirling Engine} and the words ``Stirling Engine'', relying on only one assumption: \textbf{Sensory percepts which occur together are related}.

Whilst this may not be true for all sensory percepts it is true enough that co-occurrence of percepts allows us to infer useful information about the relationship between different modalities. For example, smith et al. \cite{smith2008infants} show how infants learn object names based on ``cross-situational statistics'' i.e. how often a word and an object occur together. \autoref{fig:cross_sitch} illustrates this idea.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{Figs/introduction/shapes.png}
\caption{Associations among words and objects across multiple ambiguous scenes allow learners to find the proper mapping of words:
\textsc{Circle}, \textsc{Square}, \textsc{Green} and \textsc{Blue} to the shapes and colours: \textit{Circle},  \textit{Square},  \textit{Green} and \textit{Blue}}
\label{fig:cross_sitch}
\end{figure}

Looking at \autoref{fig:cross_sitch} we can see that from a single example, it is not possible to disambiguate which word refers to which part of the image. However, given two examples, it is very easy for an adult human to disambiguate what each word means. As we are given more examples, we can be more and more certain about what we believe the meanings of the words are. 

In the real world, visual perception is much noiser than the images presented in \autoref{fig:cross_sitch} as are the utterances of words. However, by providing more examples, we can overcome the challenges presented by this as shown in \cite{yurovsky2013statistical}. In this thesis I will show how MRL can be applied to this problem and solve it, learning the meanings of different word types, such as object names, colours, sizes and positions as well as the inverse problem, generating these labellings for images of objects. \autoref{fig:mrl_teaser} shows a few examples of MRL solving this problem.

\begin{figure}
\centering

\missingfigure[figwidth=3.5cm]{Placeholder}
\caption{Images generated from labellings of natural images (left) and labellings generated from natural images (right)}
\label{fig:mrl_teaser}
\end{figure}



%----------------------------------------------------------------------------------------







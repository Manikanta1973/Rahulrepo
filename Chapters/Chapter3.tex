% Chapter Template

\chapter{Background} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{Background}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Introduction}\label{Lit:Intro}
In this section I will layout the key ideas and technologies which have inspired and enabled the research presented later in this thesis.

Due to the broad scope of the research carried out, the background section is loosely presented so as to compare and contrast the state-of-the-art artificial neural network technologies with their biological analogues (where possible). As such, I survey a large body of machine learning, psychology and biology literature.

In doing this, I aim to demonstrate how ANNs have very good performance in specific areas, whilst lagging far behind in many others. By bringing these divergent fields together, perhaps it is possible to find ways to improve ANNs whilst also further developing our understanding of what makes humans tick. 


\section{What are Artifical Neural Networks good at?}
Artificial Neural Networks have been around for a long time but perhaps the best example of early neural networks is the MultiLayer Perceptron (MLP) \cite{rosenblatt1958perceptron}. Modern ANNs can trace their lineage back to the MLP; however the technology has advanced a lot since 1958. ANNs represent the state-of-the-art on many Artificial Intelligence benchmarks.

ANNs are currently enjoying a new renaissance due to wide spread availablity of Graphics Processing Units (GPUs), big data and software libraries like Tensorflow and CUDA.

In 2012, Krizhevsky et al. \cite{krizhevsky2012imagenet} kickstarted this new wave of interest in ANNs by getting the top performance on the ILSVRC-2012 ImageNet challenge. The unprecendented performance was achieved by utilising a very large amount of training data (1.2 million images), which was made possible by utilising the parallel computing power of GPUs. In creating AlexNet, Krizhevsky et al. demonstrated the potential of ANNs to solve real world problems, something which had been promised since ANN research began.

The field of Deep Learning (DL) has advanced greatly in recent years, with ANNs being used to solve many different types of problems. DL is being used for many differnt tasks which can be broken down into several categories: Classification, Natural Language Processing (NLP), Data Generation, Reinforcement Learning and Prediction Tasks.


%\begin{outline}
% \1 Classification:
%   \2 Image Recognition \cite{krizhevsky2012imagenet, iandola2016squeezenet, he2016deep, zoph2018learning}
%   \2 Speech Recognition \cite{amodei2016deep, graves2013speech}
%   \2 Facial Recognition
%   
% \1 Natural Language Processing:
%    \2 Language Representation Learning \cite{mikolov2013distributed, mikolov2013efficient, mikolov2013linguistic}
% 	\2 Intent Classification \cite{chen2016zero}
% 	\2 Machine Translation \cite{cho2014learning}
%
% \1 Generation:
%   \2 Image
%   \2 Voice
%   
% \1 Multimodal Learning:
% 	\2 Image Captioning \cite{vinyals2015show}
% 	\2 Video Captioning
% 	\2 Multimodal Representation Learning \cite{ngiam2011multimodal}
%   
% \1 Reinforcement Learning:
%   \2 Robotic Control \cite{lillicrap2015continuous}
%   \2 Video Games \cite{vinyals2019alphastar, mnih2013playing}
%   \2 World Modelling \cite{azar2019world}
%      
% \1 Forecasting:
%   \2 Weather \cite{mahesh2018probabilistic}
%   \2 Electrical Loads \cite{bouktif2018optimal}
%   \2 Financial Markets \cite{fischer2018deep}
%\end{outline}

Of these categories of problems, MRL (as it is presented in this thesis) draws mostly from Classification, NLP and Data Generation.

\subsection{Classification}
MRL can be considered to draw from different classification tasks, depending on which modalities it is used with. In this thesis I apply MRL to image, speech and textual data, as such I will focus on image, speech and text recognition techniques and will not focus on more nitch problems such as Facial Recognition \cite{ma2004facial}, Emotion Recognition \cite{levi2015emotion} or more general data classification problems \cite{kussul2017deep,qi2017pointnet}.


With Krizhevsky's work in\cite{krizhevsky2012imagenet}, a style of convolutional neural network (ConvNet) architecture, consisting of convolution max-pooling and dropout layers followed by fully connected layers and a softmax output layer was created. This has become a standard type architecture for image classification tasks with many similar networks appearing such as VGG 16 and 19 \cite{simonyan2014very}.

Convolution layers are used for feature extraction and fully connected layers are used for classification with downsampling happening gradually throughout the network to reduce the dimensionality of the data. In AlexNet and VGG downsampling is done using the max-pooling layers, however more recent architectures have made use of strided convolutions for this \cite{springenberg2014striving}.

The use of dropout as a regulariser has also become common place as it has been shown to outperform the most common regularisers, L1 and L2 \cite{srivastava2014dropout}.

An important advancement over the AlexNet style of architecture (beyond simply altering network hyper parameters as with VGG 16 and 19 \cite{simonyan2014very} was the addition of residual connections. Introduced in \cite{he2016deep}, residual connections allow data to flow through alternate branches of a network, skipping over some layers to rejoin the main flow at a later point in the network. This has two key effects, 1) it allows the training of much deeper networks by providing a shortcut through the network for error gradients to be backpropogated, helping to ellivate the vanishing gradient problem \cite{hochreiter1998vanishing} and 2) it allows the network to consider lower level features along side more abstracted ones for decision making.
An example of taking residual connections to their limit is seen in \cite{huang2017densely} where every preceeding layer's output is an input to the layers which follow it. This makes the neural network very wide but allows for good performance even with a small number of layers, offering a reduction in computational complexity.

Another advancement over the AlexNet style architectures was the introduction of multiscale convolutions where data is passed through multiple, parallel convolutions layers each with a different kernel size before concatenating their activations \cite{szegedy2015going}. Making use of different kernel sizes creates filters which are sensitive to different scales of features. Thus, for example, if an eye in an image is not picked up at one scale as it is too large or too small, it may be picked up by a parallel convolution with a different sized kernel.

Szegedy et.al have developed their Inception Architecture from \cite{szegedy2015going} iteratively in \cite{szegedy2016rethinking, szegedy2017inception} in each, small advancements in state-of-the-art object recognition are achieved.


\subsection{Generation}
Whilst classification tasks are concerned with grouping input data into classes, generative tasks aim to create examples of particular classes, as with class conditioned Generative Adversarial Networks (GANs) \cite{mirza2014conditional, odena2017conditional} or to generate a translation from one modaility to another, for example image and video caption generation \cite{vinyals2015show, lebret2015phrase, donahue2015long, jia2015guiding, rohrbach2014coherent, rohrbach2013translating, yao2015describing, yao2015video, venugopalan2014translating, johnson2016densecap, ordonez2011im2text} and image style transfer \cite{zhu2017unpaired}.

We can also consider autoencoders(AE) a type of generative network due to the way in which they are trained, however, it is more useful to consider them from the perspective of representation learning. For example in \cite{lu2013speech} AEs are used to generate denoised speech.

I will spend much time on the GAN based methods as they are not utilised in the research presented in this thesis. However, GANs are a very active area of research so here is a brief overview. 

In \cite{mirza2014conditional} the Generator network is fed noise and a class label is used to generate images of examples of that class. The Discriminator is fed the desired class and either a real or generated image and tasked with distinguishing whether the image is real or not. By inverting and backpropogating the error gradients from the Discriminator through the Generator, the Generator learns to fool the Discriminator by generating realistic images for each class in the dataset.

Odena et al. \cite{odena2017conditional} expand on the work in \cite{mirza2014conditional}. In stead of feeding the desired class to the Descriminator, they train an auxillory classifier network which classifies which class the generated and real images belong to.

GANs can also be used for style transfer \cite{zhu2017unpaired}. Style transfer is similar to translating from one modality to another, thus \cite{zhu2017unpaired} demonstrates the flexibility and power of GANs which could be applied to a much wider variety of problems than those I have highlighted here. Zhu et al. use the Descriminator of a GAN to determine whether an image is from a given domain (e.g. Monet paintings) or was translated by the Generator from another domain (e.g. photos). In doing so the Generator learns to produce iamges in the chosen domain from images in a source domain (e.g. photo --> Monet painting).

Translating from one modality to another can also be done by other types of neural networks, not just those trained in an adversarial manner as with GANs.
The field of image and video captioning highlights this. The typical method for image caption generation is to first train a ConvNet on an image classification task and a language model (e.g. an Long Short-Term Memory Reccurent Neural Network (LSTM) \cite{hochreiter1997long}) on a word prediction task \cite{vinyals2015show, venugopalan2014translating, johnson2016densecap}. This initalises the two subnetwroks to have useful weights for the task. In \cite{johnson2016densecap} Johnson et al. make use of the VGG net from\cite{simonyan2014very}. This off the shelf reuse of networks can be very useful and is explored in detail in \cite{keller}.
The dense layers of the ConvNet are removed and the internal image representation learnt by the ConvNet is fed to the language model which is then trained to predict image captions from this representation.

Video captioning is a natural extension of the iamge captioning domain and follows a similar procedure for generating video captions. First a representation of the visual contents of the video is generated and then a language model translates this representation into a caption. In order to generate a representation of the visual information  present in the videos, one can either make use of LSTMs to combine the representations of each frame generated by a ConvNet \cite{donahue2015long}, make use of 3D ConvNets, convolving along the time axis as well as the two spatial axes of the image frames \cite{yao2015describing, yao2015video} or make use of precomputed features such as Motion Boundary Histograms or Optical Flow \cite{rohrbach2014coherent, rohrbach2013translating}.

In all of these methods we see an important commonality, Representation Generation. In order to generate one modality from another, first a representation of the salient information from the source modality must be produced.

\subsection{Representation Learning}

Whilst many tasks involving ANNs focus on an end result such as a \cite{krizhevsky2012imagenet}, others actively focus on learning data representations \cite{radford2015unsupervised, silberer2014learning, chorowski2019unsupervised, vincent2010stacked, mikolov2013distributed, mikolov2013efficient, mikolov2013linguistic, feng2010visual, eslami2018neural, donahue2019large}.

As I discussed previously, image and video captioning are relient on learning abstract representations of the input images. In fact, representation learning is present in all neural network tasks, whether they are directly viewed this way or not. In \cite{vinyals2015show, venugopalan2014translating, johnson2016densecap}, not only is pretraining to produce a useful representation of image content leveraged, but we also see that learning this representation is inherent in learning image classification, highlighted by Johnson et al. \cite{johnson2016densecap} reusing weights trained in \cite{simonyan2014very}. 

In \cite{mikolov2013distributed, mikolov2013efficient, mikolov2013linguistic} Mikolov et al. show how learning a continuous representation of natural language can be used to solve various natural language processing tasks. By converting from a one-hot representation to a continuous representation, learnt through a word prediction task, many other problems can be solved. 
Whilst a one-hot encoding contains no information about the meaning of the word it represents, the continuous representation which arises from learning skip-gram or n-gram models which take this one-hot encoding as input do contain some of the meaning of the words they represent.

Whilst the skip-gram and n-gram representations remain ungrounded and do not contain the true meaning of the words they encode, some properties of the words can be found from this representation. For example, pronouns all end up with similar representations as do the capital cities of countires. Further to this, the gradients between the representations of countries and their capitals are also similar, this highlights that some of the meaning of words can be found just from statistical regularities in bodies of text.

\todo[inline]{you are up to here} 


\section{What are Artificial Neural Networks Bad at?}
Whilst ANNs have been applied to many tasks and acheived super-human ability at them \cite{vinyals2019alphastar}, they do not have general intelligence like humans.

\begin{displayquote}
``... people see how well [an algorithm] performs at one task and they think it can do all the things around that, and it can’t... When we see a person performing a task very well, we understand the competence [involved]. And I think they apply the same model to machine learning'' - Rodney Brooks.
\end{displayquote}

\subsection{Artificial Neural Networks are Easily Fooled}
Whilst \cite{krizhevsky2012imagenet, simonyan2014very, szegedy2015going, szegedy2016rethinking, szegedy2017inception, he2016deep, huang2017densely} all show amazing performance on a very difficult image classification problem. These same networks are easily by images, which to humans look like random noise \cite{nguyen2015deep}.

\subsection{Data Ineffciency}
look at oth ML methods which require less data but are generally less capable/require more engineering.


\section{How to get off the Symbol Grounding Merry-Go-Round} 
\subsection{What is symbol grounding?}
\subsection{How do humans do it?}
Barsalou

\subsection{How do machines do it?}

\section{Why brains are better}
\subsection{Embodiment}
\subsubsection{Sensory Redundancy}
\subsubsection{Biological Filters}
Retina, shape of ear etc.
\subsection{Development}
\subsubsection{Biological Filters, again}
Superior Colliculous guides learning in the visual cortex by controlling attention.
\subsection{Pulling yourself up by the bootstraps}

\subsection{Machine Equivelancy}
\subsubsection{How do we simulate Embodiment for ANNs?}
\subsubsection{How do we simulate Development for ANNs?}

\section{Where do we go from here?}
\subsection{Robot bodies}
\subsection{multimodality}
\subsection{transfer learning}



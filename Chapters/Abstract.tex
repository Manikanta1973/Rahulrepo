In robotics, a major challenge which many have attempted to address is that of Symbol Grounding. Symbol Grounding is the act of taking sensory percepts and connecting them to abstract symbols. For example when babies learn to speak they are performing symbol grounding by connecting words to the objects, actions and concepts they represent.

I approach Symbol Grounding through the use of Mutlimodal Representation Learning. This method learns an abstract, multimodal representation of different data modalities. I explore how this representation can be used to learn grounded meanings of words and visual attributes in an unsupervised manner.

Unlike supervised methods, my approach does not require data to be labelled, instead, I make use of acoustic packaging assuming that time aligned percepts in different modalities are related. This means that a robot using Multimodal Representation Learning could learn language and vision through natural interactions with humans.


Multimodal Representation Learning allows Multimodal Autoencoders to generate images of unseen objects as well as to ground the meaning of individual words and visual attributes which are never explicitly taught. 

I believe this method will facilitate Human Robot Interaction both for scientific experiments and for future robotic system which will be deployed in the wild.


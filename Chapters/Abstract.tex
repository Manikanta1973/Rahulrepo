I present Mutlimodal Representation Learning, a method for learning a multimodal representation of different data modalities and explore how it can be used to learn grounded meanings of words and visual attributes in an unsupervised manner.

Multimodal Representation Learning allows Multimodal Autoencoders to generate images of unseen objects as well as to ground the meaning of individual words and visual attributes which are never explicitly taught to it. 

I believe this method will facilitate Human Robot Interaction both for scientific experiments and for future robotic system which will be deployed in the wild.


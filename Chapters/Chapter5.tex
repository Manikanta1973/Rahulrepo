% Chapter Template

\chapter{Magical Vectors and Where to Find Them} % Main chapter title

\label{Chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Magical Vectors and Where to Find Them}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\section{ANN Latent space: the Final Frontier}
One of the most interesting properties of neural networks is the latent embeddings they learn which are abstract representations of the input data. Normally when working with neural networks people are very interested in the output of the neural network, for example, does the network correctly classify an image and how confident about it is it - this is the whole reson for using cross-entropy losses for training. However, in my case, I am more interested in how the neural network represents what it has learnt.

In \cite{mikolov2013distributed} and \cite{mikolov2013efficient}, it is demonstrated that skipgram models trained on large corpuses develop word embeddings whose position in the latent embedding space tell us something about the relational meanings of the words the embedding sbelong to. For example questions such as \textit{``King is to man as woman is to...''} can be solved with some vector arithmetic on these embeddings as shown in \autoref{eqn:mikolov}.
\begin{equation}
V(king) - V(man) + V(woman) \approx V(Queen)
\label{eqn:mikolov}
\end{equation} 

The embeddings learnt by skipgram models show clusterings of different word types, such as nouns, verbs and adverbs whilst things like capital cities and country names form their own subclusters within the larger cluster of all nouns. Whilst the model does not know the meaning of any of the words it has learnt to embed, clearly it has divided the embedding space in a useful way and if it can perform symbol grounding on some of the words, this will provide a basis to start producing models which do know the meaning of the words they are embedding.

To that end, I will now demonstrate how a MAE can be used to learn multimodal representations of words and the image attributes they refer to in order to solve the symbol grounding problem in an unsupervised manner.


\section{Artificial Shapes Dataset}
\subsection{Dataset Description}
The Artificial Shapes dataset (ArtS) contains images and short descriptions of the images. The dataset is artificially generated using a Python script, each image is 64x64 pixels and is described by the shape, colour, size and position of the object in the image. Additionally, the RGB value of the colour and the coordinates of the centre of the object are available, though these are not used in the following experiments. 

\begin{table}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Attribute} & \textbf{Description} \\ \hline \hline
\textbf{Shapes} & \textbf{Corners} \\ \hline
Circle (Circ) & 0\\ \hline 
Rectangle (Rect) & 4\\ \hline
Triangle (Tri)& 3\\ \hline


\textbf{Colours} & \textbf{RGB Values}	\\ \hline	
Red (r)& (75,0,0) - (255, 10, 10)\\ \hline
Orange (o) & (225,75,0) - (255, 155, 50)\\ \hline
Yellow (y) & (230,200,0) - (255, 255, 95)\\ \hline
Green  (g)& (0,75,0) - (10, 255, 10)\\ \hline
Blue   (b)& (0,0,75) - (10, 10, 255)\\ \hline
Indigo (i)& (90,0,190) - (150, 50, 255)\\ \hline
Violet (v)& (200,0,190) - (255, 50, 255)\\ \hline 

\textbf{Sizes} & 	\textbf{Length/Radius (pixels)} \\ \hline			  
Big    (B)& 32 - 35  \\ \hline
Medium (M)& 22 - 25 \\ \hline
Small  (S)& 12 - 15 \\ \hline 

\textbf{Positions} & \textbf{Object Centre Coordinate}	\\ \hline					  
Top Left (TL)& (22,42)\\ \hline	
Top Centre (TC)& (32,42)\\ \hline
Top Right (TR)& (42,42)\\ \hline
Centre Left (CL)&(22,32)\\ \hline
Centre Centre (CC) & (32,32)\\ \hline
Centre Right (CR)&(42,32)\\ \hline
Bottom Left (BL)& (22,22)\\ \hline
Bottom Centre (BC)& (22,42)\\ \hline
Bottom Right (BR)& (42,22)\\ \hline				
\end{tabular}
\caption{Artificial Shapes dataset description.}
\label{tab:Arts_desc} 
\end{table}

The dataset contains 3 different shapes, \textit{rectangle}, \textit{triangle} and \textbf{circle}. Each of these shapes comes in 7 colours, 3 sizes and 9 positions as described in \autoref{tab:Arts_desc}. Each colour covers a range of values, the RGB value for each object in the ArtS dataset is randomly selected from the ranges described in the ``Description'' column of \autoref{tab:Arts_desc}. Whilst the centre of each object is fixed based on its position, the exact size is not fixed based on its size category. The size categories \textit{Big}, \textit{Medium} and \textit{Small} do not describe fixed radiuses and lengths, there is some variation in the values of these for each category.


\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{Figs/shapes/shapes.png}
\caption{Examples of the different, shapes, colours, sizes and positions of objects in the ArtS dataset. The top left object in the left most panel would be described as ``big indigo circle, top left''.}
\label{fig:shapes}
\end{figure}

\autoref{fig:shapes} shows some example images from the ArtS dataset for each of the 3 shapes, 7 colours, 3 sizes and 9 positions.

\subsection{Problem Description}
Given the ArtS dataset, I will demonstrate that it is possible to learn a joint, multimodal representation of the images and their descriptions. Once this representation is learnt, I will demonstrate that we can correctly label novel images with their descriptions and also that we can generate correct images from descriptions.

In the previous chapter, we saw a similar problem which was broken into two subproblems, classification and bidirectional symbol grounding. With this dataset we are not interested in performing a classification as the classification is encapsulated in the generation of correct descriptions for novel images. I will therefore be demonstrating how bidirectional symbol grounding circumvents the need for classification by providing a high level understanding of the meaning of an instance of a modality rather than just a predefined class for that instance. E.g. the neural network is not simply assigning images of rectangles to the ``rectangle'' class, but instead learn what the meaning of the word ``rectangle'' is. 

Further to this, I will demonstrate that once trained, the neural network can infer the meaning of unseen descriptions to correctly generate images and vice-versa. If we ommit a subcategory of object from the training data, for example blue circles, the neural network is still able to generate images of blue circles as it has grounded the meaning of both ``Blue'' and ``Circle''. It is also able to correctly label images of blue circles for the same reason, despite never having been taught what a blue circle is.


\subsection{Network Description}
The overall structure of the neural netwrok used for the experiements in this chapter is very similar to the one used in \autoref{Chapter4}, with two inputs for each of the different modalities, this time, images and words. However, it only has two outputs, one for images and one for words, unlike in the previous chapter where we also had a third output which acted as a classification layer. \autoref{tab:Arts_MAE_description} gives the specific details on each of the layers of the network. Additionally, batchnormalisation is used between beach convolution layer.

\begin{table}
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			Block & Layer & Type & Neurons & Kernel & Strides & Activation \\ \hline
			\multirow{3}{*}{Image} & 1i	&	2D Conv & 64 & (3,3) & (1,1) & Relu \\ \cline{2-7}
			& 2i	&	2D Conv & 64 & (3,3) & (2,2) & Relu \\ \cline{2-7}
			& 3i	&	2D Conv & 32 & (3,3) & (2,2) & Relu \\ \cline{2-7}
\multirow{3}{*}{Encoder} & 4i	&	2D Conv & 32 & (3,3) & (2,2) & Relu \\ \cline{2-7}
			& 5i	&	2D Conv & 16 & (3,3) & (2,2) & Relu \\ \cline{2-7}
			& 6i	&	2D Conv & 16 & (3,3) & (2,2) & Relu \\ \cline{2-7}
			& 7i	&	Dropout p=0.25 &	 & 	     &       &  \\ \hline

			\multirow{3}{*}{Word} & 1w	& Dense & 128 & & &TanH \\ \cline{2-7}
			& 2w	& Dense & 256 & & &TanH \\ \cline{2-7}
			& 3w 	&	Dropout p=0.5 &	 & 	     &       & \\ \cline{2-7}
\multirow{4}{*}{Encoder}& 4w  &	Reshape (16,16,1) & & & & \\ \cline{2-7}
			& 5w	&	2D Conv & 16 & (3,3) & (2,2) & Relu \\ \cline{2-7}
			& 6w	&	2D Conv & 16 & (3,3) & (2,2) & Relu \\ \cline{2-7}
			& 7w	&	2D Conv & 16 & (3,3) & (2,2) & Relu \\ \hline

			Merge & 8iw	& Merge & & & & \\ \hline
Embedding & 9iw	&	2D Conv  & emb size & (3,3) & (1,1) & Relu \\ \hline
			
			
			\multirow{4}{*}{Image} & 10i 	&	Dropout p=0.25 &	 & 	     &       & \\ \cline{2-7}
			& 11i	&	2D Trans Conv & 16 & (3,3) & (2,2)  & TanH \\ \cline{2-7}
			& 12i	&	2D Trans Conv & 16 & (3,3) & (2,2)  & TanH \\ \cline{2-7}
			& 13i	&	2D Trans Conv & 16 & (3,3) & (2,2)  & TanH \\ \cline{2-7}
\multirow{4}{*}{Decoder}& 14i	&	2D Trans Conv & 32 & (3,3) & (1,1)  & TanH \\ \cline{2-7}
			& 15i	&	2D Trans Conv & 64 & (3,3) & (1,1)  & TanH \\ \cline{2-7}
			& 16i	&	2D Trans Conv & 64 & (3,3) & (1,1)  & TanH \\ \cline{2-7}
			& 17i	&	2D Trans Conv & 3 & (3,3) & (1,1) & Sigmoid\\ \hline 

			\multirow{4}{*}{Word} & 10w 	&	Dropout p=0.25 &	 & 	     &       & \\ \cline{2-7}
			& 12w	&	2D Trans Conv & 16 & (3,3) & (2,2)  & Relu \\ \cline{2-7}
			& 13w	&	2D Trans Conv & 16 & (3,3) & (2,2)  & Relu \\ \cline{2-7}
			& 14w	&	2D Trans Conv & 16 & (3,3) & (2,2)  & Relu \\ \cline{2-7}
\multirow{5}{*}{Decoder}& 15w	& Reshape (256) & & & & \\ \cline{2-7}
			& 16w	& Dropout p=0.5 &	 & 	     &       & \\ \cline{2-7}
			& 17w	& Dense & 256 & & &TanH \\ \cline{2-7}
			& 18w	& Dense & 128 & & &TanH \\ \cline{2-7}
			& 19w	& Dense & 22 & & & Sigmoid \\ \hline
			
			
		\end{tabular}
		\caption{Image and Word multimodal autoencoder. Layers marked i, w, and iw are image, word, and image and word respectively. The number of neurons, ``emb size'', in the Embedding block is varied in some experiments.}
		\label{tab:Arts_MAE_description}

	\end{table}

\section{Experiments with the ArtS Dataset}
For each of the following experiments I will test the MAE in three ways, similarly to the experimental method of \autoref{Chapter4}. Following the protocol of testing in a Bimodal, Image Only and Words Only manner allows me to explore how each of the modalities and their combination, affects the quality of the embedding learnt by the neural network.


Due to the complexity of ArtS and the variety of factors which can be explored with it, I will perform 5 sets of experiments, each using different subsets of the dataset. All experiments will make use of all 3 shapes, but different numbers of colours, sizes and positions will be used.

Each experiment is four fold cross validated using randomly selected training data and weight initialisations.

\subsection{Experiment 1}
The first experiment will utilise 3 colours, 3 positions and 1 size as shown in \autoref{tab:exp1_data}. The aim of this experiment is to find a resonable size for the embedding layer. I wish to find a number of filters which minimises the reconstruction loss for both the images and descriptions under all testing conditions. Futher to this I also wish to find an embedding size which allows for the accurate grounding of all words in the dataset. For example, if the MAE is given only the word ``Blue'' I want it to generate a blue image, or given the word ``Triangle'' an image of a triangle. 

For each object, colour and position combination, I will generate 50 training samples giving a total of 1350 training samples. The validation and testing data consist of 100 samples per object, colour and position combination giving 2700 validation and 2700 testing samples.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Attribute} & \textbf{Description} \\ \hline \hline
\textbf{Shapes} & \textbf{Corners} \\ \hline
Rectangle & 4\\ \hline
Triangle & 3\\ \hline
Circle & 0\\ \hline 

\textbf{Colours} & \textbf{RGB Values}	\\ \hline	
Red & (75,0,0) - (255, 10, 10)\\ \hline
Green  & (0,75,0) - (10, 255, 10)\\ \hline
Blue   & (0,0,75) - (10, 10, 255)\\ \hline


\textbf{Size} & 	\textbf{Length/Radius (pixels)} \\ \hline			  
Big    & 32 - 35  \\ \hline


\textbf{Positions} & \textbf{Object Centre Coordinate}	\\ \hline				  
Centre Left &(22,32)\\ \hline
Centre Centre & (32,32)\\ \hline
Centre Right &(42,32)\\ \hline
				
\end{tabular}
\caption{Experiment 1 data subset.}
\label{tab:exp1_data} 
\end{table}

\subsubsection{Results}

The affect of increasing the embedding size is mostly to improve the reconstruction loss for images, in the Bimodal and Image Only testing conditions as shown by the blue and brown lines in \autoref{fig:graph331} (B). The reconstruction of images from their descriptions is only affected a very small amount by the size of the embedding. With the exception of when the embedding is very small, the description to image reconstruction errors only vary a small amount with respect to the embedding size. 

\begin{figure}[h]
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tikzpicture}
    \begin{axis}[
     name=plot1,
     axis x line=middle,
     axis y line=middle,
     enlarge y limits=true,
     enlarge x limits=true,
     legend style={at={(0.5,-0.5)}, anchor=north},
     %xmin=0, xmax=2150,
     %ymin=0, ymax=600,
     %width=15cm, height=8cm,     % size of the image
     grid = major,
     grid style={dashed, gray!30},
     ylabel= Total MSE,
     %ytick={0,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009},
     xlabel= (A) Embedding Size,
     %xtick={6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56},     
	 xlabel near ticks,
	 ylabel near ticks]
         ] 
    ]
    \addplot table[x = size, y = bimodal, col sep = comma]{csvs/331/total331.csv}; 
    \addplot table[x = size, y = words only, col sep = comma]{csvs/331/total331.csv};
    \addplot table[x = size, y = image only, col sep = comma]{csvs/331/total331.csv};    
    \legend{Bimodal, Words Only, Image Only}
    
    \end{axis}
    
    
     \begin{axis}[
     name=plot2,
     at=(plot1.right of south east), 
     anchor=left of south west,
     axis x line=middle,
     axis y line=middle,
     enlarge y limits=true,
     enlarge x limits=true,
     %legend style={at={(1,0.7)}, anchor=north},
     %xmin=0, xmax=2150,
     %ymin=0, ymax=600,
     %width=15cm, height=8cm,     % size of the image
     grid = major,
     grid style={dashed, gray!30},
     ylabel= Image MSE,
     %ytick={0,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009},
     xlabel= (B) Embedding Size,
     %xtick={6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56},     
     xlabel near ticks,
	 ylabel near ticks]
         ] 
    ]
    \addplot table[x = size, y = bimodal, col sep = comma]{csvs/331/image331.csv}; 
    \addplot table[x = size, y = words only, col sep = comma]{csvs/331/image331.csv};
    \addplot table[x = size, y = image only, col sep = comma]{csvs/331/image331.csv}; 
       
    %\legend{Bimodal, Words Only, Image Only}
    \end{axis}
    
	
    
    \begin{axis}[
     name=plot3,
     at=(plot2.below south west),
     anchor=above north west,
     axis x line=middle,
     axis y line=middle,
     enlarge y limits=true,
     enlarge x limits=true,
     %legend style={at={(1,0.7)}, anchor=north},
     %xmin=0, xmax=2150,
     %ymin=0, ymax=600,
     %width=15cm, height=8cm,     % size of the image
     grid = major,
     grid style={dashed, gray!30},
     ylabel= Word MSE,
     %ytick={0,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009},
     xlabel= (C) Embedding Size,
     %xtick={6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56},     
     xlabel near ticks,
	 ylabel near ticks]
         ] 
    ]
    \addplot table[x = size, y = bimodal, col sep = comma]{csvs/331/word331.csv}; 
    \addplot table[x = size, y = words only, col sep = comma]{csvs/331/word331.csv};
    \addplot table[x = size, y = image only, col sep = comma]{csvs/331/word331.csv};    
    %\legend{Bimodal, Words Only, Image Only}
    \end{axis}
    

\end{tikzpicture}
}
\caption{Mean Squared Error for reconstruction of images and words under different testing conditions, Bimodal, Words Only and Image Only for 3 colours, 3 positions and 1 size. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the word output.}
\label{fig:graph331}
\end{figure}


\begin{table}[h]
\centering
	\begin{tabular}{|c|c|c|c|}
	\hline
	Size & 	Bimodal & 	Image Only 	& 	Words Only \\ \hline
8	&	9.20E-03	$\mypm$	6.00E-04	&	9.70E-03	$\mypm$	7.00E-04	&	9.30E-03	$\mypm$	8.00E-04	\\ \hline
40	&	8.30E-03	$\mypm$	1.00E-04	&	8.60E-03	$\mypm$	1.00E-04	&	7.90E-03	$\mypm$	1.00E-04	\\ \hline
72	&	8.10E-03	$\mypm$	2.00E-04	&	8.40E-03	$\mypm$	1.00E-04	&	7.60E-03	$\mypm$	2.00E-04	\\ \hline
104	&	7.70E-03	$\mypm$	4.00E-04	&	8.40E-03	$\mypm$	2.00E-04	&	7.00E-03	$\mypm$	5.00E-04	\\ \hline
136	&	7.10E-03	$\mypm$	6.00E-04	&	8.40E-03	$\mypm$	2.00E-04	&	6.50E-03	$\mypm$	7.00E-04	\\ \hline
168	&	7.00E-03	$\mypm$	6.00E-04	&	8.40E-03	$\mypm$	2.00E-04	&	6.30E-03	$\mypm$	7.00E-04	\\ \hline
200	&	\textbf{6.80E-03}	$\mypm$	3.00E-04	&	\textbf{8.30E-03}	$\mypm$	2.00E-04	&	\textbf{6.10E-03}	$\mypm$	3.00E-04	\\ \hline
232	&	7.00E-03	$\mypm$	6.00E-04	&	8.50E-03	$\mypm$	0.00E+00	&	6.30E-03	$\mypm$	7.00E-04	\\ \hline
264	&	6.90E-03	$\mypm$	9.00E-04	&	8.40E-03	$\mypm$	2.00E-04	&	6.30E-03	$\mypm$	1.00E-03	\\ \hline
296	&	\textbf{6.80E-03}	$\mypm$	1.10E-03	&	8.60E-03	$\mypm$	3.00E-04	&	6.20E-03	$\mypm$	1.20E-03	\\ \hline


	\end{tabular}
\caption{Experiment 1: Total MSE for a selection of different embedding sizes for the three testing conditions, Bimodal, Image Only and Words Only. An Embedding Size of 200 neurons provides the minimum reconstruction error.}
\label{tab:res331}
\end{table}

\autoref{tab:res331} allows a closer look at the values of the total loss (the sum of image and description reconstruction errors) for the different testing conditions. An embedding size of 200 neurons provides the best total loss in all testing conditions. This performance is matched by an embedding size of 296 neurons in the Bimodal testing condition.


From the numerical results presented in \autoref{tab:res331} it is clear that the majority of the loss comes from the image output. This is reasonable as the dimensionality of the images is much larger, 64x64x3 vs 22x1 for the description  output. Notice in particular the scale of the errors in \autoref{fig:graph331}, the image output loss is of a scale $10^{-2}$ compared to the description output which has a scale of $10^{-5}$, which is three orders of magnitude smaller. 

There is a limit to how good the images reconstructed from descriptions can be as the descriptions do not contain all of the information necessary to perfectly reconstruct the images. This shows up as the difference between image reconstruction in the Image Only testing condition vs the Words Only condition. Put simply, this difference is caused by the standard deviation within the colours and sizes of the shapes. 

Whilst the images generated in the Image Only and Bimodal testing conditions have access to exact RGB values, the descriptions only contain the name of the colour  which represents the object and can therefore only produce objects with the ``average'' RGB values for those words. For example, looking at \autoref{tab:exp1_data} we see that the colour ``red'' can have RGB values anywhere in the range (75,0,0) - (255, 10, 10).

The standard deviation across the fours runs of the experiment is very low for all embedding sizes, showing that the images generated by the MAE are independent of the starting weights and the randomly selected training data from the ArtS dataset. 

\paragraph{Image Generation}

Generating images from full descriptions gives very good quality results as seen in \autoref{fig:331multi}. Here we see images generated with an embedding size of 200 neurons. From \autoref{tab:res331} we can see that 200 gave the best reconstruction error in the Bimodal, Image Only and Words Only testing conditions. However, the small changes in reconstruction error in the Words Only testing condition are negligble when considering that the semantic information contained in the images is always correct regardless of embedding size. Each description leads to the generation of an image which clearly matches its descriptions, regardless of the embedding size. See \autoref{fig:8vs200} in \autoref{appendix:B} for a comparison of the best and worst case scenario.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{Figs/shapes/multiword331.png}
\caption{Experiment 1:  Images generated from descriptions with an embedding size of 200.}
\label{fig:331multi}
\end{figure}

Given how well images can be generated from full descriptions, it is unsurpising to see that the MAE has been able to disentangle the descriptions and correctly identify the meanings of each of the words. In \autoref{fig:331shapes} it can be seen that each shape is correctly generated into an image for most embedding sizes. It is difficult to specify which embedding size disentangles the meanings of each word the best as this is purely subjective. I would suggest that and embedding size of 296 neurons grounds the meanings of individual words the best, this is due to it producing the most solid circle, rectangle and triangle across all four runs, in my opinion. 

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{Figs/shapes/shapes331.png}
\caption{Experiment 1, runs A-D:  Images generated of each shape for different sizes of embedding.}
\label{fig:331shapes}
\end{figure}

The images generated for every word can be found in the suplimental material. As the colour and position are always  correctly learnt across the four runs, they are not shown in the main text.



\paragraph{Multilabel classification}
The MAE gets 100\% accuracy on all descriptions for all testing conditions and all embedding sizes. Accuracy is calculated by counting the number of correctly predicted words and averaging over the entire dataset. For example, given an image of ``big blue circle left'' and the prediction ``small blue circle left'' the accuracy would be 0.75 for that instance as 3 out of 4 words are correctly predicted. A word is considered to be predicted if its neurons activation is above 0.5. As I am using a sigmoid activation function, the output for each neuorn is always between 0 and 1.


\subsubsection{Discussion}
The results of experiment 1 show clearly that the MAE is capable of preforming symbol grounding on this subset of the ArtS dataset. However, it is difficult to make any conclusions about which embedding size gave the best results. Whilst 200 neurons gave the lowest reconstruction error in all testing conditions (see \autoref{tab:res331}), it did not do the best job of disentangling the meanings of each word in the descritptions (see \autoref{fig:331shapes}). 

In the next experiment, we will observe how embedding size affects the reconstruction error and symbol grounding of the MAE when a larger variety of sizes for the objects in the images are available.



\subsection{Experiment 2}
I will now explore how adding more variety to the data affects the quality of reconstruction. To that end, two new sizes of object are added as seen in \autoref{tab:exp2_data}. 

\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Attribute} & \textbf{Description} \\ \hline \hline
\textbf{Shapes} & \textbf{Corners} \\ \hline
Rectangle & 4\\ \hline
Triangle & 3\\ \hline
Circle & 0\\ \hline 

\textbf{Colours} & \textbf{RGB Values}	\\ \hline	
Red & (75,0,0) - (255, 10, 10)\\ \hline
Green  & (0,75,0) - (10, 255, 10)\\ \hline
Blue   & (0,0,75) - (10, 10, 255)\\ \hline


\textbf{Sizes} & 	\textbf{Length/Radius (pixels)} \\ \hline			  
Big    & 32 - 35  \\ \hline
Medium & 22 - 25 \\ \hline
Small  & 12 - 15 \\ \hline 

\textbf{Positions} & \textbf{Object Centre Coordinate}	\\ \hline					  
Centre Left &(22,32)\\ \hline
Centre Centre & (32,32)\\ \hline
Centre Right &(42,32)\\ \hline				
\end{tabular}
\caption{Experiment 2 data subset.}
\label{tab:exp2_data} 
\end{table}



\subsubsection{Results}
\begin{figure}
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tikzpicture}
    \begin{axis}[
     name=plot1,
     axis x line=middle,
     axis y line=middle,
     enlarge y limits=true,
     enlarge x limits=true,
     legend style={at={(0.5,-0.5)}, anchor=north},
     %xmin=0, xmax=2150,
     %ymin=0, ymax=600,
     %width=15cm, height=8cm,     % size of the image
     grid = major,
     grid style={dashed, gray!30},
     ylabel= Total MSE,
     %ytick={0,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009},
     xlabel= (A) Embedding Size,
     %xtick={6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56},     
	 xlabel near ticks,
	 ylabel near ticks]
         ] 
    ]
    \addplot table[x = size, y = bimodal, col sep = comma]{csvs/333/total333.csv}; 
    \addplot table[x = size, y = words only, col sep = comma]{csvs/333/total333.csv};
    \addplot table[x = size, y = image only, col sep = comma]{csvs/333/total333.csv};    
    \legend{Bimodal, Words Only, Image Only}
    
    \end{axis}
    
    
     \begin{axis}[
     name=plot2,
     at=(plot1.right of south east), 
     anchor=left of south west,
     axis x line=middle,
     axis y line=middle,
     enlarge y limits=true,
     enlarge x limits=true,
     %legend style={at={(1,0.7)}, anchor=north},
     %xmin=0, xmax=2150,
     %ymin=0, ymax=600,
     %width=15cm, height=8cm,     % size of the image
     grid = major,
     grid style={dashed, gray!30},
     ylabel= Image MSE,
     %ytick={0,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009},
     xlabel= (B) Embedding Size,
     %xtick={6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56},     
     xlabel near ticks,
	 ylabel near ticks]
         ] 
    ]
    \addplot table[x = size, y = bimodal, col sep = comma]{csvs/333/image333.csv}; 
    \addplot table[x = size, y = words only, col sep = comma]{csvs/333/image333.csv};
    \addplot table[x = size, y = image only, col sep = comma]{csvs/333/image333.csv}; 
       
    %\legend{Bimodal, Words Only, Image Only}
    \end{axis}
    
	
    
    \begin{axis}[
     name=plot3,
     at=(plot2.below south west),
     anchor=above north west,
     axis x line=middle,
     axis y line=middle,
     enlarge y limits=true,
     enlarge x limits=true,
     %legend style={at={(1,0.7)}, anchor=north},
     %xmin=0, xmax=2150,
     %ymin=0, ymax=600,
     %width=15cm, height=8cm,     % size of the image
     grid = major,
     grid style={dashed, gray!30},
     ylabel= Word MSE,
     %ytick={0,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009},
     xlabel= (C) Embedding Size,
     %xtick={6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56},     
     xlabel near ticks,
	 ylabel near ticks]
         ] 
    ]
    \addplot table[x = size, y = bimodal, col sep = comma]{csvs/333/word333.csv}; 
    \addplot table[x = size, y = words only, col sep = comma]{csvs/333/word333.csv};
    \addplot table[x = size, y = image only, col sep = comma]{csvs/333/word333.csv};    
    %\legend{Bimodal, Words Only, Image Only}
    \end{axis}
    

\end{tikzpicture}
}
\caption{Mean Squared Error for reconstruction of images and words under different testing conditions, Bimodal, Words Only and Image Only for 3 colours, 3 positions and 3 sizes. (A) shows the total MSE, (B) shows the MSE of the image output and (C) shows the MSE of the word output.}
\label{fig:graph333}
\end{figure}

Once again we see that the majority of error comes from the image reconstruction loss by comparing the scales of \autoref{fig:graph333} (B) and (C). The description reconstruction error is larger and less stable with respect to the embedding size than in experiment 1, howver it still remains very small with a scale of $10e^{-4}$.

Interestingly, we see that increasing the embedding size seems to have a positive effect on image reconstruction loss under all testing conditions, though this is still not a very significant increase improvement in the wWrds Only testing condition. \autoref{tab:res333} highlights this, with the best total loss occuring for an embedding size of 296 neurons in all three testing conditions. 
Comparing this to the results of experiment 1, where 200 neurons was the best embedding size with respect to reconstruction loss, it is possible that as the data becomes more complex, the extra neurons allow for representing the multimodal data better.

\begin{table}
\centering
	\begin{tabular}{|c|c|c|c|}
	\hline
	Size & 	Bimodal & 	Image Only 	& 	Words Only \\ \hline
8	&	6.65E-03	$\mypm$	2.00E-04	&	7.37E-03	$\mypm$	3.00E-04	&	6.68E-03	$\mypm$	3.00E-04	\\ \hline
40	&	5.38E-03	$\mypm$	1.00E-04	&	5.66E-03	$\mypm$	1.00E-04	&	5.27E-03	$\mypm$	3.00E-04	\\ \hline
72	&	5.57E-03	$\mypm$	5.00E-04	&	5.90E-03	$\mypm$	5.00E-04	&	6.67E-03	$\mypm$	1.90E-03	\\ \hline
104	&	5.04E-03	$\mypm$	2.00E-04	&	5.52E-03	$\mypm$	2.00E-04	&	4.69E-03	$\mypm$	2.00E-04	\\ \hline
136	&	4.89E-03	$\mypm$	2.00E-04	&	5.53E-03	$\mypm$	1.00E-04	&	4.57E-03	$\mypm$	2.00E-04	\\ \hline
168	&	4.91E-03	$\mypm$	2.00E-04	&	5.51E-03	$\mypm$	1.00E-04	&	4.50E-03	$\mypm$	2.00E-04	\\ \hline
200	&	4.70E-03	$\mypm$	2.00E-04	&	5.47E-03	$\mypm$	1.00E-04	&	4.60E-03	$\mypm$	7.00E-04	\\ \hline
232	&	4.91E-03	$\mypm$	3.00E-04	&	5.66E-03	$\mypm$	2.00E-04	&	4.53E-03	$\mypm$	3.00E-04	\\ \hline
264	&	4.64E-03	$\mypm$	2.00E-04	&	5.46E-03	$\mypm$	1.00E-04	&	4.28E-03	$\mypm$	2.00E-04	\\ \hline
296	&	\textbf{4.41E-03}	$\mypm$	1.00E-04	&	\textbf{5.44E-03}	$\mypm$	1.00E-04	&	\textbf{4.19E-03}	$\mypm$	3.00E-04	\\ \hline



	
	\end{tabular}
\caption{Experiment 2:  Total MSE for a selection of different embedding sizes for the three testing conditions, Bimodal, Image Only and Words Only. An Embedding Size of 296 neurons provides the minimum reconstruction error.}
\label{tab:res333}
\end{table}


\paragraph{Image Generation}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{Figs/shapes/multiword333.png}
\caption{Images generated from descriptions with an embedding size of 296 neurons using the MAE from experiment 2.}
\label{fig:333multi}
\end{figure}

Generating images from full descriptions remains effective as seen in \autoref{fig:333multi}.
With the addition of the extra sizes, generation of images from a single word has become significantly poorer. However, it is still clear that at least some of the words have been succefully grounded. Particularly, the meaning of colours and positions are correctly learnt, with images of distinct colours and positions being generated as seen in \autoref{fig:333single}. 

There is also a clear releationship between the three sizes, shown by the generation of the most coloured pixels for the word ``big'', fewer for ``medium'' and the least for ``small'.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Figs/shapes/singlelabel333A.png}
\caption{Images generated of each word for different sizes of embedding using the MAE trained in experiment 2 run A.}
\label{fig:333single}
\end{figure} 

The quality of the shapes being generated from the words ``Circle'', ``Rectangle'' and ``Triangle'' has suffered the most. However, there is a clear difference between the three shapes and they do show properties of the shapes they are supposed to be. The ``Circle'' is generally more round than either of the other shapes. The ``Rectangle'' has approxiamtely 4 sides at right angles to one and other, though it is significantly more ``fuzzy'' than in experiment 1. The ``Triangle '' appears triangular, however, for most embedding sizes there appear to be multiple triangles being drawn, this could signify a confusion about where to draw the triangle. I reason that this is the case as in \autoref{fig:2word333} the triangle is significantly more solid when a position is specified. 

Whilst an embedding size of 136 neurons disentangled the meanings of ``Circle'', ``Rectangle'' and ``Triangle'' in run B of experiment 2, this result was not consistent across all four runs (or for any other embedding size) as seen in \autoref{fig:shapes333}. This indicates that the weight initialisation and data selection have an effect on whether the MAE can learn the meanings of the shape names  when additional object sizes are added to the training data.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Figs/shapes/shapes333.png}
\caption{Experiment 2, runs A-D:  Images generated of each shape for different sizes of embedding.}
\label{fig:shapes333}
\end{figure}

Another way to inspect the quality of the learnt embedding is to provide pairs of words to the MAE and observe what it generates as output. \autoref{fig:2word333} shows the output of the MAE trained in run A of experiment 2 (runs B-D can be found in \autoref{appendix:B} with an embedding size of 296 neurons. Here we see clearly that the MAE is correctly generating the words ``Circle'', ``Rectangle'' and ``Triangle'' as well as the other words in its ``vocabulary''. The diagonal of \autoref{fig:2word333} is the equivalent of the bottom row (296) of \autoref{fig:333single}. The addition of a second word allows for the correct generation of the three shapes in different positions and sizes, suggesting that when only the shape is provided as input, the MAE does not know where to draw the shape or how big to draw it. By adding a second word and removing some uncertainty, the output of the MAE is much more defined. This is a particularly strong result in the cas eof when a position is provided in conjuction with either a shape, size or colour. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Figs/shapes/2word333A.png}
\caption{Experiment 2, run A:  Images generated using word pairs using an embedding size of 296 neurons.}
\label{fig:2word333}
\end{figure}

\paragraph{Multilabel classification}
Even with the added complexity of the additional sizes, the MAE gets 100\% accuracy as before. 


\subsubsection{Discussion}

Increasing the number of variable attributes in the dataset, i.e. adding the extra sizes, has had a negative affect on the ability of the MAE to disentangle the meanings of each of the words in the descriptions. This is highlighted when the results of experiments 1 and 2 are seen side by side as in \autoref{fig:shapes333v331}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Figs/shapes/shapes331v333.png}
\caption{Images generated of each shape for different sizes of embedding using the MAEs trained in experiments 1 and 2 run A.}
\label{fig:shapes333v331}
\end{figure} %This issue could likely be solved by the addition of more examples per object.
However, the MAE reamins able to accurately generate images from full descriptions (see \autoref{fig:333multi}). 
An embedding size of 296 neurons gave the all around best performance in experiment 2 and performed the best at symbol grounding in experiment 1, therefore it will be used for the later experiments.

\subsection{Experiment 3}
This experiment adds further vairety to the data, adding an additional 6 positions as seen in \autoref{tab:exp3_data}. In this experiment, we will also observe how incrementally learning these new properties affects the quality of the generated outputs as well as the grounding of the different words from the descriptions. 

Incremental learning is performed by comparing the quality of the output from a MAE trained starting from, random weights, the weights learned in expermiment one and the weights learning in experiment two. This will help answer the fundamental question of whether, having knowledge of the meaning of a subset of words and their image equivalents, aids the learning of new words and their image equivalents.

Unlike in the previous two experiments, the embedding size will be fixed to 296 neurons as this gave the best performance in experiment 2 in terms of loss.


\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Attribute} & \textbf{Description} \\ \hline \hline
\textbf{Shapes} & \textbf{Corners} \\ \hline
Rectangle & 4\\ \hline
Triangle & 3\\ \hline
Circle & 0\\ \hline 

\textbf{Colours} & \textbf{RGB Values}	\\ \hline	
Red & (75,0,0) - (255, 10, 10)\\ \hline
Green  & (0,75,0) - (10, 255, 10)\\ \hline
Blue   & (0,0,75) - (10, 10, 255)\\ \hline

\textbf{Sizes} & 	\textbf{Length/Radius (pixels)} \\ \hline			  
Big    & 32 - 35  \\ \hline
Medium & 22 - 25 \\ \hline
Small  & 12 - 15 \\ \hline 

\textbf{Positions} & \textbf{Object Centre Coordinate}	\\ \hline					  
Top Left & (22,42)\\ \hline	
Top Centre & (32,42)\\ \hline
Top Right & (42,42)\\ \hline
Centre Left &(22,32)\\ \hline
Centre & (32,32)\\ \hline
Centre Right &(42,32)\\ \hline
Bottom Left & (22,22)\\ \hline
Bottom Centre & (22,42)\\ \hline
Bottom Right & (42,22)\\ \hline				
\end{tabular}
\caption{Experiment 2 data subset.}
\label{tab:exp3_data} 
\end{table}

\subsubsection{Results}
Using the pretrained weights from experiment 1 as a starting point for experiment 3 leads to the lowest reconstruction error as seen in \autoref{tab:res339}. The lower reconstruction error during testing does not translate to improved symbol grounding.

The different starting conditions will be referred to as: Random: randomly initialised weights, Exp1: the weights trained during experiment 1 and Exp2: the weights trained during experiment 2.

\begin{table}[h!]
\centering
	\begin{tabular}{|c|c|c|c|}
	\hline
	Initialisation & 	Bimodal & 	Image Only 	& 	Words Only \\ \hline
	Random	&	5.43E-03	$\mypm$	5.61E-04	&	5.42E-03	$\mypm$	5.60E-04	&	1.00E-06	$\mypm$	7.38E-07	\\ \hline
Exp 1	&	\textbf{5.29E-03}	$\mypm$	4.73E-04	&	\textbf{5.29E-03}	$\mypm$	4.72E-04	&	1.14E-06	$\mypm$	9.20E-07	\\ \hline
Exp 2	&	5.33E-03	$\mypm$	3.12E-04	&	5.33E-03	$\mypm$	3.12E-04	&	\textbf{8.51E-07}	$\mypm$	3.59E-07	\\ \hline

	\end{tabular}
\caption{Experiment 3: Total MSE for different weight initialisations for the three testing conditions, Bimodal, Image Only and Words only.}
\label{tab:res339}
\end{table}


\paragraph{Image Generation}
It is not feasable to show every combination of shape, colour, size and position, so in figure \autoref{fig:339multi} a selection of these combinations are shown.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{Figs/shapes/multiword339.png}
\caption{Experiment 3, run A: Images generated from descriptions, starting training from randomly initialised weights.}
\label{fig:339multi}
\end{figure}

The addition of new positions has had an effect on the quality of the images generated from descriptions by the MAE. The images have become blurrier compared to images generated in previous experiments. However they are still easily recognisable as matching their descriptions.

The new positions are correctly learnt with a clear distinction between positions on the top, centre and bottom as well as left, centre and right and their combinations (e.g. bottom right).

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{Figs/shapes/singlelabel339.png}
\caption{Experiment 3, run A: Images generated from shape, colour and size words for different weight initialisation conditions.}
\label{fig:339single}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{Figs/shapes/singlelabel339_pos.png}
\caption{Experiment 3, run A: Images generated from each position word for different weight initialisation conditions.}
\label{fig:339single_pos}
\end{figure}


As in experiment 2, the addition of a second word as input, leads to better reconstruction of the images. \autoref{fig:2word339} shows a comparison of the three different weight initialisations after training. It is clear in all three cases that the MAE learns to correctly ground the words ``Circle'', ``Rectangle'' and ``Triangle''. However, in the case of initialising with weights from experiment 1, the transfer learning results in a worse performance than random weights.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Figs/shapes/2word339_pos.png}
\caption{Experiment 3, run A: Images generated using word pairs.}
\label{fig:2word339}
\end{figure}

Initialising with weights from experiment 2 offers a marginal benifit, with each of the shapes being generally more solid in appearance. Full results from all four runs can be found in the supplimental materials.


\paragraph{Multilabel classification}

\subsubsection{Discussion}
Though the addition of extra positions had a detrimental effect on the generation of images from single words (see \autoref{fig:339single} and \autoref{fig:339single_pos}, the MAE was still able to correctly learn the meaning of all of the words. This becomes particularly apparent when pairs of words are provided as input as in \autoref{fig:2word339}.

The effects of transfer learning, that is taking a developmental approach and transfering weights from a MAE which has already mastered a simpler task are not universal. Whilst the weights from experiment 2 provided a small but noticable benifit when generating images from pairs of words over the random initialisation baseline, the weights from experiment 1 did not. 

\subsection{Experiment 4}
For this experiment we will make use of all of the data in ArtS as shown in \autoref{tab:Arts_desc}. Again I will explore the affects of incremental learning, utilising the prelearned weights of experiments 1 and 2 and 3. This will allow me to examine whether having a larger number of concepts (words and their image equivalents) mastered  helps learning new concepts or whether one can simply learn all concepts simultaneously.

The different starting conditions will be referred to as: Random: randomly initialised weights, Exp1: the weights trained during experiment 1, Exp2: the weights trained during experiment 2, Exp 3: the weights learnt during experiment 3 starting from random initialisation, Exp 1+3: the weights learnt during experiment 3 starting with the weights from experiment 1, Exp 2+3: the weights learnt during experiment 3 starting with the weights from experiment 2.

\subsubsection{Results}

\begin{table}[h!]
\centering
	\begin{tabular}{|c|c|c|c|}
	\hline
	Initialisation & 	Bimodal & 	Image Only 	& 	Words Only \\ \hline
Random	&	3.82E-03	$\mypm$	1.22E-04	&	3.26E-02	$\mypm$	3.65E-02	&	4.00E-03	$\mypm$	9.74E-05	\\ \hline
Exp1	&	3.73E-03	$\mypm$	6.07E-05	&	9.33E-03	$\mypm$	9.71E-03	&	3.91E-03	$\mypm$	3.63E-05	\\ \hline
Exp2	&	3.80E-03	$\mypm$	1.94E-04	&	8.55E-03	$\mypm$	4.70E-03	&	3.96E-03	$\mypm$	2.08E-04	\\ \hline
Exp3	&	3.75E-03	$\mypm$	2.86E-05	&	1.43E-02	$\mypm$	1.22E-02	&	3.93E-03	$\mypm$	4.73E-05	\\ \hline
Exp1+3	&	3.77E-03	$\mypm$	1.40E-04	&	1.62E-02	$\mypm$	1.80E-02	&	3.97E-03	$\mypm$	1.28E-04	\\ \hline
Exp 2+3	&	3.76E-03	$\mypm$	1.23E-04	&	3.31E-02	$\mypm$	5.70E-02	&	3.92E-03	$\mypm$	6.28E-05	\\ \hline



	\end{tabular}
\caption{Experiment 4: Total MSE for different weight initialisations for the three testing conditions, Bimodal, Image Only and Words Only.}
\label{tab:res739}
\end{table}

\paragraph{Image Generation}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{Figs/shapes/singlelabel739_shape.png}
\caption{Experiment 4, run A: Images generated from shape, colour and size words for different weight initialisation conditions.}
\label{fig:739single_shape}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{Figs/shapes/singlelabel739_col.png}
\caption{Experiment 4, run A: Images generated from each colour word for different weight initialisation conditions.}
\label{fig:739single_col}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{Figs/shapes/singlelabel739_pos.png}
\caption{Experiment 4, run A: Images generated from each position word for different weight initialisation conditions.}
\label{fig:739single_pos}
\end{figure}



\begin{figure}
\centering
\includegraphics[width=\textwidth]{Figs/shapes/2word739_pos.png}
\caption{Experiment 4, run A: Images generated using word pairs.}
\label{fig:2word739}
\end{figure}



\paragraph{Multilabel classification}


\begin{table}[h!]
\centering
	\begin{tabular}{|c|c|c|c|}
	\hline
	Initialisation & 	Bimodal & 	Image Only 	& 	Words Only \\ \hline
Random	&	1.00E+00	$\mypm$	0.00E+00	&	9.74E-01	$\mypm$	5.91E-02	&	1.00E+00	$\mypm$	0.00E+00\\ \hline
Exp1	&	1.00E+00	$\mypm$	0.00E+00	&	9.95E-01	$\mypm$	2.28E-02	&	1.00E+00	$\mypm$	0.00E+00\\ \hline
Exp2	&	1.00E+00	$\mypm$	2.98E-05	&	9.95E-01	$\mypm$	1.26E-02	&	1.00E+00	$\mypm$	0.00E+00\\ \hline
Exp3	&	1.00E+00	$\mypm$	0.00E+00	&	9.91E-01	$\mypm$	2.49E-02	&	1.00E+00	$\mypm$	0.00E+00\\ \hline
Exp1+3	&	1.00E+00	$\mypm$	0.00E+00	&	9.88E-01	$\mypm$	2.75E-02	&	1.00E+00	$\mypm$	0.00E+00\\ \hline
Exp 2+3	&	1.00E+00	$\mypm$	0.00E+00	&	9.76E-01	$\mypm$	6.72E-02	&	1.00E+00	$\mypm$	0.00E+00\\ \hline


	\end{tabular}
\caption{Experiment 4: Description Accuracy for different weight initialisations for the three testing conditions, Bimodal, Image Only and Words Only}
\label{tab:res739acc}
\end{table}


\subsubsection{Discussion}

\subsection{Experiment 5}
In this experiment I will select certain subsets of objects from the data to omit, such as ``Violet Circles'' or ``Green Triangles''. Then demonstrate how images of these objects can still be generated from their descriptions or through vector arithematic, as well as how images of these objects can be correctly described by the network.

\subsubsection{Results}




\paragraph{Image Generation}
\paragraph{Multilabel classification}
\paragraph{Vector Arithematic}

\subsubsection{Discussion}

\section{Conclusion}

\subsubsection{The effect of  more training examples}
It might seem that the task of learning the relationship between the words of the descriptions and the attributes of the images that they describe, is trivial. Of course, it is for an adult human. Even if the descriptions were given in an unknown language, given very few examples (possibly even less than one example per per shape, colour, size and position combination) an adult could learn the relationship very quickly. Why then, does a neural network need so many examples to get things right?

The root of this issue comes down to two main factors, 1) the underlying probability distribution of the data and 2) the learning method: gradient descent.

To understand what I mean by ``the underlying probability distribution of the data'' let us consider the simple example of rolling a die to decide if it is fair.

How many times should we roll the die before we decide it is fair? If it is a six sided die and it is fair, there should be a $\frac{1}{6}$ chance of rolling each of the numbers 1 to 6. So if we roll it 6 times we should expect to see each number once, however, we most likely won't, that doesn't mean our die is rigged though.

As we roll the die more and more times, we can get a better understanding of the true probability of rolling a given number and therefore get closer to deciding if the die is indeed fair.

\begin{table}
\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
	\hline
	Rolls & 1 & 2 & 3 & 4 & 5 & 6 & $D_{KL}$ \\ \hline
	10 & 0 & 0.2 & 0.2 & 0.3 & 0.2 & 0.1 & 31.36 \\ \hline
	1000 & 0.168 & 0.148 & 0.169 & 0.174 & 0.172 & 0.169 & 10.76 \\ \hline
	100000 & 0.166 & 0.166 & 0.167 & 0.168 & 0.165 & 0.167 & 10.75 \\ \hline
	1000000 & 0.167 & 0.167 & 0.167 & 0.167 & 0.167 & 0.167 & 10.75 \\ \hline
	\end{tabular}
	\caption{Probability distributions generated from rolling a 6 sided die.}
	\label{tab:dieProb}
\end{table}

As we roll the die more and more we will get closer to the actual probability distribution which governs its behaviour. In the limit, the probability for rolling any number should converge to $\frac{1}{6} = 0.167$. Looking at \autoref{tab:dieProb} we can see that the pseudo random number generator (PRNG) I used to simulate rolling a die on my computer, is probably mostly fair. 

Looking at the final column of \autoref{tab:dieProb} we can see that the KL divergence \autoref{eqn:kld}, converges quickly towards 0 for the first 1000 rolls but the slows down and improves only in the 4th or 5th digit after the decimal point. this suggests that the PRNG isn't actually completely fair as even after 10 million rolls, the KL divergence from a uniform distribution is still quite large.

So, how does this relate to the ArtS dataset? Let us replace rolling a die in this example with selecting random images from the ArtS dataset and ask, how much information does this one sample tell me about the underlying distribution of the dataset. If we only select a few images, we are unlikely to capture the entire underlying distribution of the data.
For example, when we select only a few images we might get only dark blue rectangles and no light blue rectangles, so when the neural network sees a light blue rectangle at test time, it won't know that light blue falls into the colour category of ``Blue''. However, as we select more samples, we get a better idea of what the true distribution looks like, just as the estimated probability for rolling a die got closer and closer to uniform the more we rolled it. That means that if we have a larger sample size, we are more likely to see light blue rectnagles as well as dark blue rectangle (and every other shade, shape, size and position combination). 

On to the second point, learning by gradient descent. In \autoref{Chapter3} I explained how gradient descent operates, minimising the cost until it reaches a minimum. However, in practice, it is likely that gradient descent will get stuck in a local minimum, and not find the global minimum. 

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
axis lines=left,
xtick={90, 180, 270, 360,450,540,630},
xticklabels={},
xlabel={$\Delta w$},
ylabel={$C$},
yticklabels={},
xlabel near ticks,
ylabel near ticks]
\addplot[black, thick, smooth, samples=1000, domain=0:360]{sin(x) + sin(2*\x)+sin(3*\x)+sin(4*\x) + sin(5*\x)};

\addplot[black, thick, smooth, samples=1000, domain=360:630]{-sin(1*(\x-300)) -sin(2*(\x-300)) -sin(3*(\x-300)) -sin(4*(\x-300)) -sin(5*(\x-300))};

\addplot[blue, thick, smooth, samples=1000, domain=0:360]{sin(x) + sin(2*\x)+sin(3*\x)};

\addplot[blue, thick, smooth, samples=1000, domain=360:630]{-sin(1*(\x-270)) -sin(2*(\x-270)) -sin(3*(\x-270))};


\node [red] at (105,375)  (a) {\textbullet};
\node [blue] at (335,5)  (a) {\textbullet};
\node [yellow] at (285,345) (b) {\textbullet};
\legend{Many data, , Few data, }
\end{axis}
    

\end{tikzpicture}
\caption{Visualisation of a cost landscape.}
\label{fig:localminima}
\end{center}
\end{figure}

Consider the red point shown in \autoref{fig:localminima}, if we only have a few examples, gradient descent is suseptable to getting stuck in local minima. However, if we add more training data, the shape of the cost landscape changes. This can remove some local minima whilst also providing more information about the underlying distribution of the data, helping us to find the global minima, the blue point.

\subsubsection{How a developmental approach helps}
When we perform tansfer learning, i.e. pretraining on a different dataset or subset of the data as in experiments 3 and 4, we start at a point in the cost landscape that is hopefully closer to the global minima than starting with randomly secelted wweights.

So, by taking a developmental approach, mastering simple tasks and slowly increasing the difficulty of the tasks we set our neural networks to tackle, we can incrementally approach the global minima. In the case of the ArtS dataset this means incrementally learning to perform bidirectional symbol grounding on a wider variety of words and image properties. Learning new colours, shapes, sizes and positions.